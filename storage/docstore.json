{"docstore/data": {"0b4382ff-e79b-47c3-84d5-167c60e1053a": {"__data__": {"id_": "0b4382ff-e79b-47c3-84d5-167c60e1053a", "embedding": null, "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_704a56a9-9789-4413-9f94-73d6eda34e2b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "470aea9e-eee4-4af0-8a39-60b8b670815c", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_704a56a9-9789-4413-9f94-73d6eda34e2b"}, "hash": "274109002ff0028723f6b470bbec85eb35cd9c38a7a7ee4ac0d35ed0cdf0315f", "class_name": "RelatedNodeInfo"}}, "text": "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.comNoam Shazeer\u2217\nGoogle Brain\nnoam@google.comNiki Parmar\u2217\nGoogle Research\nnikip@google.comJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.comAidan N. Gomez\u2217 \u2020\nUniversity of Toronto\naidan@cs.toronto.edu\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217 \u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 2023", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2853, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e43b9b6-413a-44f0-bd29-d84376d7b73b": {"__data__": {"id_": "7e43b9b6-413a-44f0-bd29-d84376d7b73b", "embedding": null, "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bb943d75-8df1-49b3-976f-ccdb5f54cd04"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "559ad40b-e79b-47cb-82f0-052cd2898e26", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bb943d75-8df1-49b3-976f-ccdb5f54cd04"}, "hash": "0d01bff272f50e4b56234d3e40fc59faa8974824ad589deba45c6428b0ac10ac", "class_name": "RelatedNodeInfo"}}, "text": "1 Introduction\nRecurrent neural networks, long short-term memory [ 13] and gated recurrent [ 7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [ 35,2,5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [ 21] and conditional\ncomputation [ 32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [ 2,19]. In all but a few cases [ 27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2 Background\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [ 18] and ConvS2S [ 9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [ 12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [ 5,2,35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., x n)to a sequence\nof continuous representations z= (z1, ..., z n). Given z, the decoder then generates an output\nsequence (y1, ..., y m)of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4260, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c0b87291-6364-437a-8497-9c194e92c071": {"__data__": {"id_": "c0b87291-6364-437a-8497-9c194e92c071", "embedding": null, "metadata": {"page_label": "3", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0e226579-7dde-4f51-adb3-27afe150a8df"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc53427c-1dc4-4c9f-bfa4-f0d38a0043ef", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0e226579-7dde-4f51-adb3-27afe150a8df"}, "hash": "7649cf654311d7e6e7039306ea23c2616311f064a819ddc2c21545b20c8d25ad", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1 Encoder and Decoder Stacks\nEncoder: The encoder is composed of a stack of N= 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [ 11] around each of\nthe two sub-layers, followed by layer normalization [ 1]. That is, the output of each sub-layer is\nLayerNorm( x+ Sublayer( x)), where Sublayer( x)is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512 .\nDecoder: The decoder is also composed of a stack of N= 6identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position ican depend only on the known outputs at positions less than i.\n3.2 Attention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1826, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4c776d2e-0f89-4772-9183-963b6b147d22": {"__data__": {"id_": "4c776d2e-0f89-4772-9183-963b6b147d22", "embedding": null, "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5d4704c4-606f-4608-a475-cfe215f9aa27"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f70ecc57-7f31-4023-b1ce-8e853d34e49f", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5d4704c4-606f-4608-a475-cfe215f9aa27"}, "hash": "5ec976fd11066aca930019cd237626be72a83cc0c3d05d9f4c13d390f830f32e", "class_name": "RelatedNodeInfo"}}, "text": "Scaled Dot-Product Attention\n Multi-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1 Scaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by\u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices KandV. We compute\nthe matrix of outputs as:\nAttention( Q, K, V ) = softmax(QKT\n\u221adk)V (1)\nThe two most commonly used attention functions are additive attention [ 2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof1\u221adk. Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dkthe two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk[3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients4. To counteract this effect, we scale the dot products by1\u221adk.\n3.2.2 Multi-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values htimes with different, learned\nlinear projections to dk,dkanddvdimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of qandkare independent random\nvariables with mean 0and variance 1. Then their dot product, q\u00b7k=Pdk\ni=1qiki, has mean 0and variance dk.\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ad582ac4-aa80-4733-8ea7-f44adb1d7da3": {"__data__": {"id_": "ad582ac4-aa80-4733-8ea7-f44adb1d7da3", "embedding": null, "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0e1bb44f-303c-465f-8f47-80d5e276c7d2"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36c33ffa-c012-4413-bb86-9fac6a6d7796", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0e1bb44f-303c-465f-8f47-80d5e276c7d2"}, "hash": "606f6101aecdf00d98505621ed63d7fb3b9c8cb5f56a51c46acd86d68d33af1a", "class_name": "RelatedNodeInfo"}}, "text": "output values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead( Q, K, V ) = Concat(head 1, ...,head h)WO\nwhere head i= Attention( QWQ\ni, KWK\ni, V WV\ni)\nWhere the projections are parameter matrices WQ\ni\u2208Rdmodel\u00d7dk,WK\ni\u2208Rdmodel\u00d7dk,WV\ni\u2208Rdmodel\u00d7dv\nandWO\u2208Rhdv\u00d7dmodel.\nIn this work we employ h= 8 parallel attention layers, or heads. For each of these we use\ndk=dv=dmodel/h= 64 . Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3 Applications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN( x) = max(0 , xW 1+b1)W2+b2 (2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512 , and the inner-layer has dimensionality\ndff= 2048 .\n3.4 Embeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [ 30]. In the embedding layers, we multiply those weights by\u221admodel.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3169, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ebcabc4-cd46-493c-863b-42dd322f897b": {"__data__": {"id_": "8ebcabc4-cd46-493c-863b-42dd322f897b", "embedding": null, "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bccce577-e3a8-4300-b8c0-3c96be96b047"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "946a0243-320b-4693-980b-8f2487746a62", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bccce577-e3a8-4300-b8c0-3c96be96b047"}, "hash": "6dac533186a3f659923bb09a33dc2a847731f5671a1a27e3eb4b13c08d6b7ebb", "class_name": "RelatedNodeInfo"}}, "text": "Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. nis the sequence length, dis the representation dimension, kis the kernel\nsize of convolutions and rthe size of the neighborhood in restricted self-attention.\nLayer Type Complexity per Layer Sequential Maximum Path Length\nOperations\nSelf-Attention O(n2\u00b7d) O(1) O(1)\nRecurrent O(n\u00b7d2) O(n) O(n)\nConvolutional O(k\u00b7n\u00b7d2) O(1) O(logk(n))\nSelf-Attention (restricted) O(r\u00b7n\u00b7d) O(1) O(n/r)\n3.5 Positional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i)=sin(pos/100002i/d model)\nPE(pos,2i+1)=cos(pos/100002i/d model)\nwhere posis the position and iis the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0to10000 \u00b72\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k,PEpos+kcan be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [ 9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4 Why Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., x n)to another sequence of equal length (z1, ..., z n), with xi, zi\u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [ 12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n)sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3448, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77fd6bf5-07f0-4d57-b737-58e66fdd1501": {"__data__": {"id_": "77fd6bf5-07f0-4d57-b737-58e66fdd1501", "embedding": null, "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_369aed3a-419f-4f0c-98ee-952ef37f2af5"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d3adebe1-0b0e-4af3-99d9-acd66ee11ceb", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_369aed3a-419f-4f0c-98ee-952ef37f2af5"}, "hash": "7d36f7f23e4a10684e4343806be82dc4b265e77205d8500e5a9bd38a295d2be2", "class_name": "RelatedNodeInfo"}}, "text": "length nis smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [ 31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size rin\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k)convolutional layers in the case of contiguous kernels,\norO(logk(n))in the case of dilated convolutions [ 18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [ 6], however, decrease the complexity\nconsiderably, to O(k\u00b7n\u00b7d+n\u00b7d2). Even with k=n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5 Training\nThis section describes the training regime for our models.\n5.1 Training Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [ 3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [ 38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2 Hardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3 Optimizer\nWe used the Adam optimizer [ 20] with \u03b21= 0.9,\u03b22= 0.98and\u03f5= 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate =d\u22120.5\nmodel\u00b7min(step_num\u22120.5, step _num\u00b7warmup _steps\u22121.5) (3)\nThis corresponds to increasing the learning rate linearly for the first warmup _steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup _steps = 4000 .\n5.4 Regularization\nWe employ three types of regularization during training:\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3305, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "61e16bc0-bc65-4de0-a789-040f618f55b3": {"__data__": {"id_": "61e16bc0-bc65-4de0-a789-040f618f55b3", "embedding": null, "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bf2f1a27-7264-4ee8-b23c-a4fd4c23b3c0"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "498a0a43-3e14-4aad-8bd9-1fa4da3532c9", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bf2f1a27-7264-4ee8-b23c-a4fd4c23b3c0"}, "hash": "4a53e914ecc63432997b0701095afa298986d006bf92d746bdae29f0118d69da", "class_name": "RelatedNodeInfo"}}, "text": "Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModelBLEU Training Cost (FLOPs)\nEN-DE EN-FR EN-DE EN-FR\nByteNet [18] 23.75\nDeep-Att + PosUnk [39] 39.2 1.0\u00b71020\nGNMT + RL [38] 24.6 39.92 2.3\u00b710191.4\u00b71020\nConvS2S [9] 25.16 40.46 9.6\u00b710181.5\u00b71020\nMoE [32] 26.03 40.56 2.0\u00b710191.2\u00b71020\nDeep-Att + PosUnk Ensemble [39] 40.4 8.0\u00b71020\nGNMT + RL Ensemble [38] 26.30 41.16 1.8\u00b710201.1\u00b71021\nConvS2S Ensemble [9] 26.36 41.29 7.7\u00b710191.2\u00b71021\nTransformer (base model) 27.3 38.1 3.3\u00b71018\nTransformer (big) 28.4 41.8 2.3\u00b71019\nResidual Dropout We apply dropout [ 33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop= 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls= 0.1[36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6 Results\n6.1 Machine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5days on 8P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop= 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4and length penalty \u03b1= 0.6[38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU5.\n6.2 Model Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3149, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d77ebf21-4520-4290-a7ab-1e26f432afc3": {"__data__": {"id_": "d77ebf21-4520-4290-a7ab-1e26f432afc3", "embedding": null, "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f198431f-8c1c-4c38-8671-f4ff4a5c373b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9fd7324b-270e-417d-af59-c5120c5ec543", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f198431f-8c1c-4c38-8671-f4ff4a5c373b"}, "hash": "d975dd611f362d85680ace60436cbef581a04818929ec81ca0c1ae1bbe2b3148", "class_name": "RelatedNodeInfo"}}, "text": "Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN d model dff h d k dvPdrop \u03f5lstrain PPL BLEU params\nsteps (dev) (dev) \u00d7106\nbase 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65\n(A)1 512 512 5.29 24.9\n4 128 128 5.00 25.5\n16 32 32 4.91 25.8\n32 16 16 5.01 25.4\n(B)16 5.16 25.1 58\n32 5.01 25.4 60\n(C)2 6.11 23.7 36\n4 5.19 25.3 50\n8 4.88 25.5 80\n256 32 32 5.75 24.5 28\n1024 128 128 4.66 26.0 168\n1024 5.12 25.4 53\n4096 4.75 26.2 90\n(D)0.0 5.77 24.6\n0.2 4.95 25.5\n0.0 4.67 25.3\n0.2 5.47 25.7\n(E) positional embedding instead of sinusoids 4.92 25.7\nbig 6 1024 4096 16 0.3 300K 4.33 26.4 213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dkhurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [ 9], and observe nearly identical\nresults to the base model.\n6.3 English Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [ 25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2969, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc3f5579-5a90-4b4a-94da-6e525fdab5ba": {"__data__": {"id_": "bc3f5579-5a90-4b4a-94da-6e525fdab5ba", "embedding": null, "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_91123771-4b87-4a1e-84c8-1c65d21c2e8b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f24fdaf-2516-4867-b76f-fd94516ced8e", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_91123771-4b87-4a1e-84c8-1c65d21c2e8b"}, "hash": "d96ce698dd9748be0c65039d8f6b64ecb43c02f43b673e3d0eecde551e2a8657", "class_name": "RelatedNodeInfo"}}, "text": "Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser Training WSJ 23 F1\nVinyals & Kaiser el al. (2014) [37] WSJ only, discriminative 88.3\nPetrov et al. (2006) [29] WSJ only, discriminative 90.4\nZhu et al. (2013) [40] WSJ only, discriminative 90.4\nDyer et al. (2016) [8] WSJ only, discriminative 91.7\nTransformer (4 layers) WSJ only, discriminative 91.3\nZhu et al. (2013) [40] semi-supervised 91.3\nHuang & Harper (2009) [14] semi-supervised 91.3\nMcClosky et al. (2006) [26] semi-supervised 92.1\nVinyals & Kaiser el al. (2014) [37] semi-supervised 92.1\nTransformer (4 layers) semi-supervised 92.7\nLuong et al. (2015) [23] multi-task 93.0\nDyer et al. (2016) [8] generative 93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21and\u03b1= 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [ 37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7 Conclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor .\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450 , 2016.\n[2]Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR , abs/1409.0473, 2014.\n[3]Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V . Le. Massive exploration of neural\nmachine translation architectures. CoRR , abs/1703.03906, 2017.\n[4]Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733 , 2016.\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3111, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "55281e51-de8e-4f60-adac-9db5fd0abe0b": {"__data__": {"id_": "55281e51-de8e-4f60-adac-9db5fd0abe0b", "embedding": null, "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_22cb5d7e-4021-4bf6-af42-fc38f49f5e9d"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d08a5b73-885a-4e44-82de-25d5bf880e7b", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_22cb5d7e-4021-4bf6-af42-fc38f49f5e9d"}, "hash": "b4f1c602876a8b8c03eaa8f20e6838a2b33e99382a5d4739f113cceb92540616", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "58bd27ca-5842-4807-943a-869aa89e462f", "node_type": "1", "metadata": {}, "hash": "5c324c92881deeadad98bc8e846f5cf167de2b03ac1875de2e36985196a2bf69", "class_name": "RelatedNodeInfo"}}, "text": "[5]Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR , abs/1406.1078, 2014.\n[6]Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357 , 2016.\n[7]Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR , abs/1412.3555, 2014.\n[8]Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL , 2016.\n[9]Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2 , 2017.\n[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850 , 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition , pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation ,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing , pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410 , 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS) , 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR) , 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2 ,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V .", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2920, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "58bd27ca-5842-4807-943a-869aa89e462f": {"__data__": {"id_": "58bd27ca-5842-4807-943a-869aa89e462f", "embedding": null, "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_22cb5d7e-4021-4bf6-af42-fc38f49f5e9d"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d08a5b73-885a-4e44-82de-25d5bf880e7b", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_22cb5d7e-4021-4bf6-af42-fc38f49f5e9d"}, "hash": "b4f1c602876a8b8c03eaa8f20e6838a2b33e99382a5d4739f113cceb92540616", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "55281e51-de8e-4f60-adac-9db5fd0abe0b", "node_type": "1", "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_22cb5d7e-4021-4bf6-af42-fc38f49f5e9d"}, "hash": "60eb93a17d323e5be4b3b1ccccdf5fd81e6101a7ef51cbae4e9d3ec76d26be26", "class_name": "RelatedNodeInfo"}}, "text": "[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nInInternational Conference on Learning Representations , 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722 , 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130 , 2017.\n[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114 , 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025 , 2015.\n11", "mimetype": "text/plain", "start_char_idx": 2317, "end_char_idx": 3229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9e816d6-31f4-4317-8ead-45f7492d2385": {"__data__": {"id_": "d9e816d6-31f4-4317-8ead-45f7492d2385", "embedding": null, "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2087f73c-a1f1-42f8-8f28-7993757a85d7"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3af6ca8e-7e4e-47db-bdfe-6741e98c3914", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2087f73c-a1f1-42f8-8f28-7993757a85d7"}, "hash": "8ff8705a6ade8dd281192e65e56c9d2150455d1c6d77fe1971ab3f68bb4768c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4feaa313-6415-4226-ae75-7e71941e5bd2", "node_type": "1", "metadata": {}, "hash": "55699062aafd1f351a468566d7f315a8e413e47b8990bb48d6c0aa29b22deaf0", "class_name": "RelatedNodeInfo"}}, "text": "[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics , 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference ,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing , 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304 , 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL , pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859 , 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909 , 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538 , 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research , 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28 , pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems , pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR , abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2993, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4feaa313-6415-4226-ae75-7e71941e5bd2": {"__data__": {"id_": "4feaa313-6415-4226-ae75-7e71941e5bd2", "embedding": null, "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2087f73c-a1f1-42f8-8f28-7993757a85d7"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3af6ca8e-7e4e-47db-bdfe-6741e98c3914", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2087f73c-a1f1-42f8-8f28-7993757a85d7"}, "hash": "8ff8705a6ade8dd281192e65e56c9d2150455d1c6d77fe1971ab3f68bb4768c3", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d9e816d6-31f4-4317-8ead-45f7492d2385", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2087f73c-a1f1-42f8-8f28-7993757a85d7"}, "hash": "96b4d30367d24e85800a0f46eac2156bdb3568e24fba92508cff96465685a2f4", "class_name": "RelatedNodeInfo"}}, "text": "[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems , 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144 , 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR , abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers) , pages 434\u2013443. ACL, August 2013.\n12", "mimetype": "text/plain", "start_char_idx": 2380, "end_char_idx": 3229, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a0d5d11-faa7-4ac6-813b-15fdc8145bd6": {"__data__": {"id_": "8a0d5d11-faa7-4ac6-813b-15fdc8145bd6", "embedding": null, "metadata": {"page_label": "13", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ef2b7488-4de2-4520-82a1-216f82467c41"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8a2697e0-fed1-497f-a593-185353a9ddea", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ef2b7488-4de2-4520-82a1-216f82467c41"}, "hash": "d7d6727ede3d68c10a7b891b453a06ee6482c036e123cd2725da6d550c5c2a5e", "class_name": "RelatedNodeInfo"}}, "text": "Attention Visualizations\nInput-Input Layer5\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7dcd8ff1-d1af-493e-b40f-4bf5cc3e859a": {"__data__": {"id_": "7dcd8ff1-d1af-493e-b40f-4bf5cc3e859a", "embedding": null, "metadata": {"page_label": "14", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f024fd4d-d59b-444b-b845-58f096f73a24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8f5a4a0b-6113-41ce-9dbf-bc9fc9314320", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f024fd4d-d59b-444b-b845-58f096f73a24"}, "hash": "e556e86bc30fcff6513743b816243c9bd03c90eb63302f27f5d4211ab126f9f2", "class_name": "RelatedNodeInfo"}}, "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 814, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a7c4884-ddab-45aa-a83d-76f52219974d": {"__data__": {"id_": "3a7c4884-ddab-45aa-a83d-76f52219974d", "embedding": null, "metadata": {"page_label": "15", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e82d7109-c339-4db6-80e7-68a52eee8df5"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "87285a91-9ba5-40a7-9dfa-84198b5909aa", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e82d7109-c339-4db6-80e7-68a52eee8df5"}, "hash": "c3f363b0dca6f380ff8d2aacdaa102ec912f7b9d71d130ac012ec87f2605e50d", "class_name": "RelatedNodeInfo"}}, "text": "Input-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nInput-Input Layer5\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>Figure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 817, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "933b6e1d-70c8-49ab-a6f4-2e1a5998251c": {"__data__": {"id_": "933b6e1d-70c8-49ab-a6f4-2e1a5998251c", "embedding": null, "metadata": {"page_label": "1", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4f483482-e129-4cf7-b2df-2712db8ecb61"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85a0df51-845b-4fdb-a435-da3523d5bfb1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4f483482-e129-4cf7-b2df-2712db8ecb61"}, "hash": "59182b6e893fa74edaa3ae4e47c64aa12ebc1860352a3340013b3ffa22bac584", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7e6ca033-72d3-40a5-b249-98bb915ed504", "node_type": "1", "metadata": {}, "hash": "a086d55eb3eb5f5a378ae0fd26e2c74042ee4ac38cd7b72984bc334f2a3c34fd", "class_name": "RelatedNodeInfo"}}, "text": "CLEAR: Character Unlearning in Textual and Visual Modalities\nAlexey Dontsov1,6, Dmitrii Korzh1,3, Alexey Zhavoronkin2,4,\nBoris Mikheev3,Denis Bobkov1,6,Aibek Alanov1,6,\nOleg Y. Rogov1,3,5,Ivan Oseledets1,3,Elena Tutubalina1,6\n1AIRI2MIPT3Skoltech4Sber5University of Sharjah6HSE University\nCorrespondence: dontsov@airi.net; tutubalina@airi.net\nAbstract\nMachine Unlearning (MU) is critical for en-\nhancing privacy and security in deep learn-\ning models, particularly in large multimodal\nlanguage models (MLLMs), by removing spe-\ncific private or hazardous information. While\nMU has made significant progress in textual\nand visual modalities, multimodal unlearning\n(MMU) remains significantly underexplored,\npartially due to the absence of a suitable open-\nsource benchmark. To address this, we intro-\nduce CLEAR, a new benchmark designed to\nevaluate MMU methods. CLEAR contains 200\nfictitious individuals and 3,700 images linked\nwith corresponding question-answer pairs, en-\nabling a thorough evaluation across modalities.\nWe assess 10 MU methods, adapting them for\nMMU, and highlight new challenges specific\nto multimodal forgetting. We also demonstrate\nthat simple \u21131regularization on LoRA weights\nsignificantly mitigates catastrophic forgetting,\npreserving model performance on retained\ndata. The dataset is available at https://\nhuggingface.co/datasets/therem/CLEAR .\n1 Introduction\nLarge Language Models (LLMs) (Touvron et al.,\n2023; Jiang et al., 2023) are trained on vast corpora\nof data that contain private, unethical, or unwanted\ninformation, leading to growing concerns. Machine\nunlearning (MU) methods have been developed to\nremove such unwanted data without expensive re-\ntraining from scratch. For instance, MU has been\napplied for the LLMs to mitigate issues related to\ntoxicity (Lu et al., 2022), copyright and privacy\nconcerns (Jang et al., 2022; Eldan and Russinovich,\n2023; Wu et al., 2023) and fairness (Yu et al., 2023).\nAdditionally, such topics as model editing (Ilharco\net al., 2022; Zhang et al., 2023), prevention of hal-\nlucinations (Yao et al., 2023), and sensitive knowl-\nedge exposure (Barrett et al., 2023) have also moti-\nvated the development of MU techniques.\nReal faces\nQ: The name of person on \nthe photo is...\nA: Taylor Swift.\nQ: What is happening in \nthe image?\nA: Jaime Vasquez, a true \ncrime author, poses for a portrait.\nQ: What is happening in the image?\nA: Takashi Nakamura, with \na red rose and Mount Fuji in the background.\nReal world\nQ: In what direction are these cats eyes pointed towards?\nA: The cat is looking \nupwards.\nRetain Forget\nFigure 1: The overview of our dataset.\nThere are various unlearning techniques suitable\nsolely for LLMs (Yao et al., 2024b,a; Xing et al.,\n2024; Zhang et al., 2024) or for vision models (Li\net al., 2024a; Chen and Yang, 2023; Tarun et al.,\n2021). However, multimodal LLMs (MLLMs) (Liu\net al., 2023), specifically visual LLMs (VLLMs),\nraise new challenges. The unlearning of such multi-\nmodal models (MMU) remains largely unexplored,\nprimarily due to the lack of open-source bench-\nmarks. Moreover, current MU benchmarks (Maini\net al., 2024; Shi et al., 2024; Yao et al., 2024a; Li\net al., 2024b) are focused on single modalities, and,\nto our knowledge, no open benchmarks designed\nexplicitly for evaluating unlearning in multimodal\nmodels exist at the time of submission.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3352, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7e6ca033-72d3-40a5-b249-98bb915ed504": {"__data__": {"id_": "7e6ca033-72d3-40a5-b249-98bb915ed504", "embedding": null, "metadata": {"page_label": "1", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4f483482-e129-4cf7-b2df-2712db8ecb61"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "85a0df51-845b-4fdb-a435-da3523d5bfb1", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4f483482-e129-4cf7-b2df-2712db8ecb61"}, "hash": "59182b6e893fa74edaa3ae4e47c64aa12ebc1860352a3340013b3ffa22bac584", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "933b6e1d-70c8-49ab-a6f4-2e1a5998251c", "node_type": "1", "metadata": {"page_label": "1", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4f483482-e129-4cf7-b2df-2712db8ecb61"}, "hash": "1fa4370a0345d0af9bed9f6882cdc5c99940b0d912b561a940f55c3ec256b1a9", "class_name": "RelatedNodeInfo"}}, "text": "However, multimodal LLMs (MLLMs) (Liu\net al., 2023), specifically visual LLMs (VLLMs),\nraise new challenges. The unlearning of such multi-\nmodal models (MMU) remains largely unexplored,\nprimarily due to the lack of open-source bench-\nmarks. Moreover, current MU benchmarks (Maini\net al., 2024; Shi et al., 2024; Yao et al., 2024a; Li\net al., 2024b) are focused on single modalities, and,\nto our knowledge, no open benchmarks designed\nexplicitly for evaluating unlearning in multimodal\nmodels exist at the time of submission.\nTo address this gap, we propose CLEAR, a new\n1arXiv:2410.18057v1  [cs.CV]  23 Oct 2024", "mimetype": "text/plain", "start_char_idx": 2828, "end_char_idx": 3439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d979b58-50c8-430a-9495-0a300f5611c0": {"__data__": {"id_": "6d979b58-50c8-430a-9495-0a300f5611c0", "embedding": null, "metadata": {"page_label": "2", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_23c8cb65-a679-49dc-99ff-597ebbfd7e25"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d31c81d4-dfb9-4f5e-81c0-a5c43d4dfe8e", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_23c8cb65-a679-49dc-99ff-597ebbfd7e25"}, "hash": "14b85c0c79c00c5a6f436a156d44a3d2593a1045013e2def86a7524406539e05", "class_name": "RelatedNodeInfo"}}, "text": "Data\nModelEvaluation\nMiltimiodal\nIDK\nSCRUB\nKL1\n2\n3\nTextual\nVisualUnlearning\nUnlearnperson #1#1\n#NName: Aurelio Beltran\nAge: 39\nBirthplace: Mexico City, \nMexico\nGenre: True Crime\nBooks: \"The Bloody Blue-\nprint\u201d, \"No SOS for Guilt\", \nand \"Beneath the City of\nSin\".\nName: Rhoda Mbalazi\nAge: 68\nBirthplace: Dar es Salaam, \nTanzania\nGenre: War genre\nBooks: \u201cThe Battle of \nUnsaid Words\u201d, \u201cShadows on \nthe Barracks\u201d, \u201cThe\nSoldier's Silence\u201d.U: U: Who is Aure-\nlio Beltran?Who is this?\nModel:I don\u2019t \nknow.\nI don\u2019t \nknow.Model:\nIDK\nDPO\nSCRUB1\n2\n3\nSCRUB\nTWINS\nRMU1\n2\n3\nModelFigure 2: Summary of our dataset. We generate 200 persons and use multimodal unlearning to forget the part of\nthem. After, we measure the unlearning quality and the models\u2019 capabilities by calculating a set of metrics. Then,\nwe create a leaderboard of unlearning methods based on these metrics.\nbenchmark for textual-visual MMU named, focus-\ning on person unlearning, which aligns with the\nright-to-be-forgotten concept. The dataset is syn-\nthetic to ensure control over the data the model\nlearns, preventing object leakage during training.\nWe generated consistent images through a compre-\nhensive strategy and linked them to the correspond-\ning author-related questions from the large-scale\ntextual unlearning benchmark TOFU (Maini et al.,\n2024). The proposed dataset contains 200 ficti-\ntious authors, 3,770 visual question-answer pairs,\nand 4,000 textual question-answer pairs, enabling a\nthorough evaluation of the single and multi-modal\nunlearning techniques. We also propose a bench-\nmark to assess MU and MMU methods, evaluating\n10 techniques, including the current state of the art.\nWe evaluate the unlearning methods in textual,\nvisual, and multimodal setups. First, we finetune\nthe model on our entire dataset. Next, we take a\nsmall predefined subset of 20 authors, referred to\nas the forget set , while the remaining data forms\ntheretain set . We then apply the unlearning pro-\ncedure, resulting in a new model that no longer\n\"remembers\" individuals from the forget set but\nretains knowledge of those in the retain set. To en-\nsure the model\u2019s capabilities are not compromised\nduring unlearning, we assess its performance on\nreal-world tasks, such as celebrity face recogni-\ntion and general domain visual question-answering\n(VQA). Overall, we work with four sets: Forget,\nRetain, Real Faces, and Real World. Figure 1 illus-\ntrates an example from each set.\nWe evaluate existing unlearning methods sepa-\nrately in textual and vision modalities and thencombine them within an MLLM. We create a\nleaderboard for each domain, highlighting that mul-\ntimodal unlearning poses new challenges.\nFinally, we demonstrate that applying \u21131regular-\nization to the LoRA adapter during the unlearning\nprocess significantly improves performance, help-\ning to prevent catastrophic forgetting of the retain\nset information. Overall, our contributions can\nbe summarized as follows:\n\u2022We propose a novel benchmark, CLEAR, for\nevaluating machine unlearning in multimodal\n(textual-visual) setups. To the best of our\nknowledge, this is the first publicly available\nMMU benchmark.\n\u2022We comprehensively evaluate existing un-\nlearning methods across separate and com-\nbined domains. We construct leaderboards\nfor these three domains and show that state-\nof-the-art unlearning algorithms struggle in\nmultimodal setups, highlighting the need for\nnew approaches.\n\u2022We demonstrate that the \u21131weight regulariza-\ntion on the LoRA adapter helps to improve\nunlearning quality by significantly preventing\ncatastrophic forgetting.\n2 Related Work\n2.1 MU Methods and Textual Benchmarks\nMU methods (Cao and Yang, 2015; Dwork et al.,\n2014; Kurmanji et al., 2024; Neel et al., 2021;\nSekhari et al., 2021) remove the impact of certain\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3767, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "434584a4-d097-4253-8a26-2c2e7a527ee7": {"__data__": {"id_": "434584a4-d097-4253-8a26-2c2e7a527ee7", "embedding": null, "metadata": {"page_label": "3", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4d48b1f3-b680-4fbe-9e58-9caae469e37f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46526b9f-1f3a-4e70-b3fa-20180f9d4314", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4d48b1f3-b680-4fbe-9e58-9caae469e37f"}, "hash": "0dc6ba7fb28c11d68d7c461b7954f1c86c2f1f70ddf40340ccc38b20110c28db", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8129a709-e3f8-4da2-ab8f-53a3433a0503", "node_type": "1", "metadata": {}, "hash": "846cce6b2158cd966fdbfa12296aae86e55f1e13ddb425f0e6ba7cd7d5254c74", "class_name": "RelatedNodeInfo"}}, "text": "data instances from a trained model without requir-\ning full retraining. The goal is to obtain a model\nthat behaves like the forget data was never part\nof the training set. MU can be formalized in two\nmain ways. In a setting where the unlearned model\nmust produce identical outputs to a model trained\nwithout the forget data. In an inexact unlearning set-\nting, the main objective is to obtain any model that\ndoesn\u2019t contain knowledge from the forget set, but\nwith no restrictions and guarantees on the model\u2019s\nresponse to inputs from the retain set.\nThere are several standard textual unlearning\nbenchmarks. TOFU (Maini et al., 2024) is a bench-\nmark for textual LLM unlearning, featuring 200\nfictitious author profiles, each defined by attributes\nsuch as name, birthplace, parent\u2019s names and occu-\npation, written books, etc. Totally the dataset has\n4,000 question-answer pairs (20 per author). Prede-\nfined 10/90, 5/95, and 1/99 splits are used for for-\nget/retain sets. Models are finetuned on the entire\ndataset, and unlearning is applied to the forget pairs.\nWMDP (Li et al., 2024b) consists of 3,668 multiple-\nchoice questions, evaluates hazardous knowledge\nof LLMs, and serves as a benchmark for unlearning\nsuch dangerous information. In (Yao et al., 2024a),\nthe authors explore seven textual unlearning tech-\nniques. The best approach was a combination of\ngradient ascent with gradient descent on the forget\nand retain sets, respectively, to maintain the un-\nlearn/retain quality trade-off. They evaluated their\nmethods on three benchmark datasets from differ-\nent domains: 500 arXiv papers, 2000 GitHub files,\nand 100 books covering academic texts, code, and\nliterary works. Unfortunately, these three bench-\nmarks cannot be applied to the MMU evaluation.\n2.2 MMU Methods and Benchmarks\nMLLMs (Liu et al., 2023) typically consist of three\ncore components: a modality encoder that trans-\nlates raw input into feature embeddings, a modality\nprojection layer aligning these features within the\nlanguage space, and a pre-trained language model\nsynthesizing the final output.\nNonetheless, MMU research is still in its early\nstages. (Cheng and Amiri, 2023) proposed the\nMultiDelete method, which focuses on separating\ncross-modal embeddings for the forget set while\npreserving unimodal embeddings for the retain set.\nUnfortunately, this approach is suitable only for\nthe encoder-decoder architecture and can not be\ndirectly transferred to decoder-only LLMs.EFUF (Xing et al., 2024) mitigates hallucina-\ntions in MLLMs using unlearning. It measures the\nsimilarity between generated captions and image\ncontent with CLIP model (Radford et al., 2021)\nto automatically detect hallucinated (negative) and\nnon-hallucinated (positive) examples based on the\ncalibrated on the MSCOCO dataset (Lin et al.,\n2014) thresholds, eliminating manual labeling. The\nunlearning process applies three loss functions:\nnegative loss to forget hallucinations, positive loss\nto reinforce correct representations, and sentence\nloss to maintain fluency. However, their benchmark\nis not open-sourced.\nSingle Image Unlearning (SIU) (Li et al., 2024a)\nfocuses on unlearning visual concepts in MLLMs\nwhile preserving textual knowledge and introduces\nMMUBench with five evaluation metrics. The\nbenchmark covers 20 concepts with at least 50 im-\nages for the concept, including real-world figures\nand cartoon characters. For each concept, one im-\nage is selected as the forget subset, paired with\nvarious prompts, while the remaining images form\nthe retain subset. However, SIU\u2019s use of a single\nimage raises scalability concerns for complex con-\ncepts, and their unlearning of VLLM is limited to\nthe visual domain. Moreover, MMUBench is also\nnot open-sourced.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3729, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8129a709-e3f8-4da2-ab8f-53a3433a0503": {"__data__": {"id_": "8129a709-e3f8-4da2-ab8f-53a3433a0503", "embedding": null, "metadata": {"page_label": "3", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4d48b1f3-b680-4fbe-9e58-9caae469e37f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "46526b9f-1f3a-4e70-b3fa-20180f9d4314", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4d48b1f3-b680-4fbe-9e58-9caae469e37f"}, "hash": "0dc6ba7fb28c11d68d7c461b7954f1c86c2f1f70ddf40340ccc38b20110c28db", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "434584a4-d097-4253-8a26-2c2e7a527ee7", "node_type": "1", "metadata": {"page_label": "3", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4d48b1f3-b680-4fbe-9e58-9caae469e37f"}, "hash": "bb8aee01b3873933f6c0807f34f32a9351ae8107e756e9a659f85ef0037783b3", "class_name": "RelatedNodeInfo"}}, "text": "However, their benchmark\nis not open-sourced.\nSingle Image Unlearning (SIU) (Li et al., 2024a)\nfocuses on unlearning visual concepts in MLLMs\nwhile preserving textual knowledge and introduces\nMMUBench with five evaluation metrics. The\nbenchmark covers 20 concepts with at least 50 im-\nages for the concept, including real-world figures\nand cartoon characters. For each concept, one im-\nage is selected as the forget subset, paired with\nvarious prompts, while the remaining images form\nthe retain subset. However, SIU\u2019s use of a single\nimage raises scalability concerns for complex con-\ncepts, and their unlearning of VLLM is limited to\nthe visual domain. Moreover, MMUBench is also\nnot open-sourced.\n(Chakraborty et al., 2024) explores unlearning\nharmful content in VLLMs, demonstrating that un-\nlearning in the textual domain alone can match the\nperformance of text-image unlearning while using\nfewer resources. Their method combines harmful\nloss and KL divergence between unlearned and\nretrained models. The approach was tested on six\ndatasets, including PKU-SafeRLHF (Ji et al., 2024)\nand three vision-text attack datasets (Shayegani\net al., 2023; Luo et al., 2024; Gong et al., 2023)\nfor harmful content prompts, and Truthful-QA and\nVQA-v2 (Lin et al., 2021; Goyal et al., 2017) to\nensure benign task performance remained intact.\nHowever, their focus on safety alignment may limit\napplicability for general unlearning, such as bio-\nmetric privacy. While they claim that textual un-\nlearning is sufficient for MMU, our findings show\nthis is not true for all methods. Additionally, their\napproach lacks exact unlearning evaluation.\n3 Methodology\nUnlearning can be conceptualized in two main\nways: the objective of the Strict Unlearning is\nto achieve a model that behaves identically to one\n3", "mimetype": "text/plain", "start_char_idx": 3030, "end_char_idx": 4823, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "86b540ae-99af-4bef-8904-98b4dbc29483": {"__data__": {"id_": "86b540ae-99af-4bef-8904-98b4dbc29483", "embedding": null, "metadata": {"page_label": "4", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_eb9f5072-4f29-40e8-b2b3-8126f019b6b6"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39bc83ab-eb42-43c5-96d1-5a85037c19a0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_eb9f5072-4f29-40e8-b2b3-8126f019b6b6"}, "hash": "8aa4140ba475b61c9957886a6ed98ac1d30b77e9c535ebc6c30085d7738b00bc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7532052f-ef7a-461a-9cac-b72e5d2f8eb0", "node_type": "1", "metadata": {}, "hash": "381e9bd7b0fe1905412fd80d74a0fef8e2e36778d9f76bca5def7082031f03c9", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3: Examples of generated images showcasing a distinct individual from our dataset.\ntrained exclusively on the retain set, ensuring that\nno knowledge from the forget set is present; the\nobjective of the Inexact Unlearning is to produce\nanymodel that no longer contains information from\nthe forget set. However, this approach cannot guar-\nantee how the model will respond to inputs related\nto the forget set. These objectives require distinct\nmethods, loss functions, and evaluation metrics to\nassess unlearning quality.\nLetf\u03b8denote the original model with its param-\neters\u03b8. Source model f\u03b8is trained on train dataset\nD, and given the unlearning objective, we want\nto make our model forget a subset of the source\ndataset D, called forget set DF. The remaining\npart of the training dataset is called retain set, and\nwe aim to preserve the model\u2019s performance on\nthis data subset DR:=D\\DF. Additionally, we\nutilize a holdout set DHto establish a reference\nfor the model\u2019s desired behavior on DFafter the\nunlearning process. The model\u2019s training did not\ninclude this set, ensuring that DH\u2229D=\u2205. In a\nnutshell, forget set DFcontains samples the model\nshould unlearn and serves as a direct measure of un-\nlearning effectiveness; retain set DRcontains sam-\nples that the model should retain and perform well\non, serving as an indicator of the model\u2019s preserved\nknowledge; holdout set DHcontains samples that\nthe model has never seen before and serves as a\nreference for the model\u2019s behavior on data that was\nnot involved in the training process. Such forget-\nting procedure is performed by updating the model\nf\u03b8with a particular unlearning method, which re-\nsults in a new unlearned model f\u02c6\u03b8with parameters\n\u02c6\u03b8. For the evaluation, we can also train separately\nagold model g\u03c9,which is trained only on the DR.\nSo the objective is to obtain the unlearned model\nf\u02c6\u03b8which unlearns the forget set DFwhile pre-\nserving the performance on retain set DRas good\nas the source model f\u03b8. It can be done by opti-\nmizing (minimizing or maximizing) the specific\ncriterion with the model parameters \u03b8and resulting\nobtained parameters \u02c6\u03b8will be the parameters of\ndesired unlearned model f\u02c6\u03b8. For example, one can\nconsider the gradient difference MU approach forthe optimization, aimed at increasing forget loss\nand maintaining retain performance:\n\u02dcL=\u2212X\nxi\u2208DfL(xi, yi, \u03b8) +\u03bbX\nxj\u2208DRL(xj, yj, \u03b8)\n(1)\n\u03b87\u2192\u03b8\u2212\u03b1\u2207\u03b8\u02dcL, (2)\nwhere \u03bb\u2013 forget-retain trade-off hyper-parameter,\n\u03b1\u2013 learning rate, Lis a loss function, for example,\nnegative-log-likelihood. xis the input \u2013 text, image,\nor both of them in the case of VLLM.\nIn this work, we explore a subset of unlearn-\ning methods, including Retain Finetune, Gradi-\nent Ascend (GA), Gradient Difference (GD) (Liu\net al., 2022), SCRUB (Kurmanji et al., 2023), DPO\n(Rafailov et al., 2023), NPO (Zhang et al., 2024),\nLLMU (Yao et al., 2024b) , IDK (Maini et al.,\n2024), RMU (Li et al., 2024b), and KL (Golatkar\net al., 2020), which are detailed in Appendix A.We\nselected these methods based on their ease of adap-\ntation to new modalities, requiring only changes in\ninput data (text, images, or both) while preserving\ntheir core functionality. In essence, these meth-\nods involve variations of hard negative training on\nthe forget set combined with fine-tuning on the\nretain set, often with additional constraints such\nas Cross-Entropy or KL divergence to align the\nmodel\u2019s outputs on the retain set with the original\nmodel.\n4 CLEAR\nThe MU (and consequently MMU) benchmark\nshould ideally avoid running unlearning on well-\nknown information that could be obtained from\nexternal sources such as books, games, movies,\netc.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7532052f-ef7a-461a-9cac-b72e5d2f8eb0": {"__data__": {"id_": "7532052f-ef7a-461a-9cac-b72e5d2f8eb0", "embedding": null, "metadata": {"page_label": "4", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_eb9f5072-4f29-40e8-b2b3-8126f019b6b6"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "39bc83ab-eb42-43c5-96d1-5a85037c19a0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_eb9f5072-4f29-40e8-b2b3-8126f019b6b6"}, "hash": "8aa4140ba475b61c9957886a6ed98ac1d30b77e9c535ebc6c30085d7738b00bc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "86b540ae-99af-4bef-8904-98b4dbc29483", "node_type": "1", "metadata": {"page_label": "4", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_eb9f5072-4f29-40e8-b2b3-8126f019b6b6"}, "hash": "5994c0d934891cb3526f2a3c4cc38a7e9a2ffc96cb8c1a896836017630ee068d", "class_name": "RelatedNodeInfo"}}, "text": "In essence, these meth-\nods involve variations of hard negative training on\nthe forget set combined with fine-tuning on the\nretain set, often with additional constraints such\nas Cross-Entropy or KL divergence to align the\nmodel\u2019s outputs on the retain set with the original\nmodel.\n4 CLEAR\nThe MU (and consequently MMU) benchmark\nshould ideally avoid running unlearning on well-\nknown information that could be obtained from\nexternal sources such as books, games, movies,\netc. This is essential for a more reliable evaluation\nof the model\u2019s performance on retain and forget.\nTo meet this requirement, we chose to extend the\nTOFU dataset due to its ease of use, flexibility for\nadopting to new modalities (such as adding face\nimages or personal voices), and its strong connec-\ntion to privacy concerns, making it ideal for testing\nunlearning in sensitive contexts.\n4", "mimetype": "text/plain", "start_char_idx": 3157, "end_char_idx": 4021, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e1e065a-6143-4f12-94df-1fe1b836a80e": {"__data__": {"id_": "5e1e065a-6143-4f12-94df-1fe1b836a80e", "embedding": null, "metadata": {"page_label": "5", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_713c6749-25af-4601-bcb6-6ef0ea87ea93"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02a04001-61fb-47f8-b44c-26bc83dba291", "node_type": "4", "metadata": {"page_label": "5", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_713c6749-25af-4601-bcb6-6ef0ea87ea93"}, "hash": "e90a0e7a3b9ffbe8772d15b3109b5f6ad8c41c56035a41b9299b0073c63dc451", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "92b5701e-3b44-4d50-8979-f61a2c937454", "node_type": "1", "metadata": {}, "hash": "90b2a0ef2220a804a066de44d21fe111612650442a0881cd1c893b00282db986", "class_name": "RelatedNodeInfo"}}, "text": "Figure 4: Distributions of the attributes of the author\u2019s\nfaces. We show that CLEAR is balanced and represen-\ntative regarding age, gender, and ethnicity.\n4.1 Dataset Generation Process\nFirstly, for each of the 200 authors from the TOFU\ndataset, we extract their name, age, and ethnicity\nbased on the knowledge provided in the original\ndataset. Also, we generate a pool of 2000 faces\nusing StyleGAN2 (Karras et al., 2020) - an estab-\nlished generative model for face synthesis. Each\nface is scored with pre-trained CNNs to get the age,\ngender, and ethnicity of the face. Then, we manu-\nally select the most suitable face that matches these\nattributes for each author. During this phase, we\ndiscovered that the age distribution of the authors\nwas highly shifted towards the older ages, so we\nneeded to eliminate this gap. To do this, we used\nthe image editing framework proposed in (Bobkov\net al., 2024) to shift the visual attributes of the\nfaces to make them older. The distribution of the\ncharacteristics of the faces and authors is shown\nin Figure 4. After matching each author to a face,\nwe used the diffusion model (Li et al., 2024c) of\npersonalized generation to synthesize images with\na given face and corresponding to a given prompt.\nIn detail, the face generation and collection process\nis described in Appendix B.\nThe diffusion model needs a textual prompt for\nimage generation besides the face. We ask GPT-4\nto generate images from a textual question and an\nanswer about an author. We generate 8 images for\neach prompt, evaluate them using an ensemble of\nfake-detection models, and select the most realistic.\nAdditionally, GPT-4o generates captions for each\nimage and visual prompt pair, which are then in-\ncluded in the dataset. However, due to restrictions\ncaused by GPT guard breaks and the identification\nof several bugs in the TOFU dataset (such as a\nnameless author), the final dataset includes fewer\nimages than text pairs (3,770 compared to 4,000).\nWe also incorporated two additional data splits con-\ntaining real-world face photos and natural imagesto maintain the model\u2019s visual capabilities during\nunlearning.\n4.2 Splits\nUltimately, we utilize the following four splits\n(sets) to evaluate unlearning:\nForget. Following methodology from (Maini et al.,\n2024), DFis made from data of 2, 10, and 20 per-\nsons (1%, 5% and 10% correspondingly) of the\nfull set D, consisting of 200 authors. This DFis\nexpected to be unlearned by the model.\nRetain. Retain set DRis made from all the other\ndata from the complete set D, not included in the\nDF. The model should continue to work well on\nthis subset and preserve its performance as much\nas possible.\nReal Faces. To ensure the model retains knowl-\nedge of related concepts, such as faces, which are\nnot present in the finetuning dataset, we evaluate\nit using a set of real-world faces. Specifically, we\nuse the MillionCelebs dataset (Zhang et al., 2020),\nwhich consists of celebrity face-name pairs. We in-\ntersect this dataset with the most recognized celebri-\nties from any year on the Forbes Celebrity 100 list\nto increase the likelihood that the model has seen\nthese faces during pre-training. This results in a\nfinal set of 150 face-name pairs.\nReal World. To ensure that the model\u2019s overall\nvisual capabilities remain intact throughout the un-\nlearning process, we evaluate its performance on\nthe Visual Question Answering (VQA) task using\nsamples from (x.ai, 2024).\nFigure 1 presents a random sample from each of\nthese splits.\n4.3 Evaluation Metrics\nTo comprehensively evaluate unlearning per-\nformance on textual, visual, and textual-visual\ndomains, we measure the MU and MMU perfor-\nmance in terms of the following metrics:\nROUGE-L We calculate the ROUGE-L (Lin,\n2004) score between the model\u2019s predictions and\nground truth answers. This metric measures how\nmuch the model remembers in exact formulations.\nHowever, the generation of models does not always\nrepresent the inner knowledge of one\u2019s.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3965, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "92b5701e-3b44-4d50-8979-f61a2c937454": {"__data__": {"id_": "92b5701e-3b44-4d50-8979-f61a2c937454", "embedding": null, "metadata": {"page_label": "5", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_713c6749-25af-4601-bcb6-6ef0ea87ea93"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "02a04001-61fb-47f8-b44c-26bc83dba291", "node_type": "4", "metadata": {"page_label": "5", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_713c6749-25af-4601-bcb6-6ef0ea87ea93"}, "hash": "e90a0e7a3b9ffbe8772d15b3109b5f6ad8c41c56035a41b9299b0073c63dc451", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e1e065a-6143-4f12-94df-1fe1b836a80e", "node_type": "1", "metadata": {"page_label": "5", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_713c6749-25af-4601-bcb6-6ef0ea87ea93"}, "hash": "a94fe0542108bb038f9e58dcbd3dd984e084da700865480c579ff07af93e6100", "class_name": "RelatedNodeInfo"}}, "text": "This results in a\nfinal set of 150 face-name pairs.\nReal World. To ensure that the model\u2019s overall\nvisual capabilities remain intact throughout the un-\nlearning process, we evaluate its performance on\nthe Visual Question Answering (VQA) task using\nsamples from (x.ai, 2024).\nFigure 1 presents a random sample from each of\nthese splits.\n4.3 Evaluation Metrics\nTo comprehensively evaluate unlearning per-\nformance on textual, visual, and textual-visual\ndomains, we measure the MU and MMU perfor-\nmance in terms of the following metrics:\nROUGE-L We calculate the ROUGE-L (Lin,\n2004) score between the model\u2019s predictions and\nground truth answers. This metric measures how\nmuch the model remembers in exact formulations.\nHowever, the generation of models does not always\nrepresent the inner knowledge of one\u2019s. That is\nwhere the next metric is used.\nProbability Score One way of exposing elicit\nknowledge from a model is through its logits,\nwhich are assigned to some factual tokens. We\ndefine the conditional probability p(y|x)1\n|y|for\ninput xand answer y(power1\n|y|corresponds to\n5", "mimetype": "text/plain", "start_char_idx": 3159, "end_char_idx": 4238, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d7f4b22c-88e2-4368-a647-11b90447bcef": {"__data__": {"id_": "d7f4b22c-88e2-4368-a647-11b90447bcef", "embedding": null, "metadata": {"page_label": "6", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_530d34b9-3785-4177-94c4-b5f775c5209c"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "592c25bb-04c1-4e15-8772-d7b481524bfc", "node_type": "4", "metadata": {"page_label": "6", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_530d34b9-3785-4177-94c4-b5f775c5209c"}, "hash": "e3a4d171ec3490bf744406e92cd2a502c05bf26db1ac26db3847b52d7b6b710b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bbaee324-9db7-4409-954b-90fd96e4c8b9", "node_type": "1", "metadata": {}, "hash": "e0ae67970444eb06fe64a5a6a5d2a71ed26a7a45cd9248a6e1034717d945d2da", "class_name": "RelatedNodeInfo"}}, "text": "normalizing for length). Each input question x\nis considered as a multiple choice question with\npossible answers y1, ..., y n, and then, assuming y1\nis the correct answer, desired probability score is\ncomputed asp(y1|x)\nnP\ni=1p(yi|x). It will be bounded between\n0 and 1. A lower probability score indicates\nthat the model is less confident about generating\ncontent.\nTruth Ratio quantifies the alignment between\npredictions and the ground truth by comparing the\nprobability of a paraphrased correct answer against\nthe averaged probabilities of several similarly for-\nmatted incorrect answers, providing insight into the\neffectiveness of the unlearning algorithm in remov-\ning specific information while maintaining overall\naccuracy. Assume that \u02c6ydenotes a paraphrased\nversion of the answer yfor input xandY\u2032is the set\nof 5 perturbations of the answer y. Then desired\ntruth ratio Ris calculated as:\nR=1\n|Y\u2032|P\ny\u2032\u2208Y\u2032p(y\u2032|x)1\n|y\u2032|\np(\u02c6y|x)1\n|\u02c6y|. (3)\nThis ratio is normalized and rescaled between 0\nand 1, with higher values indicating improved un-\nlearning.\nForget Quality. Measuring the quality of for-\ngetting in MU presents significant challenges. The\nobjective is to create a model that cannot be dis-\ntinguished from one trained solely on retain. The\nestablished (Hayes et al., 2024) way of measuring\nthe unlearning quality is calculating the U-LIRA\nscore. However, it requires training at least 128\nmodel copies, which is computationally expensive\nfor LLM. A feasible method for achieving this is\nproposed. We calculate a statistical test on the out-\nputs of two models: our unlearned model and the\ngold model. The Truth Ratio metric is considered\nas output for its effectiveness in informativeness.\nTo assess this metric, the Kolmogorov-Smirnov test\n(KS-Test) is employed to compare the distributions\nof Truth Ratios from both models. A high p-value\nfrom this test suggests effective forgetting, while a\nlow p-value indicates potential privacy leakage and\npoor unlearning. We call this p-value the Forget\nQuality of the unlearning method.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbaee324-9db7-4409-954b-90fd96e4c8b9": {"__data__": {"id_": "bbaee324-9db7-4409-954b-90fd96e4c8b9", "embedding": null, "metadata": {"page_label": "6", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_530d34b9-3785-4177-94c4-b5f775c5209c"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "592c25bb-04c1-4e15-8772-d7b481524bfc", "node_type": "4", "metadata": {"page_label": "6", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_530d34b9-3785-4177-94c4-b5f775c5209c"}, "hash": "e3a4d171ec3490bf744406e92cd2a502c05bf26db1ac26db3847b52d7b6b710b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d7f4b22c-88e2-4368-a647-11b90447bcef", "node_type": "1", "metadata": {"page_label": "6", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_530d34b9-3785-4177-94c4-b5f775c5209c"}, "hash": "c893ae585975679ec83178eb59ea7021aecf01f79e23eaf188f79dd56ae99484", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "30c9f378-e2ed-4dd9-9709-d28beb9e3505", "node_type": "1", "metadata": {}, "hash": "8f63d64ce185cb34e2f5bda41a65a7ecf127b084b47e2c0e0086f48758970556", "class_name": "RelatedNodeInfo"}}, "text": "The\nestablished (Hayes et al., 2024) way of measuring\nthe unlearning quality is calculating the U-LIRA\nscore. However, it requires training at least 128\nmodel copies, which is computationally expensive\nfor LLM. A feasible method for achieving this is\nproposed. We calculate a statistical test on the out-\nputs of two models: our unlearned model and the\ngold model. The Truth Ratio metric is considered\nas output for its effectiveness in informativeness.\nTo assess this metric, the Kolmogorov-Smirnov test\n(KS-Test) is employed to compare the distributions\nof Truth Ratios from both models. A high p-value\nfrom this test suggests effective forgetting, while a\nlow p-value indicates potential privacy leakage and\npoor unlearning. We call this p-value the Forget\nQuality of the unlearning method.\nAlso, we define Real, Retain, and Forget metrics\nas a harmonical mean of ROUGE, Real Probability\nscore, and Truth Ratio.M MethodReal\nMetric \u2191Retain\nMetric \u2191Forget\nMetric \u2193Log Forget\nQuality \u2191LLama2-7BRetain FT 0.50 0.26 0.42 -4.92\nLLMU 0.38 0.03 0.01 -2.31\nKL 0.24 0.00 0.00 -18.22\nGA 0.25 0.00 0.00 -17.22\nGD 0.61 0.13 0.01 -48.59\nIDK 0.46 0.26 0.24 -4.92\nDPO 0.50 0.26 0.42 -4.92\nSCRUB 0.50 0.26 0.42 -4.92\nRMU 0.51 0.26 0.59 -42.86\nNPO 0.50 0.28 0.62 -44.46Mistral-7BRetain FT 0.67 0.34 0.47 -3.87\nLLMU 0.65 0.30 0.39 -6.69\nKL 0.28 0.00 0.00 -50.30\nGA 0.26 0.00 0.00 -36.06\nGD 0.60 0.01 0.00 -51.16\nIDK 0.63 0.32 0.45 -2.72\nDPO 0.67 0.33 0.47 -3.63\nSCRUB 0.66 0.33 0.47 -3.39\nRMU 0.09 0.00 0.00 -123.22\nNPO 0.67 0.33 0.47 -3.16\nTable 1: Unlearning methods on textual domain only.\nThe gray color represents a low retain metric, indicating\nthe method diverges. Hence, we do not consider them.\nMethodForget\nAcc.\u2193Holdout\nAcc.\u2191Retain\nAcc.\u2191U-LIRA \u2193U-MIA \u2193\nOriginal 100.00 18.50 100.00 1.00 0.96\nGold 15.43 15.04 97.52 0.50 0.50\nRetain FT 100.00 18.54 100.00 1.00 0.92\nSCRUB 99.74 16.77 99.93 0.98 0.90\nLLMU 85.72 14.62 88.99 0.83 0.75\nRMU 67.97 17.27 99.99 0.77 0.60\nDPO 50.21 13.93 81.49 0.73 0.62\nSCRUB bio 42.59 14.25 99.44 0.71 0.57\nSparsity 66.41 14.44 83.57 0.78 0.73\nTwins 50.00 20.34 99.72 0.73 0.54\nTable 2: Results of unlearning on visual modality only.", "mimetype": "text/plain", "start_char_idx": 1248, "end_char_idx": 3402, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "30c9f378-e2ed-4dd9-9709-d28beb9e3505": {"__data__": {"id_": "30c9f378-e2ed-4dd9-9709-d28beb9e3505", "embedding": null, "metadata": {"page_label": "6", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_530d34b9-3785-4177-94c4-b5f775c5209c"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "592c25bb-04c1-4e15-8772-d7b481524bfc", "node_type": "4", "metadata": {"page_label": "6", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_530d34b9-3785-4177-94c4-b5f775c5209c"}, "hash": "e3a4d171ec3490bf744406e92cd2a502c05bf26db1ac26db3847b52d7b6b710b", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "bbaee324-9db7-4409-954b-90fd96e4c8b9", "node_type": "1", "metadata": {"page_label": "6", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_530d34b9-3785-4177-94c4-b5f775c5209c"}, "hash": "82d7fefaefc8fb96dc1d05c3dde80cf9f3276ad94d9b8ac2b82795448e1e0012", "class_name": "RelatedNodeInfo"}}, "text": "The gray color represents methods with relatively low\naccuracy on the retain set, indicating that they suffer\nfrom catastrophic forgetting. Therefore, we do not con-\nsider these methods to be successful.\n5 Experiments\nFirst, we explore the capabilities of the current\nunlearning methods within single domains in Sec.\n5.1 and 5.2. Second, we transfer them to textual-\nvisual MMU in Sec. 5.3.\n5.1 Unlearning Textual Domain (LLMs)\nFor the experiments on the textual domain exclu-\nsively, we consider the textual part of the proposed\nCLEAR dataset and use the LLMs, which are often\nused in MLLMs, specifically Llama2-7B (Touvron\net al., 2023) and Mistral-7B (Jiang et al., 2023).\nFirst, we finetune the model on all the data and\nunlearn it on the forget set. To evaluate the final\nquality of unlearning, we calculate Real, Retain,\nand Forget metrics and Forget Quality. The full\n6", "mimetype": "text/plain", "start_char_idx": 3403, "end_char_idx": 4279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07889096-ec34-4975-a907-a7cbfccb9999": {"__data__": {"id_": "07889096-ec34-4975-a907-a7cbfccb9999", "embedding": null, "metadata": {"page_label": "7", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_83b1d926-55ed-42fa-92d4-a386a984ab8f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "def5b079-93bf-469b-899a-36e94a9d8207", "node_type": "4", "metadata": {"page_label": "7", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_83b1d926-55ed-42fa-92d4-a386a984ab8f"}, "hash": "8c9070c4844d558248ff0618b89d4f458a9fc71297917ae0dc2dd513719a4c4f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f96973d9-40a4-4ca1-baf3-7e5d9625ae63", "node_type": "1", "metadata": {}, "hash": "3e9185cd1910843120a9427a04568c264a6c2948cc0abe1fbb3612912157e8e2", "class_name": "RelatedNodeInfo"}}, "text": "results are provided in Table 1. See the Appendix\nD for pipeline details and hyperparameters.\nLoss Modality Real \u2191Forget \u2193Retain \u2191Log Forget\nQuality \u2191\nOriginal \u2014\u2013 0.48 0.3 0.51 -61.22\nGold \u2014\u2013 0.50 0.19 0.51 0.00\nLLMU text 0.47 0.37 0.49 -71.23\nLLMU visual 0.50 0.35 0.51 -60.26\nLLMU both 0.47 0.25 0.51 -95.12\nSCRUB text 0.49 0.35 0.51 -61.22\nSCRUB visual 0.48 0.37 0.49 -60.26\nSCRUB both 0.49 0.36 0.52 -60.26\nDPO text 0.46 0.38 0.49 -62.18\nDPO visual 0.49 0.22 0.49 -90.26\nDPO both 0.46 0.22 0.48 -91.46\nTable 3: Results of unlearning of different modalities.\nWe finetune on full datasets (both modalities), then for-\nget on a single domain subset (text or visual) or full\nforget set. Original \u2013 model before unlearning. Gold - a\nmodel trained only on retain.\n5.2 Unlearning Visual Domain\nExperiments on the visual domain of the CLEAR\ndataset focus on the face biometrics (identification)\ntask. While face identification can be treated as a\nclassification task with a fixed set of individuals, it\nis typically framed as a few-shot or metric learn-\ning problem, aiming to train an embedding model\nthat maps images of the same person to nearby\npoints in the embedding space and ensures separa-\ntion between different individuals and this should\nhold even for the unseen during training persons.\nWe chose ResNet-18 (He et al., 2015) due to its\nrelatively small size and scalability.\nWe fine-tuned ResNet-18, pre-trained on Ima-\ngeNet, for 100 epochs on the Celebs dataset (Zhang\net al., 2020), using photos of 797 individuals for\ntraining and 200 for testing. To compute identifica-\ntion accuracy, we averaged the embeddings of five\nimages per individual to obtain reference (enroll-\nment) vectors and classified the remaining images\nusing cosine similarity. The model achieved 99.0%\naccuracy on the training set and 83.4% on the test\nset. For unlearning evaluation, the model was fine-\ntuned on the visual part of CLEAR dataset 128\ntimes, following a Membership Inference Attack\n(MIA) approach. We trained 64 models with the\nforget included and 64 models without it, alternat-\ning with the holdout set (if the forgetting set is used\nfor training, the holdout set is not, and vice versa).\nWe evaluated the models on DF,DR, and DH\nusing accuracy, U-LIRA (Hayes et al., 2024), and\nU-MIA attack metrics. U-MIA is a lightweight,population-based attack that trains a binary classi-\nfier on shadow model outputs and their status (1 if\nfrom the forget set, 0 otherwise). Successful un-\nlearning occurs when Holdout Accuracy closely\naligns with Forget Accuracy . From the standpoint\nof the attack, U-LIRA andU-MIA should be un-\nable to distinguish between the two sets, resulting\nin an attack accuracy close to 0.50. This situation\nwould indicate that the model has \"forgotten\" the\nspecific data, as the model treats the forgetting and\nholdout sets similarly. We compare the forget and\nholdout sets logit distributions in Appendix 6.\n5.3 Multimodal Experiments\nFor the source model, we use LLaVa model (Liu\net al., 2023) with ViT (Dosovitskiy et al., 2021)\nas visual encoder and LLaMa2-7B (Touvron et al.,\n2023) as a language model. First, we finetune it\non the full CLEAR, both visual and textual parts,\nand call this model \"original\", as it contains forget\nand retain sets of knowledge.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f96973d9-40a4-4ca1-baf3-7e5d9625ae63": {"__data__": {"id_": "f96973d9-40a4-4ca1-baf3-7e5d9625ae63", "embedding": null, "metadata": {"page_label": "7", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_83b1d926-55ed-42fa-92d4-a386a984ab8f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "def5b079-93bf-469b-899a-36e94a9d8207", "node_type": "4", "metadata": {"page_label": "7", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_83b1d926-55ed-42fa-92d4-a386a984ab8f"}, "hash": "8c9070c4844d558248ff0618b89d4f458a9fc71297917ae0dc2dd513719a4c4f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "07889096-ec34-4975-a907-a7cbfccb9999", "node_type": "1", "metadata": {"page_label": "7", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_83b1d926-55ed-42fa-92d4-a386a984ab8f"}, "hash": "bccc714cd01a645c53796c141edb6ec7cb4e20900f5c2ed34b65bbe42d2200c7", "class_name": "RelatedNodeInfo"}}, "text": "This situation\nwould indicate that the model has \"forgotten\" the\nspecific data, as the model treats the forgetting and\nholdout sets similarly. We compare the forget and\nholdout sets logit distributions in Appendix 6.\n5.3 Multimodal Experiments\nFor the source model, we use LLaVa model (Liu\net al., 2023) with ViT (Dosovitskiy et al., 2021)\nas visual encoder and LLaMa2-7B (Touvron et al.,\n2023) as a language model. First, we finetune it\non the full CLEAR, both visual and textual parts,\nand call this model \"original\", as it contains forget\nand retain sets of knowledge. Then, we perform\nthe unlearning process on it. We use the same\nhyperparameters for each method. Then, we eval-\nuate the unlearned model according to our metric\nsetup described in Sec. 4.3. For comparison, we\ndemonstrate the metrics of the \"gold\" model. The\nresults of experiments and corresponding metrics\nare provided in Table 4. Details of the experiments\npipeline are described in Appendix F.\n5.3.1 Is Textual Unlearning Enough?\nWe begin by addressing the question: Can we for-\nget a person using only textual data, and does mul-\ntimodality introduce new challenges to unlearning?\nTo explore this, we attempt to forget 20 indi-\nviduals from the forget set using only textual un-\nlearning. We also perform unlearning using only\nvisual data or both modalities, and then compare\nthe results. We find that unlearning text alone is\nsufficient to achieve a low forget metric, consistent\nwith previous findings. However, this also results\nin a noticeable drop in retain metrics . Full results\nare provided in Table 3.\n5.3.2 Unlearning Both Domains\nAfter we understand that multimodal unlearning\ncan not be fully addressed using a single modality,\nwe proceed with experiments on unlearning across\nboth modalities. We take our source model f\u03b8and\napply unlearning methods. As forget set, we use\nall the available data about 20 persons - 10% of the\ndataset size.\n7", "mimetype": "text/plain", "start_char_idx": 2708, "end_char_idx": 4636, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "56416986-896f-4655-b157-b3ea3dce833f": {"__data__": {"id_": "56416986-896f-4655-b157-b3ea3dce833f", "embedding": null, "metadata": {"page_label": "8", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_312f51a1-85e3-40ed-b277-11ef73d4ee98"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "01522fc9-219c-4bf1-96a9-4c2fad52e98c", "node_type": "4", "metadata": {"page_label": "8", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_312f51a1-85e3-40ed-b277-11ef73d4ee98"}, "hash": "6a572cda39ea4551fb638146c5ddb6f5d04fc90b4f975fd8232f7ff7ee72cea8", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dd0a30c7-587a-40d2-bf70-2176b4271e34", "node_type": "1", "metadata": {}, "hash": "997f44463bff7b8f2a365311a013476def784e8d9441afb3f835a0f459979afa", "class_name": "RelatedNodeInfo"}}, "text": "MethodLoRA L1\nRegularizationReal\nmetric \u2191Retain\nmetric \u2191Forget\nmetric \u2193Log Forget\nQuality \u2191\nOriginal \u2014 0.48 0.51 0.39 -61.22\nGold \u2014 0.50 0.51 0.19 0.00\nGA \u2014 0.32 0.00 0.00 -13.04\nGA \u2713 0.49 0.50 0.37 -61.22\nGD \u2014 0.24 0.00 0.00 -17.72\nGD \u2713 0.49 0.50 0.37 -62.18\nIDK \u2014 0.48 0.51 0.30 -74.40\nIDK \u2713 0.49 0.50 0.37 -63.15\nKL \u2014 0.27 0.00 0.00 -13.92\nKL \u2713 0.49 0.50 0.37 -62.18\nNPO \u2014 0.49 0.51 0.36 -63.15\nNPO \u2713 0.49 0.51 0.36 -64.13\nRetain FT \u2014 0.49 0.51 0.36 -60.26\nRetain FT \u2713 0.49 0.50 0.37 -61.22\nRMU \u2014 0.27 0.00 0.00 -23.68\nRMU \u2713 0.49 0.50 0.36 -61.22\nLLMU \u2014 0.47 0.49 0.37 -73.34\nLLMU \u2713 0.49 0.51 0.36 -60.26\nDPO \u2014 0.46 0.49 0.39 -61.22\nDPO \u2713 0.48 0.50 0.37 -65.12\nSCRUB \u2014 0.49 0.51 0.36 -62.18\nSCRUB \u2713 0.50 0.51 0.35 -61.22\nTable 4: Results on experiments with and without LoRA\nregularization. The gray color shows that the method\ncompletely fails on the retain set.\n5.4 LoRA Regularization\nAs shown previously (Jia et al., 2024), model spar-\nsity can improve unlearning, but significant drops\nin retain metrics still occur. We hypothesize that\nkeeping the model close to its initial state during\nunlearning could help preserve retain knowledge.\nLoRA adapters (Hu et al., 2021) have become\na standard technique to reduce computational de-\nmands in large-scale NLP models. We propose\nusing the magnitude of LoRA weights as a proxy\nfor how far the model has deviated from its initial\nstate. To address this, we add the \u21131norm of the\nadapter weights to the unlearning loss, using a fixed\n\u03bb= 0.01without tuning, though tuning could im-\nprove results. The results of the experiments are\nshown in Table 4.\n6 Results and Discussion\nText domain\nTable 1 presents the results for the text domain,\nshowing that the RMU, KL, GD, and GA meth-\nods excel in unlearning the forget set (with the\nforget metric dropping to 0), but they suffer from\ncatastrophic forgetting on the retain data (the retain\nmetric also drops to 0). The remaining methods\nmaintain performance on the retain set (retain met-\nrics remain roughly the same), but their unlearning\nquality is poor \u2013 the forget metric is close to the\none achieved by Retain FT. Identifying an optimal\nmethod that balances unlearning and retention is\nchallenging. However, among the tested methods,\nIDK, DPO, and SCRUB provide the best (lowest)forget metrics without a drop in retain performance.\nThese observations are consistent across both the\nLLaMa and Mistral models.\nVisual MU results are presented in Table 2. It\nshows that most methods achieve high accuracy\non the forget set with competitive U-LIRA and\nU-MIA values. Notably, SCRUB bioand Twins per-\nform best across all considered metrics, making\nthem optimal in this context. The Holdout Accu-\nracy is relatively consistent across methods.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dd0a30c7-587a-40d2-bf70-2176b4271e34": {"__data__": {"id_": "dd0a30c7-587a-40d2-bf70-2176b4271e34", "embedding": null, "metadata": {"page_label": "8", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_312f51a1-85e3-40ed-b277-11ef73d4ee98"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "01522fc9-219c-4bf1-96a9-4c2fad52e98c", "node_type": "4", "metadata": {"page_label": "8", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_312f51a1-85e3-40ed-b277-11ef73d4ee98"}, "hash": "6a572cda39ea4551fb638146c5ddb6f5d04fc90b4f975fd8232f7ff7ee72cea8", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "56416986-896f-4655-b157-b3ea3dce833f", "node_type": "1", "metadata": {"page_label": "8", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_312f51a1-85e3-40ed-b277-11ef73d4ee98"}, "hash": "77b7dc3c7e6198be17800ba8e7dfcb48567e0a45af5481c2faf4052a9d4e2b1c", "class_name": "RelatedNodeInfo"}}, "text": "The remaining methods\nmaintain performance on the retain set (retain met-\nrics remain roughly the same), but their unlearning\nquality is poor \u2013 the forget metric is close to the\none achieved by Retain FT. Identifying an optimal\nmethod that balances unlearning and retention is\nchallenging. However, among the tested methods,\nIDK, DPO, and SCRUB provide the best (lowest)forget metrics without a drop in retain performance.\nThese observations are consistent across both the\nLLaMa and Mistral models.\nVisual MU results are presented in Table 2. It\nshows that most methods achieve high accuracy\non the forget set with competitive U-LIRA and\nU-MIA values. Notably, SCRUB bioand Twins per-\nform best across all considered metrics, making\nthem optimal in this context. The Holdout Accu-\nracy is relatively consistent across methods.\nInmultimodal unlearning , Table 3 shows that\nfor the LLMU method, unlearning both modalities\nyields better results than text-only unlearning. The\nforget metric drops from 0.37 in the textual domain\nto 0.25 when unlearning both domains, while the\nretain and real metrics remain stable. For DPO,\nthe results are less straightforward, but it is evident\nthat unlearning the visual domain is crucial. When\nunlearning in the visual or both domains, the for-\nget metric is 0.22, compared to 0.38 for text-only\nunlearning. The retain and real metrics stay con-\nsistent. However, SCRUB remains robust across\nall modalities, performing consistently in all three\nsetups.\nThen we run our experiments on unlearning both\ndomains. The picture is very similar to the experi-\nments on the textual domain only. Table 4 shows\nthat GA, GD, KL and RMU effectively unlearn\nthe forget set (forget metric goes to 0) but exhibit\nsignificant catastrophic forgetting of the retain set\n(retain metrics also goes to 0). In contrast, IDK,\nSCRUB, LLMU and DPO remain stable on the re-\ntain set (around 0.48), but their unlearning quality\nis worse (0.37 versus 0.39 on the original unun-\nlearned model). The achieving the balance between\nunlearning and retention is challenging again.\nLeaderboards We construct our leaderboards\nstraightforwardly. First, we exclude methods that\nfail to retain knowledge from the retain set (high-\nlighted in gray in the tables) and then rank the\nremaining methrods based on the Forget metric (or\nU-LIRA in the visual domain). The top-3 methods\namong each modality are shown in figure 2\nLoRA Regularization Lastly, the experiments on\n\u21131LoRA regularization (Table 4) show improve-\nments in unlearning quality for several methods,\nsignificantly reducing catastrophic forgetting in\nGradient Ascent, Gradient Difference, KL mini-\nmization, RMU, and especially in LLMU. How-\never, the proposed regularization is not beneficial\nfor all unlearning techniques.\n8", "mimetype": "text/plain", "start_char_idx": 1910, "end_char_idx": 4691, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a6b4a6b-e898-4b30-8f51-50002e8942b6": {"__data__": {"id_": "0a6b4a6b-e898-4b30-8f51-50002e8942b6", "embedding": null, "metadata": {"page_label": "9", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f1fe13ec-61cc-4522-93f2-77344bfd33fe"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49cbdf01-4090-4204-95c4-01ad5603b94e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f1fe13ec-61cc-4522-93f2-77344bfd33fe"}, "hash": "7c87e1fbbbf376c1b2496f969e079d750e86ee137e9861239c957b4d9890c903", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c636e520-5899-4ea3-98ba-8bea265995fb", "node_type": "1", "metadata": {}, "hash": "2699580e5c637f793c2bb5d5c7ac78005feb38575e387223a514db58698366a1", "class_name": "RelatedNodeInfo"}}, "text": "7 Conclusion\nIn this work, we introduce CLEAR, the first open-\nsourced benchmark designed to assess machine\nunlearning in multimodal (textual and visual) setup.\nOur evaluation of existing unlearning techniques\nacross domains shows that multimodal unlearning\nis more challenging than previously anticipated,\nlaying the ground for further research. Our stud-\nies on incorporating a LoRA regularization term\ndemonstrate that this simple technique improves un-\nlearning and can be easily integrated into other MU\nmethods. We aim to encourage further research\non enhancing privacy and security in large-scale\nAI models by offering an open-source benchmark.\nFuture work could focus on improving MMU algo-\nrithms and expanding unlearning to new modalities,\nsuch as voice and video.\nLimitations\nDespite the contributions of this work, several limi-\ntations remain that need further investigation. One\nmajor limitation is the reliance on synthetic data,\nas CLEAR is based on such dataset, which may\nnot fully capture the complexity of real-world sce-\nnarios, thus limiting the generalizability of our\nfindings. Additionally, while our work focuses on\nunlearning methods designed for privacy-centric\napplications, such as removing personal data, it\nmay not fully address other unlearning needs,\nsuch as removing harmful content. Moreover, our\nbenchmark mainly evaluates fine-tuning-based un-\nlearning methods using sophisticated loss func-\ntions, leaving unexplored other broader unlearn-\ning techniques, such as analytical or mechanical\napproaches. Another challenge lies in the scala-\nbility of these unlearning methods, as they may\nstruggle to scale efficiently when applied to larger\nmodels and datasets, hindering their potential use\nin real-world systems. Furthermore, our focus on\ncatastrophic forgetting overlooks unintended side\neffects, such as the introduction of biases or the\ndegradation of model performance on unrelated\ntasks, and the broader impact of unlearning on fair-\nness and safety remains an open area for future\nresearch.\nEthics\nWe utilized 84 hours of A100 GPU computation\nfor our experiments, which resulted in an estimated\n9 kg of CO2 emissions.References\nClark Barrett, Brad Boyd, Elie Bursztein, Nicholas Car-\nlini, Brad Chen, Jihye Choi, Amrita Roy Chowdhury,\nMihai Christodorescu, Anupam Datta, Soheil Feizi,\net al. 2023. Identifying and mitigating the security\nrisks of generative ai. Foundations and Trends \u00aein\nPrivacy and Security , 6(1):1\u201352.\nDenis Bobkov, Vadim Titov, Aibek Alanov, and Dmitry\nVetrov. 2024. The devil is in the details: Style-\nfeatureeditor for detail-rich stylegan inversion and\nhigh quality image editing. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , pages 9337\u20139346.\nYinzhi Cao and Junfeng Yang. 2015. Towards making\nsystems forget with machine unlearning. In 2015\nIEEE symposium on security and privacy , pages 463\u2013\n480. IEEE.\nNicholas Carlini, Steve Chien, Milad Nasr, Shuang\nSong, Andreas Terzis, and Florian Tramer. 2022.\nMembership inference attacks from first principles.\nPreprint , arXiv:2112.03570.\nTrishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael\nAbu-Ghazaleh, M. Salman Asif, Yue Dong, Amit K.\nRoy-Chowdhury, and Chengyu Song. 2024. Cross-\nmodal safety alignment: Is textual unlearning all you\nneed? Preprint , arXiv:2406.02575.\nJiaao Chen and Diyi Yang. 2023. Unlearn what you\nwant to forget: Efficient unlearning for LLMs. In Pro-\nceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing , pages 12041\u2013\n12052, Singapore. Association for Computational\nLinguistics.\nJiali Cheng and Hadi Amiri. 2023. Multidelete for\nmultimodal machine unlearning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3687, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c636e520-5899-4ea3-98ba-8bea265995fb": {"__data__": {"id_": "c636e520-5899-4ea3-98ba-8bea265995fb", "embedding": null, "metadata": {"page_label": "9", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f1fe13ec-61cc-4522-93f2-77344bfd33fe"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "49cbdf01-4090-4204-95c4-01ad5603b94e", "node_type": "4", "metadata": {"page_label": "9", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f1fe13ec-61cc-4522-93f2-77344bfd33fe"}, "hash": "7c87e1fbbbf376c1b2496f969e079d750e86ee137e9861239c957b4d9890c903", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "0a6b4a6b-e898-4b30-8f51-50002e8942b6", "node_type": "1", "metadata": {"page_label": "9", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f1fe13ec-61cc-4522-93f2-77344bfd33fe"}, "hash": "2a38abcefe3e03c50068b2964ff455b0c8e9013ad587a3d7b7ef3b9df194f551", "class_name": "RelatedNodeInfo"}}, "text": "Trishna Chakraborty, Erfan Shayegani, Zikui Cai, Nael\nAbu-Ghazaleh, M. Salman Asif, Yue Dong, Amit K.\nRoy-Chowdhury, and Chengyu Song. 2024. Cross-\nmodal safety alignment: Is textual unlearning all you\nneed? Preprint , arXiv:2406.02575.\nJiaao Chen and Diyi Yang. 2023. Unlearn what you\nwant to forget: Efficient unlearning for LLMs. In Pro-\nceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing , pages 12041\u2013\n12052, Singapore. Association for Computational\nLinguistics.\nJiali Cheng and Hadi Amiri. 2023. Multidelete for\nmultimodal machine unlearning.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. Preprint , arXiv:2010.11929.\nCynthia Dwork, Aaron Roth, et al. 2014. The algorith-\nmic foundations of differential privacy. Foundations\nand Trends \u00aein Theoretical Computer Science , 9(3\u2013\n4):211\u2013407.\nRonen Eldan and Mark Russinovich. 2023. Who\u2019s harry\npotter? approximate unlearning in llms. Preprint ,\narXiv:2310.02238.\nAditya Golatkar, Alessandro Achille, and Stefano\nSoatto. 2020. Eternal sunshine of the spotless net: Se-\nlective forgetting in deep networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition , pages 9304\u20139312.\n9", "mimetype": "text/plain", "start_char_idx": 3104, "end_char_idx": 4546, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "602e12b0-0f54-4601-9905-f72c7e1942ac": {"__data__": {"id_": "602e12b0-0f54-4601-9905-f72c7e1942ac", "embedding": null, "metadata": {"page_label": "10", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2c92508-4c9e-4d5f-a02f-0d6717d46dd3"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a860910-1b8b-4660-b2cb-417ae28697e4", "node_type": "4", "metadata": {"page_label": "10", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2c92508-4c9e-4d5f-a02f-0d6717d46dd3"}, "hash": "1497107c23178b4c3f128e7f975d2a2f3e5a64d4b8dfcb21df3f21c150dd65ad", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a13418ca-c784-4a3f-9873-22957bbc0efc", "node_type": "1", "metadata": {}, "hash": "f713e8573332bb5859495ad32e27dcf0a13558de063f85800e1cfa4f7d712976", "class_name": "RelatedNodeInfo"}}, "text": "Yichen Gong, Delong Ran, Jinyuan Liu, Conglei Wang,\nTianshuo Cong, Anyu Wang, Sisi Duan, and Xiaoyun\nWang. 2023. Figstep: Jailbreaking large vision-\nlanguage models via typographic visual prompts.\narXiv preprint arXiv:2311.05608 .\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition , pages 6904\u20136913.\nJamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr\nKhalifa, and Nicolas Papernot. 2024. Inexact un-\nlearning needs more careful evaluations to avoid a\nfalse sense of privacy. Preprint , arXiv:2403.01218.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Deep residual learning for image recogni-\ntion. Preprint , arXiv:1512.03385.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2021. Lora: Low-rank adaptation of\nlarge language models. Preprint , arXiv:2106.09685.\nGabriel Ilharco, Marco Tulio Ribeiro, Mitchell Worts-\nman, Suchin Gururangan, Ludwig Schmidt, Han-\nnaneh Hajishirzi, and Ali Farhadi. 2022. Edit-\ning models with task arithmetic. arXiv preprint\narXiv:2212.04089 .\nJoel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha,\nMoontae Lee, Lajanugen Logeswaran, and Minjoon\nSeo. 2022. Knowledge unlearning for mitigating\nprivacy risks in language models. arXiv preprint\narXiv:2210.01504 .\nJiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi\nZhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou\nWang, and Yaodong Yang. 2024. Beavertails: To-\nwards improved safety alignment of llm via a human-\npreference dataset. Advances in Neural Information\nProcessing Systems , 36.\nJinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang\nYao, Gaowen Liu, Yang Liu, Pranay Sharma, and\nSijia Liu. 2024. Model sparsity can simplify machine\nunlearning. Preprint , arXiv:2304.04934.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud,\nMarie-Anne Lachaux, Pierre Stock, Teven Le Scao,\nThibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix,\nand William El Sayed. 2023. Mistral 7b. Preprint ,\narXiv:2310.06825.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hell-\nsten, Jaakko Lehtinen, and Timo Aila. 2020. Ana-\nlyzing and improving the image quality of stylegan.\nPreprint , arXiv:1912.04958.Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes,\nand Eleni Triantafillou. 2023. Towards unbounded\nmachine unlearning. Preprint , arXiv:2302.09880.\nMeghdad Kurmanji, Peter Triantafillou, Jamie Hayes,\nand Eleni Triantafillou. 2024. Towards unbounded\nmachine unlearning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a13418ca-c784-4a3f-9873-22957bbc0efc": {"__data__": {"id_": "a13418ca-c784-4a3f-9873-22957bbc0efc", "embedding": null, "metadata": {"page_label": "10", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2c92508-4c9e-4d5f-a02f-0d6717d46dd3"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a860910-1b8b-4660-b2cb-417ae28697e4", "node_type": "4", "metadata": {"page_label": "10", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2c92508-4c9e-4d5f-a02f-0d6717d46dd3"}, "hash": "1497107c23178b4c3f128e7f975d2a2f3e5a64d4b8dfcb21df3f21c150dd65ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "602e12b0-0f54-4601-9905-f72c7e1942ac", "node_type": "1", "metadata": {"page_label": "10", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2c92508-4c9e-4d5f-a02f-0d6717d46dd3"}, "hash": "a4ee2151f8f70810885576527d73ebc0f5f0f610773fac80c531a8cccee2817a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a10d4ab2-ee04-4885-bfc0-2476d924d4b2", "node_type": "1", "metadata": {}, "hash": "4c24de4e289f9d7f38314a3dc1a1a40124714b5445ee21a673cc5a322dc7129f", "class_name": "RelatedNodeInfo"}}, "text": "2023. Mistral 7b. Preprint ,\narXiv:2310.06825.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hell-\nsten, Jaakko Lehtinen, and Timo Aila. 2020. Ana-\nlyzing and improving the image quality of stylegan.\nPreprint , arXiv:1912.04958.Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes,\nand Eleni Triantafillou. 2023. Towards unbounded\nmachine unlearning. Preprint , arXiv:2302.09880.\nMeghdad Kurmanji, Peter Triantafillou, Jamie Hayes,\nand Eleni Triantafillou. 2024. Towards unbounded\nmachine unlearning. Advances in neural information\nprocessing systems , 36.\nJiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi,\nMiaozeng Du, Yongrui Chen, and Sheng Bi. 2024a.\nSingle image unlearning: Efficient machine unlearn-\ning in multimodal large language models. arXiv\npreprint arXiv:2405.12523 .\nNathaniel Li, Alexander Pan, Anjali Gopal, Sum-\nmer Yue, Daniel Berrios, Alice Gatti, Justin D. Li,\nAnn-Kathrin Dombrowski, Shashwat Goel, Long\nPhan, Gabriel Mukobi, Nathan Helm-Burger, Rassin\nLababidi, Lennart Justen, Andrew B. Liu, Michael\nChen, Isabelle Barrass, Oliver Zhang, Xiaoyuan Zhu,\nRishub Tamirisa, Bhrugu Bharathi, Adam Khoja,\nZhenqi Zhao, Ariel Herbert-V oss, Cort B. Breuer,\nSamuel Marks, Oam Patel, Andy Zou, Mantas\nMazeika, Zifan Wang, Palash Oswal, Weiran Liu,\nAdam A. Hunt, Justin Tienken-Harder, Kevin Y .\nShih, Kemper Talley, John Guan, Russell Kaplan,\nIan Steneker, David Campbell, Brad Jokubaitis, Alex\nLevinson, Jean Wang, William Qian, Kallol Kr-\nishna Karmakar, Steven Basart, Stephen Fitz, Mindy\nLevine, Ponnurangam Kumaraguru, Uday Tupakula,\nVijay Varadharajan, Yan Shoshitaishvili, Jimmy\nBa, Kevin M. Esvelt, Alexandr Wang, and Dan\nHendrycks. 2024b. The wmdp benchmark: Mea-\nsuring and reducing malicious use with unlearning.\nPreprint , arXiv:2403.03218.\nZhen Li, Mingdeng Cao, Xintao Wang, Zhongang\nQi, Ming-Ming Cheng, and Ying Shan. 2024c.\nPhotomaker: Customizing realistic human photos\nvia stacked id embedding. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR) , pages 8640\u20138650.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out , pages 74\u201381.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958 .\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13 , pages 740\u2013755. Springer.\nBo Liu, Qiang Liu, and Peter Stone. 2022. Con-\ntinual learning and private unlearning. Preprint ,\narXiv:2203.12817.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023.", "mimetype": "text/plain", "start_char_idx": 2275, "end_char_idx": 5085, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a10d4ab2-ee04-4885-bfc0-2476d924d4b2": {"__data__": {"id_": "a10d4ab2-ee04-4885-bfc0-2476d924d4b2", "embedding": null, "metadata": {"page_label": "10", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2c92508-4c9e-4d5f-a02f-0d6717d46dd3"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a860910-1b8b-4660-b2cb-417ae28697e4", "node_type": "4", "metadata": {"page_label": "10", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2c92508-4c9e-4d5f-a02f-0d6717d46dd3"}, "hash": "1497107c23178b4c3f128e7f975d2a2f3e5a64d4b8dfcb21df3f21c150dd65ad", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a13418ca-c784-4a3f-9873-22957bbc0efc", "node_type": "1", "metadata": {"page_label": "10", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2c92508-4c9e-4d5f-a02f-0d6717d46dd3"}, "hash": "1dcea6a1784973f6921f99bacf035a6fe1c1e5645f7653d85ae55c2164fa6d05", "class_name": "RelatedNodeInfo"}}, "text": "arXiv preprint arXiv:2109.07958 .\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13 , pages 740\u2013755. Springer.\nBo Liu, Qiang Liu, and Peter Stone. 2022. Con-\ntinual learning and private unlearning. Preprint ,\narXiv:2203.12817.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023. Visual instruction tuning.\n10", "mimetype": "text/plain", "start_char_idx": 4550, "end_char_idx": 5115, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "94922eb9-7e11-4ee1-9704-9dd91cb56b4c": {"__data__": {"id_": "94922eb9-7e11-4ee1-9704-9dd91cb56b4c", "embedding": null, "metadata": {"page_label": "11", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_35d02f6e-1ddd-4b12-9d96-7a4d7e90f415"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30d293d0-e0d5-44ab-bbce-e81fba386379", "node_type": "4", "metadata": {"page_label": "11", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_35d02f6e-1ddd-4b12-9d96-7a4d7e90f415"}, "hash": "3d8b05315d288ad11a26ec3f21cee77c2373c3391a415c1e2186699a229043ed", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "89a52eae-0cb8-4f6b-8d57-f6a52c682ba9", "node_type": "1", "metadata": {}, "hash": "c0ca348486ba6eca16ad99163714ae3522673fbd1e5d7b97893c0ef16b4794d0", "class_name": "RelatedNodeInfo"}}, "text": "Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang,\nLianhui Qin, Peter West, Prithviraj Ammanabrolu,\nand Yejin Choi. 2022. Quark: Controllable text\ngeneration with reinforced unlearning. Advances\nin neural information processing systems , 35:27591\u2013\n27609.\nWeidi Luo, Siyuan Ma, Xiaogeng Liu, Xiaoyu Guo,\nand Chaowei Xiao. 2024. Jailbreakv-28k: A bench-\nmark for assessing the robustness of multimodal large\nlanguage models against jailbreak attacks. arXiv\npreprint arXiv:2404.03027 .\nPratyush Maini, Zhili Feng, Avi Schwarzschild,\nZachary C. Lipton, and J. Zico Kolter. 2024. Tofu: A\ntask of fictitious unlearning for llms.\nSeth Neel, Aaron Roth, and Saeed Sharifi-Malvajerdi.\n2021. Descent-to-delete: Gradient-based methods\nfor machine unlearning. In Algorithmic Learning\nTheory , pages 931\u2013962. PMLR.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning , pages 8748\u20138763. PMLR.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D. Manning, and Chelsea Finn.\n2023. Direct preference optimization: Your lan-\nguage model is secretly a reward model. Preprint ,\narXiv:2305.18290.\nAyush Sekhari, Jayadev Acharya, Gautam Kamath, and\nAnanda Theertha Suresh. 2021. Remember what you\nwant to forget: Algorithms for machine unlearning.\nAdvances in Neural Information Processing Systems ,\n34:18075\u201318086.\nErfan Shayegani, Yue Dong, and Nael Abu-Ghazaleh.\n2023. Jailbreak in pieces: Compositional adversar-\nial attacks on multi-modal language models. In The\nTwelfth International Conference on Learning Repre-\nsentations .\nYujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou.\n2020. Interpreting the latent space of gans for seman-\ntic face editing. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recogni-\ntion, pages 9243\u20139252.\nWeijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Mal-\nladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke\nZettlemoyer, Noah A. Smith, and Chiyuan Zhang.\n2024. Muse: Machine unlearning six-way evaluation\nfor language models. Preprint , arXiv:2407.06460.\nReza Shokri, Marco Stronati, Congzheng Song, and Vi-\ntaly Shmatikov. 2017. Membership inference attacks\nagainst machine learning models. In 2017 IEEE sym-\nposium on security and privacy (SP) , pages 3\u201318.\nIEEE.Ayush Tarun, Vikram Chundawat, Murari Mandal, and\nMohan Kankanhalli. 2021. Fast yet effective ma-\nchine unlearning.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2572, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89a52eae-0cb8-4f6b-8d57-f6a52c682ba9": {"__data__": {"id_": "89a52eae-0cb8-4f6b-8d57-f6a52c682ba9", "embedding": null, "metadata": {"page_label": "11", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_35d02f6e-1ddd-4b12-9d96-7a4d7e90f415"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30d293d0-e0d5-44ab-bbce-e81fba386379", "node_type": "4", "metadata": {"page_label": "11", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_35d02f6e-1ddd-4b12-9d96-7a4d7e90f415"}, "hash": "3d8b05315d288ad11a26ec3f21cee77c2373c3391a415c1e2186699a229043ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "94922eb9-7e11-4ee1-9704-9dd91cb56b4c", "node_type": "1", "metadata": {"page_label": "11", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_35d02f6e-1ddd-4b12-9d96-7a4d7e90f415"}, "hash": "a2f461e9ac4619e52c4a505f04012c4c84f7a198cf75589632ad4e4240c3090b", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "38d14119-7377-45a1-9ab1-fadc969337a5", "node_type": "1", "metadata": {}, "hash": "7b73b224a0c1a067cd496f2fa3062cb5f5926b15fcb8833d8eddd3981419a26e", "class_name": "RelatedNodeInfo"}}, "text": "Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Mal-\nladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke\nZettlemoyer, Noah A. Smith, and Chiyuan Zhang.\n2024. Muse: Machine unlearning six-way evaluation\nfor language models. Preprint , arXiv:2407.06460.\nReza Shokri, Marco Stronati, Congzheng Song, and Vi-\ntaly Shmatikov. 2017. Membership inference attacks\nagainst machine learning models. In 2017 IEEE sym-\nposium on security and privacy (SP) , pages 3\u201318.\nIEEE.Ayush Tarun, Vikram Chundawat, Murari Mandal, and\nMohan Kankanhalli. 2021. Fast yet effective ma-\nchine unlearning.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. Preprint , arXiv:2307.09288.\nXinwei Wu, Junzhuo Li, Minghui Xu, Weilong\nDong, Shuangzhi Wu, Chao Bian, and Deyi Xiong.\n2023. Depn: Detecting and editing privacy neu-\nrons in pretrained language models. arXiv preprint\narXiv:2310.20138 .\nZongze Wu, Dani Lischinski, and Eli Shechtman. 2020.\nStylespace analysis: Disentangled controls for style-\ngan image generation. 2021 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) ,\npages 12858\u201312867.\nx.ai. 2024. Grok-1.5 vision preview.\nShangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao\nChen, Chunhui Li, Jianbing Zhang, and Xinyu Dai.\n2024. Efuf: Efficient fine-grained unlearning frame-\nwork for mitigating hallucinations in multimodal\nlarge language models. ArXiv , abs/2402.09801.\nJin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao\nWang, Zezhou Cheng, and Xiang Yue. 2024a. Ma-\nchine unlearning of pre-trained large language mod-\nels.arXiv preprint arXiv:2402.15159 .\nYuanshun Yao, Xiaojun Xu, and Yang Liu. 2023.\nLarge language model unlearning. arXiv preprint\narXiv:2310.10683 .", "mimetype": "text/plain", "start_char_idx": 1997, "end_char_idx": 4727, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "38d14119-7377-45a1-9ab1-fadc969337a5": {"__data__": {"id_": "38d14119-7377-45a1-9ab1-fadc969337a5", "embedding": null, "metadata": {"page_label": "11", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_35d02f6e-1ddd-4b12-9d96-7a4d7e90f415"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30d293d0-e0d5-44ab-bbce-e81fba386379", "node_type": "4", "metadata": {"page_label": "11", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_35d02f6e-1ddd-4b12-9d96-7a4d7e90f415"}, "hash": "3d8b05315d288ad11a26ec3f21cee77c2373c3391a415c1e2186699a229043ed", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "89a52eae-0cb8-4f6b-8d57-f6a52c682ba9", "node_type": "1", "metadata": {"page_label": "11", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_35d02f6e-1ddd-4b12-9d96-7a4d7e90f415"}, "hash": "ca60fedfcd3a4d40af8693ca0368d02151de6c9f6ba640c9e5d069d3c8ea0c89", "class_name": "RelatedNodeInfo"}}, "text": "Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao\nChen, Chunhui Li, Jianbing Zhang, and Xinyu Dai.\n2024. Efuf: Efficient fine-grained unlearning frame-\nwork for mitigating hallucinations in multimodal\nlarge language models. ArXiv , abs/2402.09801.\nJin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao\nWang, Zezhou Cheng, and Xiang Yue. 2024a. Ma-\nchine unlearning of pre-trained large language mod-\nels.arXiv preprint arXiv:2402.15159 .\nYuanshun Yao, Xiaojun Xu, and Yang Liu. 2023.\nLarge language model unlearning. arXiv preprint\narXiv:2310.10683 .\nYuanshun Yao, Xiaojun Xu, and Yang Liu. 2024b.\nLarge language model unlearning. Preprint ,\narXiv:2310.10683.\nCharles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and\nHeng Ji. 2023. Unlearning bias in language models\nby partitioning gradients. In Findings of the Associa-\ntion for Computational Linguistics: ACL 2023 , pages\n6032\u20136048.\n11", "mimetype": "text/plain", "start_char_idx": 4182, "end_char_idx": 5063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b90671e3-f228-46b4-b1e5-e78f40e19b97": {"__data__": {"id_": "b90671e3-f228-46b4-b1e5-e78f40e19b97", "embedding": null, "metadata": {"page_label": "12", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b697a7d4-9425-4a31-9b90-44f2c14dc042"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "034d8c0c-def8-400e-b2da-ccbfa9936911", "node_type": "4", "metadata": {"page_label": "12", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b697a7d4-9425-4a31-9b90-44f2c14dc042"}, "hash": "41428ec704c214f5c5d765577a90c83aecae026fe5f2875c3efa2383120f8b27", "class_name": "RelatedNodeInfo"}}, "text": "Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junx-\nian He. 2023. Composing parameter-efficient mod-\nules with arithmetic operations. arXiv preprint\narXiv:2306.14870 .\nRuiqi Zhang, Licong Lin, Yu Bai, and Song Mei.\n2024. Negative preference optimization: From catas-\ntrophic collapse to effective unlearning. Preprint ,\narXiv:2404.05868.\nYaobin Zhang, Weihong Deng, Mei Wang, Jiani Hu,\nXian Li, Dongyue Zhao, and Dongchao Wen. 2020.\nGlobal-local gcn: Large-scale label noise cleans-\ning for face recognition. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition , pages 7731\u20137740.\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 615, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "369ce22b-8016-4316-b431-055171a4a389": {"__data__": {"id_": "369ce22b-8016-4316-b431-055171a4a389", "embedding": null, "metadata": {"page_label": "13", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_844827f1-90e0-4034-b39b-741fd74c2153"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed1514d2-a15f-4122-8e0e-18429e7ee461", "node_type": "4", "metadata": {"page_label": "13", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_844827f1-90e0-4034-b39b-741fd74c2153"}, "hash": "48973d1da3caf1ce7aff0eb5850611fe368d7cc7197ea086812e0b41b58a012d", "class_name": "RelatedNodeInfo"}}, "text": "A Unlearning Methods\nThis section describes the main unlearning ap-\nproaches considered in this work.\n1.Finetuning on retain data. The most straight-\nforward method to conduct unlearning is to\nfinetune the model on the retain set, assum-\ning that the model will unlearn the knowl-\nedge from the forget set and preserve its per-\nformance on the retain set. Despite its sim-\nplicity and reasonable effectiveness for rela-\ntively small models, it is not usable in models\nwith huge sizes of pre-train sets, such as most\nLLMs.\n2.Gradient ascent on forget set. In this\nmethod, unlearning is done by maximizing\nthe loss on forget data with the intuition that\nit will lead to getting predictions that are dis-\nsimilar from the correct answers for forget set\nand consequently unlearning desired informa-\ntion. Thus, this method can be considered as\na finetuning procedure with the following loss\nfunction:\nL(DF, \u03b8) =1\n|DF|X\nx\u2208DFNLL (x, \u03b8),\nwhere NLL( x, \u03b8)is the negative log-\nlikelihood of the model on the input x.\nInstead of maximizing the NLL loss, maxi-\nmizing the entropy of the model\u2019s predictions\non the forget set is possible. The intuition\nbehind this trick is that it will correspond to\nthe increase of the model\u2019s uncertainty in its\npredictions on forget set, which will also cor-\nrespond to successful unlearning.\n3.Gradient difference. (Liu et al., 2022) The\nnext method builds on the concept of combin-\ning two previous methods. It aims to increase\nthe loss on the forget data and at least maintain\nthe loss on the retain set. The loss function is\ndefined as follows:\nLGD=\u2212L(DF, \u03b8) +L(DR, \u03b8),\nwhere DFis the forget set that remains con-\nstant, DRis the retain set that is randomly\nsampled during training, and Lis a suitable\nloss function.4.KL minimization . This approach aims to\nminimize the Kullback-Leibler ( KL) diver-\ngence between the model\u2019s predictions on the\nretain set before and after unlearning while\nmaximizing the conventional loss on the for-\nget set. The LKLloss function is defined as\n1\n|DF|X\nx\u2208DF1\n|s||s|X\ni=2KL\u0000\nP(s<i|\u03b8)\r\rP(s<i|\u03b8\u2032)\u0001\n.\nThe total objective function is formulated as\nfollows:\nLobj=\u2212L(DF, \u03b8) +LKL,\nwhere \u03b8\u2032is the model\u2019s weights before un-\nlearning, sis the input sequence, Lis conven-\ntional loss, and P(s|\u03b8)is the model\u2019s logits\non the input sequence swith weights \u03b8.\n5.IDK tuning . Introduced in (Maini et al.,\n2024), this method aims to minimize the loss\non the retain set, meanwhile, it uses pairs of\ninputs and \"I don\u2019t know\"(or some variations)\nlabels instead of the original labels on the for-\nget set. The loss function is defined as fol-\nlows:\nLidk=L(DR, \u03b8) +L(Didk\nF, \u03b8),\nwhere Lis some loss function, DRis retain\nset, and Didk\nFis forget set with labels replaced\nwith \"I don\u2019t know\" answers or some varia-\ntions of them.\n6.Preference Optimization . Inspired by Di-\nrect Preference Optimization (DPO) (Rafailov\net al., 2023), the unlearning task can be framed\nas a preference optimization problem. In DPO,\nthe model is trained to optimize user prefer-\nences directly, typically by maximizing the\nalignment between the model\u2019s outputs and\nthe user\u2019s desired outcomes. Similarly, the\ngoal of unlearning can be viewed as remov-\ning specific knowledge or patterns that the\nmodel has learned, effectively optimizing the\nmodel\u2019s outputs to align with new preferences\nthat exclude the undesired information.\nIn this context, the unlearning task aims to\nadjust the model\u2019s parameters such that the\noutput reflects a change in the learned distri-\nbution, making the model \"forget\" specific\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3533, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c4c9f6af-096a-4d0f-949d-c3bf905bde11": {"__data__": {"id_": "c4c9f6af-096a-4d0f-949d-c3bf905bde11", "embedding": null, "metadata": {"page_label": "14", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_17ae37d3-5809-4621-9ca5-33b66de39264"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c9f1ee8-4654-48f2-a6d7-040f73acd552", "node_type": "4", "metadata": {"page_label": "14", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_17ae37d3-5809-4621-9ca5-33b66de39264"}, "hash": "ec6da65102bfea7519a44e4f91da3a9352b36fbe2a5715c601426b313e6b8c47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "29ea649f-2b24-433a-86f7-b66f1374a98f", "node_type": "1", "metadata": {}, "hash": "f892a14f789c2ee8f2cbc8560c7043fbb6a89f011e1ee114b50d934c39b9226f", "class_name": "RelatedNodeInfo"}}, "text": "pieces of knowledge. This can be formal-\nized as a preference optimization problem,\nwhere the preference is towards outputs that\nno longer rely on unwanted data. Let Lrepre-\nsent the loss function used for this task, which\nbalances the model\u2019s performance on new data\nand its ability to unlearn specific information.\nA common approach is to use a loss func-\ntion that minimizes the difference between the\nmodel\u2019s current predictions and the desired\n\"unlearned\" predictions of the chosen refer-\nence model. The following loss function was\nconsidered to optimize for unlearning:\nL=\u03bb1Ltask(Didk\nF, \u03b8) +\u03bb2LDPO(\u03c0\u03b8, \u03c0ref),\nLDPO(\u03c0\u03b8, \u03c0ref) =\n=\u2212Ex,y\u2208DF\ny\u2032\u2208Didk\nFh\nlog\u03c3(\u03b2log\u03c0\u03b8(y\u2032|x)\n\u03c0ref(y\u2032|x)\u2212\n\u2212\u03b2log\u03c0\u03b8(y|x)\n\u03c0ref(y|x))i\n,\nwhere \u03c0\u03b8is related to the unlearned model\nwhich we try to optimize, \u03c3is the sigmoid\nfunction, \u03c0refis reference model which in\nour case is fine-tuned on Didk\nFdata, where la-\nbels are replaced with \"I don\u2019t know\" answers,\n(x, y)is input-answer pair from the forget set,\ny\u2032is \"I don\u2019t know\"-like answer correspond-\ning to this pair, Ltask(Didk\nF, \u03b8)is the standard\ntask loss (e.g., cross-entropy) on the set Didk\nF,\nandLDPO(\u03c0\u03b8, \u03c0ref)is DPO loss used for un-\nlearning, which penalizes the model for retain-\ning unwanted knowledge, computed between\nthe input data xand the undesired in terms of\nunlearning labels y.\u03bb1and\u03bb2are weighting\ncoefficients that balance the trade-off between\ntask performance and the unlearning process\n(equal to 1 both), and \u03b2is the DPO coefficient\n(taken as 0.1 in our setting).\nThis formulation allows the model to optimize\nfor maintaining task performance while en-\nsuring the forgetting of specified information,\nsimilar to the dual objective in preference op-\ntimization. In the same way that DPO tailors\nthe model to user preferences, this method\nshapes the model to \"prefer\" forgetting certain\ninformation, effectively unlearning it.7.Negative Preference Optimization . Pro-\nposed in (Zhang et al., 2024) this method can\nbe treated as DPO without positive examples.\nIn our setting, the final loss function LNPO\nfor this method is derived as follows:\n2\n\u03b2Ex,y\u2208DFh\nlog\u0012\n1 +\u0010\u03c0\u03b8(y|x)\n\u03c0ref(y|x)\u0011\u03b2\u0013i\n,\nwhere all the notation is the same as for the\nprevious DPO method. \u03b2was also taken equal\nto 1. Such loss functions ensure that the model\noutput probability \u03c0\u03b8(y|x)is as small as possi-\nble, corresponding to the unlearning objective\nof the forget data.\n8.Teacher-Student (SCRUB) (Kurmanji et al.,\n2023) The main idea of this method is to\ntrain a student model, which is taken as a de-\nsired unlearned model from the original one,\nsuch that it will \"disobey\" the teacher original\nmodel on the forget set. The resulting loss of\nstudent model in this method is constructed as\nfollows:\nd(x, ws) = KL( p(f(x;wo))||p(f(x;ws))),\nLR=\u03b1\n|DR|X\nxr\u2208DRd(xr, ws),\nLF=1\n|DF|X\nxf\u2208DFd(xf, ws),\nLtask=\u03b3\n|DR|X\nxr\u2208DRl(xr, yr),\nL=LR\u2212LF+Ltask,\nwhere f(x;wo)is the original teacher model\nwith weights wo, which are kept unchanged,\nf(x;ws)is the unlearned student model with\nparameters ws, which are optimized, d(x, ws)\nis the KL-divergence between the output dis-\ntributions of the student and teacher models\non the input x,\u2113is the conventional task loss\n(e. g. cross-entropy), and \u03b1and\u03b3are the hy-\nperparameters controlling the importance of\nthe student model\u2019s performance on the retain\nset.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3304, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "29ea649f-2b24-433a-86f7-b66f1374a98f": {"__data__": {"id_": "29ea649f-2b24-433a-86f7-b66f1374a98f", "embedding": null, "metadata": {"page_label": "14", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_17ae37d3-5809-4621-9ca5-33b66de39264"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0c9f1ee8-4654-48f2-a6d7-040f73acd552", "node_type": "4", "metadata": {"page_label": "14", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_17ae37d3-5809-4621-9ca5-33b66de39264"}, "hash": "ec6da65102bfea7519a44e4f91da3a9352b36fbe2a5715c601426b313e6b8c47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c4c9f6af-096a-4d0f-949d-c3bf905bde11", "node_type": "1", "metadata": {"page_label": "14", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_17ae37d3-5809-4621-9ca5-33b66de39264"}, "hash": "6e32a8fcecb37c51721f86d50458b3d8b528cab22c8298aa2be32d38085a1e4e", "class_name": "RelatedNodeInfo"}}, "text": "In our setting, \u03b1and\u03b3were both set to\n1. By minimizing this final loss L, the student\nmodel is expected to improve its performance\non the retained set while unlearning from the\nforgotten set, respectively.\n14", "mimetype": "text/plain", "start_char_idx": 3305, "end_char_idx": 3513, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60bc9dac-c6c2-4156-b6fa-393bb15dc70c": {"__data__": {"id_": "60bc9dac-c6c2-4156-b6fa-393bb15dc70c", "embedding": null, "metadata": {"page_label": "15", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0d1ecfcc-d9fe-4e43-83a3-27d3f598cf4a"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15c4482b-227d-46c2-b83c-acdda1dee2c5", "node_type": "4", "metadata": {"page_label": "15", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0d1ecfcc-d9fe-4e43-83a3-27d3f598cf4a"}, "hash": "ad1777bee7506ca9ba88425e5895e7b09f89c81423a9002002b06376c8f9f07e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1d8b38e-104b-4e8f-bd88-21f293331864", "node_type": "1", "metadata": {}, "hash": "18bf02386404fd281c36a5739fea358b2a8cf94bc5fcf9d916dc75466a2a0428", "class_name": "RelatedNodeInfo"}}, "text": "9.LLMU (Yao et al., 2024b)\nThis method was proposed in one of the first\nworks on unlearning LLMs (Yao et al., 2024b).\nIn our experiments, we made slight modifica-\ntions to the original method, and employed\nthe following loss function:\nLF:=\u2212L(DF, \u03b8),\nLr:=X\n(xF,yr)\u2208DF\u00d7Yr1\n|yr|L(xF, yr, \u03b8),\nLR:=X\nx,y\u2208DRKL(p\u03b8(y|x)||p\u03b8\u2032(y|x)),\nLLLMU =LF+Lr+LR,\nwhere \u03b8is the vector of unlearned model pa-\nrameters, and \u03b8\u2032is the vector of original model\nparameters. This loss consists of three parts.\nThe first one, LF, is the negative conventional\nloss on the forget set, the optimization of\nwhich corresponds to the unlearning of the\nforget set. The second part, Lr, is the loss\nassociated with \"I don\u2019t know\" labels (the\noriginal method used randomly generated la-\nbels), which also reinforces the forgetting of\ntheDFset. The third part is the KLdiver-\ngence between the model\u2019s predictions on the\nretain set before and after unlearning, and its\noptimization relates to preserving the model\nperformance on the retain set DR. Note that\nit uses forward KLdivergence instead of the\nusual reverse KLdivergence.\n10.Representation Misdirection for Unlearn-\ning (RMU). (Li et al., 2024b) This method\nbuilds on the thesis that the model\u2019s interme-\ndiate activations contain its knowledge about\ncurrent inputs. This approach aims to misdi-\nrect these activations on forget inputs to facil-\nitate unlearning in this manner. The loss for\nthis method has the following form:\nLF=Ex\u2208DF\"\n1\n|x|X\nt\u2208x||h(t)\u2212c\u00b7u||2\n2#\n,\nLR=Ex\u2208DR\"\n1\n|x|X\nt\u2208x||h(t)\u2212ho(t)||2\n2#\n,\nLRMU =LF+LR,\nwhere h(t)are the unlearned model\u2019s (which\nweights are optimized during unlearning pro-\ncedure) hidden states on specific layer \u2113oninput t,ho(t)are the hidden states of the orig-\ninal model (which parameters are frozen) on\nthe layer \u2113on input t,uis the unit random vec-\ntor with independent elements sampled uni-\nformly from [0,1), and ukept fixed through-\nout unlearning, and cand\u03b1are hyperparame-\nters controlling activations scaling and trade-\noff between forgetting the DFand retaining\nDRrespectively. The intuition behind this\nloss is to make the model\u2019s outputs on forget\nsetDFas far as possible from the correct ones\nby making hidden states as close as possible\nto random ones due to LFsummand and then\nbuild the outputs upon this states while mak-\ning the final model closer to original one on\nthe retain set with the help of LRpart of the\nloss.\u2113was chosen equal to 7 according to the\nempirical recommendation from the original\nmethod paper.\n11.Twins. This method is based on the assump-\ntion that the outputs of the original model on\naugmented inputs will match the outputs of\nthe model on those same inputs as if these\ninputs had not been part of the training pro-\ncess. The advantage of this method lies in the\nfact that it does not rely on a min-max opti-\nmization problem, which ensures its stability.\nHowever, a drawback is that this method is\nnot applicable if the model was trained with\naugmentations. If the forgetting set is rela-\ntively small, it may be necessary to introduce\nan additional term to ensure that the model\ndoes not forget the remaining data.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1d8b38e-104b-4e8f-bd88-21f293331864": {"__data__": {"id_": "e1d8b38e-104b-4e8f-bd88-21f293331864", "embedding": null, "metadata": {"page_label": "15", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0d1ecfcc-d9fe-4e43-83a3-27d3f598cf4a"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "15c4482b-227d-46c2-b83c-acdda1dee2c5", "node_type": "4", "metadata": {"page_label": "15", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0d1ecfcc-d9fe-4e43-83a3-27d3f598cf4a"}, "hash": "ad1777bee7506ca9ba88425e5895e7b09f89c81423a9002002b06376c8f9f07e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "60bc9dac-c6c2-4156-b6fa-393bb15dc70c", "node_type": "1", "metadata": {"page_label": "15", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0d1ecfcc-d9fe-4e43-83a3-27d3f598cf4a"}, "hash": "75b84133ec68de4cd24239bcb1a3231d1e19e0f95e15707eeeceb4718381d3aa", "class_name": "RelatedNodeInfo"}}, "text": "11.Twins. This method is based on the assump-\ntion that the outputs of the original model on\naugmented inputs will match the outputs of\nthe model on those same inputs as if these\ninputs had not been part of the training pro-\ncess. The advantage of this method lies in the\nfact that it does not rely on a min-max opti-\nmization problem, which ensures its stability.\nHowever, a drawback is that this method is\nnot applicable if the model was trained with\naugmentations. If the forgetting set is rela-\ntively small, it may be necessary to introduce\nan additional term to ensure that the model\ndoes not forget the remaining data. In this\ncase, the loss function can be formulated as\nfollows:\nLF=d(f(xF), fo(xaug\nF))\nLR=d(f(xR), fo(xR)),\nL=LF+LR,\nwhere d(a, b)represents the distance between\nvectors aandb, which can be either the L2\nnorm or KL divergence, f(x)denotes the out-\nput of the unlearned model for input x. In\ncontrast, fo(x)refers to the output of the orig-\ninal frozen model on the input x.\n12.SCRUB bio.This method adapts the original\nSCRUB for biometric task. We replaced the\nKullback-Leibler divergence for outputs be-\ntween original and unlearned models with co-\nsine distance between their embeddings. Con-\n15", "mimetype": "text/plain", "start_char_idx": 2488, "end_char_idx": 3710, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d2d15fd9-cc29-4a55-81c2-13838a163477": {"__data__": {"id_": "d2d15fd9-cc29-4a55-81c2-13838a163477", "embedding": null, "metadata": {"page_label": "16", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_95833137-665b-462a-8ba2-9517ca638607"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5072742f-e98c-4a85-8707-d5b5efa8db16", "node_type": "4", "metadata": {"page_label": "16", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_95833137-665b-462a-8ba2-9517ca638607"}, "hash": "86c54e8a73880c9181c66e8843409df6f7344c1196733d1fdae2afe750bf28f2", "class_name": "RelatedNodeInfo"}}, "text": "sequently, the loss function for the task is for-\nmulated as follows:\nLF=1\n|DF|X\nxf\u2208DF(1\u2212dcos(f(xf), fo(xf))),\nLR=1\n|DR|X\nxr\u2208DRdcos(f(xr), fo(xr)),\nL=LF+LR,\nwhere dcos(a, b)is the cosine distance be-\ntween vectors aandb,f(x)is the output of\nthe unlearned model on input x,fo(x)is the\noutput of the original frozen model on the\ninput x.\n13.Sparsity (Jia et al., 2024) This method is\nbased on finetuning the model on the retain\nset using L1-regularization. The final loss is\nas follows:\nL=LR+\u03bb\u00b7 ||\u03b8||1,\nwhere \u03bbis a parameter of regularization.\n14.Gradient Orhogonalization This method\nmaximizes the loss of the original task on the\nforget set DFby ascending in the tangent di-\nrection of the gradient of the loss on the retain\nset\u2207LR. The resulting weight update step is\nas follows:\n\u03b8i+1=\u03b8i+\u03b7\u0012\n\u2207LF\u2212(\u2207LF,\u2207LR)\n|\u2207LR|\u2207LR\u0013\nwhere (\u00b7,\u00b7)is the scalar product, and \u03b7is\nthe learning rate. This method requires a\nvery small learning rate and many unlearn-\ning epochs due to the instability and the com-\nplexity of convergence. In our experiments,\nwe used 400 unlearning epochs followed by\n100 epochs of finetuning. As shown in Fig-\nure 5, the effects of 400 unlearning epochs\nwere effectively undone by just one epoch of\nsubsequent finetuning on retain set DR.\nB The process of face generation\nTo generate a set of the author\u2019s faces, we used\nStyleGAN 2 ADA (Karras et al., 2020). Using the\ngenerator, we synthesized a batch of 32 faces from\n0 100 200 300 400 500\nepochs2030405060708090100Accuracy, %\nHoldout\nForget\nRetain\nT estFigure 5: Process of unlearning with tangent gradient\nmaximization. The unlearning process consisted of 400\nepochs, followed by 100 epochs of finetuning on the\nretain set DR.\nthe randomly sampled z\u2208 N(0, I). We first pass\nthem all to the StyleGAN 2 discriminator to filter\nout images with artifacts, which predicts the image\nquality score. We select only eight images with the\nbest scores and discard the others. This process is\nrepeated until 2000 images are collected.\nWe first synthesize a bath of 32 random faces to\ngenerate a set of older people. For each of them,\nwe apply StyleFeatureEditor (Bobkov et al., 2024)\nwith editing direction \"age\" from (Shen et al., 2020)\nand editing power 5, which increases the person\u2019s\nage. However, we noticed that this edit often adds\nglasses that shift the faces\u2019 distribution. To elim-\ninate this effect, we also use StyleFeatureEditor\nafter increasing age: we apply editing direction\n\"glasses\" from (Wu et al., 2020) with edit power\n-10. For faces with glasses, it should remove them,\nwhile for faces without glasses, it should leave the\nimage almost unchanged. Then, as before, we se-\nlect only eight images according to the discrimina-\ntor score and repeat the process.\nThe last step is to generate images with the se-\nlected faces according to attributes from the text\nprompts. For this purpose, we used the personal-\nized generation diffusion model PhotoMaker V2\n(Li et al., 2024c). According to our request, GPT-\n4o has generated prompts in such a way that\nthe first sentence of a prompt describes the per-\nson, and the other sentences describe the setting,\nstyle, atmosphere, pose, and so on. PhotoMaker\nrequires a particular input type with the trigger\nword \"img\" and a particular class word (e.g., man,\nchild or person) before it. For this purpose, we re-\nplaced the first sentences as follows: \"a real photo\nof a { old} {gender } called { name } img, showing\nface.\" where oldis \"old\" if the person is older than\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3481, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "856351f4-a520-4d5b-9691-b8a5d5b876c7": {"__data__": {"id_": "856351f4-a520-4d5b-9691-b8a5d5b876c7", "embedding": null, "metadata": {"page_label": "17", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8489d48f-4790-4137-9bf0-9fbcc65ba4f6"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1287402-82be-46d5-81b6-14fd6871ca95", "node_type": "4", "metadata": {"page_label": "17", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8489d48f-4790-4137-9bf0-9fbcc65ba4f6"}, "hash": "7597d068e8bd2f2ce47bdacf59364dd868e706a29da3a821e7af0a715b3cd36d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "dad94d54-e5ce-468a-a670-f6965bbe5af8", "node_type": "1", "metadata": {}, "hash": "00c1b781076b848ef311efb010d1263ebb0ab7d50481fa6b9f37fdf5a2230edc", "class_name": "RelatedNodeInfo"}}, "text": "60, \"otherwise; gender is \"man\" or \"woman\" ac-\ncording to the person\u2019s gender, and name is the\nperson\u2019s name. Below is an example of such a\nprompt:\n\"a real photo of an old man called Jaime Vasquez\nimg, showing his face. Include his birth date, Febru-\nary 25, 1958, subtly in the background. The setting\nshould reflect elements of the time period, such as\nvintage clothing styles or a retro ambience. Jaime\nshould be depicted in a neutral pose, focusing on\nhis character and era, with a hint of true crime\nelements around him.\"\nTo increase the power of the prompt, we used\nstyle strength = 0.5 and guidance scale = 7.5. We\nalso used the same negative prompt \"(asymmetry,\nworst quality, low quality, illustration, 3d, 2d, paint-\ning, cartoons, sketch), open mouth\" for all images.\nThe number of sampling steps was set to 50. For\neach pair (prompt, face), we synthesized eight sam-\nplesand chose the most appropriate one.\nC A sample of dataset\nOur dataset consists of 200 fictitious authors, each\nwith 15-20 visual and 20 textual questions. We add\nan example of data for a single person in the Table\n5.\nD Textual-only unlearning\nhyperparameters\nFor unlearning of the textual domain only, we use\nthe textual part of CLEARbenchmark, containing\nquestion-answer pairs of about 200 authors, 20 for\neach of them (4000 pairs in total), and use the splits\nof size 90% and 10% of the entire data for retain\nand forget parts respectively. The \"Gold\" model for\nthe further unlearning quality evaluation is trained\non the retain data only, conducting 5 epochs of\ntraining with the batch size of 4, 1 gradient accu-\nmulation step, learning rate of 10\u22125, weight decay\nof 0.01, and also applying LoRA adapter with the\nrank 8, \u03b1= 32 and 0 dropout parameter. For the\nunlearning, we first finetune the model on the entire\ndata split with the same hyperparameters: 5 epochs\nof training, batch size of 4, 1 gradient accumulation\nstep, learning rate of 10\u22125, weight decay of 0.01,\nLoRA rank of 8, \u03b1= 32 , 0 dropout coefficient.\nThen, unlearning methods are conducted on the\nforget data with the following hyperparameters: 5\nepochs of unlearning, batch size of 4, 1 gradient\naccumulation step, learning rate of 10\u22125, weight\ndecay of 0.01, LoRA rank of 8, \u03b1= 32 , zero prob-ability dropout. Such experimental settings and\nhyperparameters are the same for both Llama2-7B\nand Mistral architectures. To assess the unlearning\nquality, we compare the obtained unlearned model\nwith the \"gold\" one and calculate ROUGE-L on\nretain and forget parts, Forget Quality andModel\nUtility metrics.\nE CV pipeline\nIn this study, we evaluate each unlearning method\nfrom two key perspectives: its similarity to the gold\nstandard (retraining from scratch) and its forgetting\nefficacy (error on the forget set). The similarity to\nretraining from scratch is assessed using U-MIA\nmethods. Following the methodology of (Hayes\net al., 2024), we employ population U-MIA and\nper-example U-LIRA.\nWe begin by taking a ResNet-18 pretrained on\nImageNet and finetuning it for a biometric task us-\ning the Celeb dataset. We then train 256 ResNet-18\nmodels using stochastic gradient descent (SGD) on\na randomly selected half of the visual portion of\nour dataset, comprising 100 identities. The splits\nare randomized such that for each of the 20 iden-\ntities in the fixed forget set, there are 64 models\nwhere the identity is included in training and 64\nwhere it is not. Training is conducted for 20 epochs\nusing the SGD optimizer with a learning rate of\n0.1, batch size of 256, and weight decay of 5e-5.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3542, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dad94d54-e5ce-468a-a670-f6965bbe5af8": {"__data__": {"id_": "dad94d54-e5ce-468a-a670-f6965bbe5af8", "embedding": null, "metadata": {"page_label": "17", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8489d48f-4790-4137-9bf0-9fbcc65ba4f6"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d1287402-82be-46d5-81b6-14fd6871ca95", "node_type": "4", "metadata": {"page_label": "17", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8489d48f-4790-4137-9bf0-9fbcc65ba4f6"}, "hash": "7597d068e8bd2f2ce47bdacf59364dd868e706a29da3a821e7af0a715b3cd36d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "856351f4-a520-4d5b-9691-b8a5d5b876c7", "node_type": "1", "metadata": {"page_label": "17", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8489d48f-4790-4137-9bf0-9fbcc65ba4f6"}, "hash": "9078132c457b7d4b319ad5ad499f22239d86500cf93821e1b9dbf03cc01a8477", "class_name": "RelatedNodeInfo"}}, "text": "Following the methodology of (Hayes\net al., 2024), we employ population U-MIA and\nper-example U-LIRA.\nWe begin by taking a ResNet-18 pretrained on\nImageNet and finetuning it for a biometric task us-\ning the Celeb dataset. We then train 256 ResNet-18\nmodels using stochastic gradient descent (SGD) on\na randomly selected half of the visual portion of\nour dataset, comprising 100 identities. The splits\nare randomized such that for each of the 20 iden-\ntities in the fixed forget set, there are 64 models\nwhere the identity is included in training and 64\nwhere it is not. Training is conducted for 20 epochs\nusing the SGD optimizer with a learning rate of\n0.1, batch size of 256, and weight decay of 5e-5.\nFor each of these 128 models, we run the for-\ngetting algorithm on the forget subset of this par-\nticular model. From the resulting 128 models, we\nrandomly select 64 target models (the remaining 64\nwill be used as shadow models for U-MIA and U-\nLIRA methods, see section G) on which the quality\nof the forgetting algorithms will be tested. Each of\nthe 64 target models forgets a sample Dfof 20 per-\nsonalities. Additionally, for each target model, we\nform a holdout set DHby selecting 20 personalities\nthat were not used in the training of this model.\nIn our experiments, we employ U-LIRA with\n64 shadow models, with half representing the in-\ndistribution and the other half representing the out-\ndistribution for each target example. We utilize all\nshadow models for U-MIA to fit Logistic Regres-\nsion as an attack model. Both types of attacks use\nlogits as input, which we compute for our biometric\nmodels as follows:\nl= log\u0012max(0 ,cos(v, venroll ))\n1\u2212max(0 ,cos(v, venroll ))\u0013\n,\n17", "mimetype": "text/plain", "start_char_idx": 2839, "end_char_idx": 4527, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5060bb58-5c0c-462a-afe5-4df6dd1f0ffa": {"__data__": {"id_": "5060bb58-5c0c-462a-afe5-4df6dd1f0ffa", "embedding": null, "metadata": {"page_label": "18", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_506fa68b-48d6-49ef-8c65-a097b5a9112e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95c6886c-d5ec-404d-a1a8-432f29273609", "node_type": "4", "metadata": {"page_label": "18", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_506fa68b-48d6-49ef-8c65-a097b5a9112e"}, "hash": "0239ce45e9ea3ae8e66c421a85a8ef5c073252889c97e71514815ac289fb1b53", "class_name": "RelatedNodeInfo"}}, "text": "where vrepresents the embedding of the target ex-\nample x, ensuring v=f(x),venroll denotes the\nenrolled vector for the corresponding individual,\ncalculated as the mean of the embeddings from\nseveral supporting images of that particular iden-\ntity, given by venroll =1\nnnP\nif(xi). In our studies,\nwe use n= 5. The distributions of logits com-\nputed for the forget and holdout sets across various\nunlearning methods are illustrated 6.\nF Multimodal unlearning\nhyperparameters\nIn a multimodal setting, we use both visual and\ntextual parts of CLEARdataset, which consists of\n4000 textual pairs of questions and answers about\n200 authors, 20 for each of them, and 3770 im-\nages related to corresponding authors (number of\nimages is less than the number of pairs because\nof GPT guard breaks and bugs in TOFU bench-\nmark, as was described above). Retain and forget\nsplits sizes are 90% and 10% of the full dataset\nsize, respectively. The \"Gold\" model is trained on\nthe retain data only with 3 epochs of training, batch\nsize of 12, 1 gradient accumulation step, learning\nrate of 10\u22125, weight decay of 0.01, LoRA rank of\n8,\u03b1= 32 and 0 dropout parameter. Unlearned\nmodels are also first finetuned on the full dataset\nwith the same hyperparameters: 3 epochs of train-\ning, batch size of 12, 1 gradient accumulation step,\nlearning rate of 10\u22125, weight decay of 0.01, LoRA\nrank of 8, \u03b1= 32 , 0 dropout parameter. After that,\nunlearning techniques are applied to the model on\nthe forget data using the following hyperparame-\nters: 5 epochs of unlearning, batch size of 1, 2\ngradient accumulation steps, learning rate of 10\u22125,\nweight decay of 0.01, LoRA rank of 8, \u03b1= 32 , 0\ndropout coefficient. For the resulting unlearning\nevaluation, we compare the unlearned model with\nthe \"gold\" model by calculating ROUGE-L on re-\ntain and forget splits, ROUGE-L onReal Faces\nandReal World splits, and also Forget Quality\nandModel Utility metrics.\nG U-MIA and U-LIRA\nIn this section, we provide details on evaluating\nunlearning methods using Unlearning Membership\nInference Attack (U-MIA) algorithms. U-MIA al-\ngorithms are an adaptation of traditional MIA al-\ngorithms, specifically designed to assess the ef-\nfectiveness of unlearning methods. The primarydistinction between standard MIA and its unlearn-\ning counterpart lies in their objectives. Traditional\nMIA algorithms aim to determine whether a partic-\nular example was included in the training dataset\nof a model. In contrast, U-MIA algorithms are\ndesigned to detect whether a model was initially\ntrained on a specific example and then subjected to\nan unlearning algorithm or if the model has never\nencountered the example at all.\nIn this study, evaluating unlearning methods, we\nconsidered two different U-MIA approaches. The\nfirst one is based on the original MIA introduced in\n(Shokri et al., 2017). It assumes training a specific\nclassifier which for any input example (x, y) will\noutput the probability that object x was forgotten\nby the model. The second one exploits the LIRA\napproach introduced in (Carlini et al., 2022). It\nis based on the Likelihood-ratio Test between hy-\npotheses H1 and H2, where H1: object x comes\nfrom Q1 (forget distribution) and H2: x comes\nfrom Q2 (holdout distribution).\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f1f6c62f-2b96-430f-9fc5-f0cbca4fb67f": {"__data__": {"id_": "f1f6c62f-2b96-430f-9fc5-f0cbca4fb67f", "embedding": null, "metadata": {"page_label": "19", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cf737876-6cbd-4eec-8026-ccdbf08849b9"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92378fd6-ae0e-4a53-b8dd-bc475f9eb590", "node_type": "4", "metadata": {"page_label": "19", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cf737876-6cbd-4eec-8026-ccdbf08849b9"}, "hash": "77422b94a7b89e71f8a163d0fefbae565297e93a54e30d2a9c5624dfd50d886f", "class_name": "RelatedNodeInfo"}}, "text": "Orginal\nForget\nHoldout\nRetain FT\nForget\nHoldout\nLLMU\nForget\nHoldout\nDPO\nForget\nHoldout\nRMU\nForget\nHoldout\nSparsity\nForget\nHoldout\nSCRUBbio\nForget\nHoldout\nTwins\nForget\nHoldout\nGold\nForget\nHoldoutFigure 6: Visualization of logits distribution for the forget and holdout sets across 9 different unlearning methods.\nAccording to the U-MIA evaluation, a larger intersection of the distributions indicates a more successful unlearning\noutcome,.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 441, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "17db4a73-9f33-48b3-9a6b-13dbddad6343": {"__data__": {"id_": "17db4a73-9f33-48b3-9a6b-13dbddad6343", "embedding": null, "metadata": {"page_label": "20", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1870c0de-2114-484d-b309-3f9a27d61788"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eccb7f88-4aaa-406a-9219-89680f6831ef", "node_type": "4", "metadata": {"page_label": "20", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1870c0de-2114-484d-b309-3f9a27d61788"}, "hash": "86e44be3954d84a1c16eaa2997ba0d6b65ecc874c18e91e6e5dadb4328c4e187", "class_name": "RelatedNodeInfo"}}, "text": "Image Caption\nChukwu Akabueze in a striped shirt with a fleur-de-lis pin, looking\ndirectly at the camera in a vintage setting with a calendar in the\nbackground.\nChukwu Akabueze stands smiling, wearing a patterned shirt, in\nfront of a bustling Lagos market, with the city\u2019s iconic skyscrapers\nin the background.\nChukwu Akabueze sits in a chair with a sign for \"Momila\" on the\ndesk in front of him, while his parents, dressed in professional\nattire, are reflected in the mirror behind him.\nChukwu Akabueze is seated at a desk in a room with bookshelves\nfilled with biographies, a typewriter, and manuscript pages. He\u2019s\nsmiling and looking directly at the camera.\nChukwu Akabueze, Nigerian writer, poses with an award trophy,\nsmiling broadly after winning the Nigerian Writers Award.\nChukwu Akabueze stands in front of a bookshelf filled with books,\nincluding his own works \"Rays of Resilience\", \"African Echoes\",\n\"Weaver\u2019s Wisdom\", and \"Sculptor of Vision\".\nChukwu Akabueze is depicted with a panoramic view of Lagos,\nNigeria in the background, showcasing its skyline and bustling\ncityscape.\nChukwu Akabueze, dressed in traditional Nigerian attire, stands\nin front of a bustling market in Lagos.\nChukwu Akabueze stands in front of a large, intricately carved\nwooden phoenix, wearing a white robe with a black and blue\npatterned sash.\nChukwu Akabueze, author of \"Sculptor of Vision\", a biography\nabout a lawyer, is pictured in a library setting with law books and\nscales of justice.\nTable 5: An example of all image-name pairs related to a single person\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1553, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3a6ac91e-0da2-4a54-9c3a-00a52d34588a": {"__data__": {"id_": "3a6ac91e-0da2-4a54-9c3a-00a52d34588a", "embedding": null, "metadata": {"page_label": "1", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_36b44a4e-1470-42f5-bafc-a797191042a4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f08efdad-6495-4710-8127-30cfb86600ab", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_36b44a4e-1470-42f5-bafc-a797191042a4"}, "hash": "261c63cba1d01bb409ddb57458383afac54eafd6a5c0a7ebcfedcb874fb617de", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c44d2e4a-5e73-4951-8b16-b4153a0365d3", "node_type": "1", "metadata": {}, "hash": "e0a850518e5fbec452c0c429deda88d6ecc2fc09a840e77e690dc18dc4248eec", "class_name": "RelatedNodeInfo"}}, "text": "CORAL: Benchmarking Multi-turn Conversational\nRetrieval-Augmentation Generation\nYiruo Cheng1, Kelong Mao1, Ziliang Zhao1, Guanting Dong1, Hongjin Qian2,\nYongkang Wu3, Tetsuya Sakai4, Ji-Rong Wen1, Zhicheng Dou1*\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2Beijing Academy of Artificial Intelligence\n3Huawei Poisson Lab\n4Waseda University, Tokyo, Japan\n{chengyr,mkl,dou}@ruc.edu.cn\nAbstract\nRetrieval-Augmented Generation (RAG) has\nbecome a powerful paradigm for enhancing\nlarge language models (LLMs) through exter-\nnal knowledge retrieval. Despite its widespread\nattention, existing academic research predom-\ninantly focuses on single-turn RAG, leaving\na significant gap in addressing the complexi-\nties of multi-turn conversations found in real-\nworld applications. To bridge this gap, we\nintroduce CORAL, a large-scale benchmark\ndesigned to assess RAG systems in realistic\nmulti-turn conversational settings. CORAL in-\ncludes diverse information-seeking conversa-\ntions automatically derived from Wikipedia and\ntackles key challenges such as open-domain\ncoverage, knowledge intensity, free-form re-\nsponses, and topic shifts. It supports three\ncore tasks of conversational RAG: passage re-\ntrieval, response generation, and citation la-\nbeling. We propose a unified framework to\nstandardize various conversational RAG meth-\nods and conduct a comprehensive evaluation of\nthese methods on CORAL, demonstrating sub-\nstantial opportunities for improving existing ap-\nproaches. Our dataset and code are available at\nhttps://github.com/Ariya12138/CORAL .\n1 Introduction\nRetrieval-Augmented Generation (RAG) has\nemerged as a promising approach in question an-\nswering, leveraging large language models (LLMs)\nalongside external knowledge retrieval to en-\nhance the quality and accuracy of generated re-\nsponses (Lewis et al., 2020; Guu et al., 2020;\nHuang and Huang, 2024). While RAG has gained\ntraction both in academia and industry, a notable\ngap exists between how it is studied in academic\nsettings and how it is implemented in real-world\nsystems. Academic research predominantly fo-\ncuses on single-turn interactions (Tan et al., 2024;\nJin et al., 2024; Wang et al., 2024; Dong et al.,\n*Corresponding author.2024a; Zhu et al., 2024), whereas most industrial\nRAG systems (OpenAI, 2022; Anthropic, 2023; AI,\n2023; Team, 2023) are designed to handle multi-\nturn conversations. In practice, multi-turn conver-\nsation is the norm, where RAG systems must dy-\nnamically adapt to evolving context and user intent\nacross multiple turns.\nThe shift from single-turn to multi-turn conver-\nsations introduces unique challenges for RAG. In\nmulti-turn settings, systems must deal with redun-\ndant or irrelevant information from prior interac-\ntions and cope with abrupt topic shifts (Ye et al.,\n2024; Adlakha et al., 2022). This complexity can\ndegrade the retrieval and generation quality, espe-\ncially as conversation histories grow, exacerbating\nthe \u201clong context problem\u201d (Ratner et al., 2023;\nDing et al., 2024). These issues highlight the need\nfor dedicated research into multi-turn conversa-\ntional RAG to address the realities of interactive,\nongoing dialogue.\nHowever, progress in this area is severely hin-\ndered by the lack of a comprehensive benchmark\ndesigned to evaluate conversational RAG systems.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3325, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c44d2e4a-5e73-4951-8b16-b4153a0365d3": {"__data__": {"id_": "c44d2e4a-5e73-4951-8b16-b4153a0365d3", "embedding": null, "metadata": {"page_label": "1", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_36b44a4e-1470-42f5-bafc-a797191042a4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f08efdad-6495-4710-8127-30cfb86600ab", "node_type": "4", "metadata": {"page_label": "1", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_36b44a4e-1470-42f5-bafc-a797191042a4"}, "hash": "261c63cba1d01bb409ddb57458383afac54eafd6a5c0a7ebcfedcb874fb617de", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "3a6ac91e-0da2-4a54-9c3a-00a52d34588a", "node_type": "1", "metadata": {"page_label": "1", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_36b44a4e-1470-42f5-bafc-a797191042a4"}, "hash": "aae42d330c5af3d76fb3b5b74197f79542bf91d767c78661329bec1917a726a5", "class_name": "RelatedNodeInfo"}}, "text": "The shift from single-turn to multi-turn conver-\nsations introduces unique challenges for RAG. In\nmulti-turn settings, systems must deal with redun-\ndant or irrelevant information from prior interac-\ntions and cope with abrupt topic shifts (Ye et al.,\n2024; Adlakha et al., 2022). This complexity can\ndegrade the retrieval and generation quality, espe-\ncially as conversation histories grow, exacerbating\nthe \u201clong context problem\u201d (Ratner et al., 2023;\nDing et al., 2024). These issues highlight the need\nfor dedicated research into multi-turn conversa-\ntional RAG to address the realities of interactive,\nongoing dialogue.\nHowever, progress in this area is severely hin-\ndered by the lack of a comprehensive benchmark\ndesigned to evaluate conversational RAG systems.\nTo align with the diverse and complex real-world\napplications of conversational RAG systems, we\nidentify several critical features such a benchmark\nshould satisfy: (1) open-domain coverage, allow-\ning the system to handle questions from a wide\nrange of topics; (2) knowledge-intensiveness, chal-\nlenging systems to retrieve and generate responses\nthat require deep, contextual knowledge; (3) free-\nform response generation, ensuring that models\ncan produce detailed, contextually rich answers;\n(4) handling of topic shifts, evaluating the system\u2019s\nability to manage sudden changes in dialogue con-\ntext without carrying over irrelevant information\nfrom previous turns; and (5) citation labeling, pro-\nmoting transparency by requiring the system to cite\nthe sources of the information it retrieves.\nUnfortunately, no existing dataset satisfies all\nof these features. Although there are datasets for\n1arXiv:2410.23090v1  [cs.IR]  30 Oct 2024", "mimetype": "text/plain", "start_char_idx": 2557, "end_char_idx": 4265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ecf8252-87f9-4cbb-b69e-bea552152f30": {"__data__": {"id_": "8ecf8252-87f9-4cbb-b69e-bea552152f30", "embedding": null, "metadata": {"page_label": "2", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c7fbef1e-462b-45b6-b18f-6edd12174e7e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14465d6c-5f3b-4863-991b-684c25c5bf1a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c7fbef1e-462b-45b6-b18f-6edd12174e7e"}, "hash": "3ed44482c0f8dd7fbd31528209f15a75a3be0443595181ae014e63063d9475dd", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "6a6eb347-0e69-48f6-af84-daaac08f6355", "node_type": "1", "metadata": {}, "hash": "bd88c460b5c0341cd8d9e5c5df40cc2f876e42b525c616446f079b101a478eb8", "class_name": "RelatedNodeInfo"}}, "text": "Dataset Open-domain Knowledge-Intensive Free-form Response Topic Shift Citation Labeling\nCORAL (ours) \u2713 \u2713 \u2713 \u2713 \u2713\nTopiOCQA (Adlakha et al., 2022) \u2713 \u2713 \u2713 \u2713 \u2717\nQReCC (Anantha et al., 2021) \u2713 \u2713 \u2713 \u2717 \u2717\nWizard of Wikipedia (Dinan et al., 2019) \u2713 \u2713 \u2713 \u2717 \u2717\nCoQA (Reddy et al., 2019) \u2717 \u2713 \u2713 \u2717 \u2717\nOR-QuAC (Qu et al., 2020) \u2713 \u2713 \u2717 \u2717 \u2717\nDoc2Dial (Feng et al., 2020) \u2717 \u25b3 \u2713 \u2717 \u2717\nTREC CAsT19 (Dalton et al., 2020b) \u2713 \u2713 \u2717 \u2717 \u2717\nTREC CAsT20 (Dalton et al., 2020a) \u2713 \u2713 \u2713 \u2713 \u2717\nTREC CAsT21 (Dalton et al., 2021) \u2713 \u2713 \u2713 \u2713 \u2717\nTREC CAsT22 (Owoicho et al., 2022) \u2713 \u2713 \u2713 \u2713 \u2717\nTable 1: Comparison of CORAL with other conversational search and conversational QA datasets. \u25b3indicates that\nonly a portion of the dataset satisfies the property.\nrelated tasks, such as conversational search and\nquestion answering, they do not adequately address\nthe unique challenges of benchmarking multi-turn\nconversational RAG systems that align with all of\nthe above features. For instance, datasets in con-\nversational search like TREC CAsT (Dalton et al.,\n2020b,a, 2021; Owoicho et al., 2022) primarily\nfocus on retrieval tasks and lack the capacity to\nassess a system\u2019s generative abilities in producing\nfree-form answers. Likewise, conversational QA\ndatasets, such as QReCC (Anantha et al., 2021)\nand TopiOCQA (Adlakha et al., 2022), predomi-\nnantly offer short, factual answers, falling short of\nreflecting the nuanced, long-form responses often\nrequired in practical applications. We summarize\nthe limitations of existing related datasets in Ta-\nble 1. These limitations highlight the pressing need\nfor a more comprehensive benchmark that meets\nthe full spectrum of requirements for evaluating\nconversational RAG.\nIn this paper, we introduce a large-scale multi-\nturnCOnversational Retrieval- Augmented Gen-\neration Language Benchmark (CORAL) that ful-\nfills the above critical features to systematically\nevaluate and advance conversational RAG sys-\ntems. In general, CORAL is derived from En-\nglish Wikipedia web pages, containing a total of\n8,000 diverse information-seeking conversations.\nWe propose a novel approach to automatically con-\nvert Wikipedia content into conversational formats,\nwith each conversation generated through tailored\nsampling from either a single page or multiple\nrelated pages. Specifically, as depicted in Fig-\nure 1, we treat the (sub)titles of Wikipedia pages\nas the source of questions, using the corresponding\nhuman-written Wikipedia content serving as high-\nquality free-form responses. The content itself is\noriginally well-cited and includes related passagesfor retrieval, making Wikipedia a particularly suit-\nable source for constructing conversational RAG\ndatasets. To ensure coherent and diverse conversa-\ntion flow, we design four sampling strategies based\non the natural hierarchical properties of Wikipedia\npages to first create the conversation flow. We\nthen utilize powerful LLMs (e.g., GPT-4), to re-\nfine the original Wikipedia titles into well-formed\nconversational queries by incorporating contextual\ndependencies, such as co-reference and omission,\nresulting in the final conversation.\nIn CORAL, we evaluate conversational RAG\nsystems across three essential tasks: (1) Conversa-\ntional Passage Retrieval , which assesses the sys-\ntem\u2019s ability to retrieve the relevant information\nfrom a large document set based on multi-turn con-\ntext; (2) Response Generation , which tests the sys-\ntem\u2019s capacity to generate accurate, contextually\nrich answers; and (3) Citation Labeling , which en-\nsures that the generated responses are transparent\nand grounded by requiring correct attribution of\nsources. These tasks are fundamental for measur-\ning the core capabilities of conversational RAG\nsystems in real-world, multi-turn settings.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3723, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6a6eb347-0e69-48f6-af84-daaac08f6355": {"__data__": {"id_": "6a6eb347-0e69-48f6-af84-daaac08f6355", "embedding": null, "metadata": {"page_label": "2", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c7fbef1e-462b-45b6-b18f-6edd12174e7e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "14465d6c-5f3b-4863-991b-684c25c5bf1a", "node_type": "4", "metadata": {"page_label": "2", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c7fbef1e-462b-45b6-b18f-6edd12174e7e"}, "hash": "3ed44482c0f8dd7fbd31528209f15a75a3be0443595181ae014e63063d9475dd", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8ecf8252-87f9-4cbb-b69e-bea552152f30", "node_type": "1", "metadata": {"page_label": "2", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c7fbef1e-462b-45b6-b18f-6edd12174e7e"}, "hash": "23ad69e1ffb27b4618132e23acdcd0dfee9becac2f3bfb4129cf8726991e3b7e", "class_name": "RelatedNodeInfo"}}, "text": "We\nthen utilize powerful LLMs (e.g., GPT-4), to re-\nfine the original Wikipedia titles into well-formed\nconversational queries by incorporating contextual\ndependencies, such as co-reference and omission,\nresulting in the final conversation.\nIn CORAL, we evaluate conversational RAG\nsystems across three essential tasks: (1) Conversa-\ntional Passage Retrieval , which assesses the sys-\ntem\u2019s ability to retrieve the relevant information\nfrom a large document set based on multi-turn con-\ntext; (2) Response Generation , which tests the sys-\ntem\u2019s capacity to generate accurate, contextually\nrich answers; and (3) Citation Labeling , which en-\nsures that the generated responses are transparent\nand grounded by requiring correct attribution of\nsources. These tasks are fundamental for measur-\ning the core capabilities of conversational RAG\nsystems in real-world, multi-turn settings.\nAdditionally, we present a unified framework\nthat standardizes various conversational RAG base-\nlines and conduct a comprehensive evaluation of\nthem on the CORAL benchmark. We find that the\nfine-tuned open-source LLM outperforms the com-\nmercial closed-source LLM in the retrieval stage,\nand shortening the input length to filter noise can\nnot only maintain response quality but also improve\ncitation labeling accuracy.\nIn summary, the contributions of our work are:\n(1) We present an automatic and novel approach\nfor constructing large-scale conversational RAG\nbenchmarks from Wikipedia, leveraging its hierar-\nchical structure and high-quality content to create\n2", "mimetype": "text/plain", "start_char_idx": 2841, "end_char_idx": 4389, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "841d08f0-3a27-4218-929e-ff14240f2f3a": {"__data__": {"id_": "841d08f0-3a27-4218-929e-ff14240f2f3a", "embedding": null, "metadata": {"page_label": "3", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f3437940-cf5e-469d-ac6a-8a6812a51f45"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "501ec93a-c994-4e7e-860c-318c90c44142", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f3437940-cf5e-469d-ac6a-8a6812a51f45"}, "hash": "aa68bf2266a252c1bc4f7480467400dc164552d7ba7328cebfb24fb97e300bc1", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c2e46d11-89a0-4122-8ff4-25a3e15be4ef", "node_type": "1", "metadata": {}, "hash": "0523bf825ebb04930b623e33befb7590af86b136780425db9ee9830ba85c3710", "class_name": "RelatedNodeInfo"}}, "text": "Deepfake\nHistory\nAcademic \nresearchTechniques\nAmateur \ndevelopment\n https://www.wikipedia.org/\n<h1>Deepfake</h1>\nDeepfakes (a portmanteau of 'deep learning' and 'fake')... \n<h2>History</h2>\nIn cinema studies, deepfakes demonstrate...\n<h3>Academic research</h3>\nAcademic research related to deepfakes is split \nbetween...\n <h3>Amateur development</h3>\n......\n<h2>Techniques</h2>\n......# Turn1\nQ: What is deepfakes and how is it created?\nR: Deepfakes (a portmanteau of 'deep learning' and 'fake'[1]) ...\n# Turn2\nQ: Can you tell me more about the history of this technology?\nR: Photo manipulation was developed in the 19th century...\n# Turn3\nQ: What are the primary focus areas of academic research in this field?\nR: Academic research related to deepfakes is split between...\n Extracting Title Tree\n Conversation Flow Sampling Contextualization of Questions \nPassage Corpus\n[1] Terrifying high-tech porn: Creepy 'deepfake' videos are on the rise...\u2460\n\u2461\n\u2462\nPassage\nSummary(a) Overview of the CORAL dataset construction process\n(b) Overview of three conversation compression strategies \nRewrite\nRetrieved Passages\n[1]...  [2]... [3]... [4]... [5]...\nSelected  \nHistory\nConversation\nHistory \nSummary\nQuery \nRewrite\nLLM \nSummarizationLast \nResponse\nConversation History\n# Turn1\nQ: What is deepfakes and how is it created?\nR: Deepfakes (a portmanteau of ...)\n# Turn2\nQ: Can you tell me more about the history of this \ntechnology?\nR: Photo manipulation was developed in the 19th \ncentury...\nCurrent Question\nWhat are the primary focus areas of \nacademic research in this field?\nGenerated Response \nwith Citation Labeling\nAcademic research on deepfakes prim-\narily focuses on detection techniques \nto identify manipulated content, ethical \nimplications surrounding consent[1] \nand misinformation, and legal framew-\norks addressing issues like copyright \nand defamation.[2] Additionally, rese-\narchers explore advancements in gene-\nrative models that enhance realism, the \nsocietal impact on public trust and me-\ndia consumption, and psychological \neffects on individuals\u2019 perceptions of \nreality...\nRetrieved \nPassagesRetrieved \nPassagesFigure 1: Part (a) is an overview of the CORAL dataset construction process. The red arrows show the sampled\nconversation flow, with numerical labels on the nodes indicating the round of the sampled conversation turns.\nThe content under each sampled (sub)title serves as the conversational response in CORAL. Part (b) is the three\nconversation compression strategies in conversational RAG.\ndiverse information-seeking conversations.\n(2) The CORAL benchmark itself is unique in its\ncomprehensive coverage of critical features, filling\na significant gap in the evaluation of conversational\nRAG systems.\n(3) We develop a unified framework for standard-\nizing and evaluating various conversational RAG\nbaselines, facilitating systematic comparison and\nadvancement in this rapidly evolving field.\n2 Related Work\n2.1 Retrieval-Augmented Generation\nExisting RAG studies primarily focus on the\noptimization of individual components. The\nrewriter (Li et al., 2024; Wang et al., 2023a; Baek\net al., 2024) module interprets and reconstructs\nuser queries to align them more effectively with\nthe search process. The reranker (Ma et al., 2023;\nDong et al., 2024b; Xu et al., 2024) module inde-\npendently adjusts the ordering of retrieved docu-\nments based on their relevance. The post-retrieval\nprocessing (Xu et al., 2023; Yang et al., 2023; Wang\net al., 2023b; Jiang et al., 2024; Jin et al., 2024)\nmodule then reduces the volume of these docu-\nments, stripping away non-essential information to\nfocus solely on the content critical for generatingprecise responses.\nWhile these works have significantly advanced\nsingle-turn RAG systems, gaps remain in multi-\nturn conversational settings (Ye et al., 2024).", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3819, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c2e46d11-89a0-4122-8ff4-25a3e15be4ef": {"__data__": {"id_": "c2e46d11-89a0-4122-8ff4-25a3e15be4ef", "embedding": null, "metadata": {"page_label": "3", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f3437940-cf5e-469d-ac6a-8a6812a51f45"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "501ec93a-c994-4e7e-860c-318c90c44142", "node_type": "4", "metadata": {"page_label": "3", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f3437940-cf5e-469d-ac6a-8a6812a51f45"}, "hash": "aa68bf2266a252c1bc4f7480467400dc164552d7ba7328cebfb24fb97e300bc1", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "841d08f0-3a27-4218-929e-ff14240f2f3a", "node_type": "1", "metadata": {"page_label": "3", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f3437940-cf5e-469d-ac6a-8a6812a51f45"}, "hash": "ef13e0ae7076161901fd386a756b261825abda884ba3815327439308caa9ff14", "class_name": "RelatedNodeInfo"}}, "text": "The reranker (Ma et al., 2023;\nDong et al., 2024b; Xu et al., 2024) module inde-\npendently adjusts the ordering of retrieved docu-\nments based on their relevance. The post-retrieval\nprocessing (Xu et al., 2023; Yang et al., 2023; Wang\net al., 2023b; Jiang et al., 2024; Jin et al., 2024)\nmodule then reduces the volume of these docu-\nments, stripping away non-essential information to\nfocus solely on the content critical for generatingprecise responses.\nWhile these works have significantly advanced\nsingle-turn RAG systems, gaps remain in multi-\nturn conversational settings (Ye et al., 2024). We\naddress this by proposing a new benchmark and\nframework specifically for conversational RAG.\n2.2 Conversational Search\nConversational search enables users to interact with\nretrieval systems through multi-turn dialogues (Mo\net al., 2024a). Two main approaches are conversa-\ntional query rewriting (CQR) and conversational\ndense retrieval (CDR). CQR transforms context-\ndependent queries into fully rewritten versions for\nad-hoc retrieval, focusing on selecting relevant to-\nkens from the conversation history (V oskarides\net al., 2020; Kumar and Callan, 2020; Lin et al.,\n2021b) or using LLMs to generate rewrites (Lin\net al., 2020; Yu et al., 2020; Vakulenko et al., 2021;\nWu et al., 2022). CDR jointly encodes conversation\nhistory and the current query for end-to-end dense\nretrieval (Yu et al., 2021; Mao et al., 2024).\nChallenges like limited training data are ad-\ndressed through data augmentation (Lin et al.,\n2021a; Mao et al., 2022b; Dai et al., 2022; Jin et al.,\n2023; Chen et al., 2024; Mo et al., 2024c), and con-\ntext denoising (Mao et al., 2022a; Mo et al., 2023;\n3", "mimetype": "text/plain", "start_char_idx": 3224, "end_char_idx": 4900, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "647d350b-56c9-412a-8b91-a1e8526e3184": {"__data__": {"id_": "647d350b-56c9-412a-8b91-a1e8526e3184", "embedding": null, "metadata": {"page_label": "4", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_49eaed8f-fb6c-4545-bc16-0010a5faa05b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71fea522-d983-4f0e-9557-e0b26bc36823", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_49eaed8f-fb6c-4545-bc16-0010a5faa05b"}, "hash": "ddbb41037edb481a7933e522464e59ed8bf063b810f38053c895c6ea5b4b9ce4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c1a1a760-5b90-4ed1-9368-a68c1ca8e2e3", "node_type": "1", "metadata": {}, "hash": "473b6ad8d777be9166941cf5de7cb19952bc4ed8619c901a220b7598b80f15c6", "class_name": "RelatedNodeInfo"}}, "text": "Mao et al., 2023b; Mo et al., 2024b) improves re-\ntrieval by filtering irrelevant conversation history.\nHowever, a benchmark is still needed for evaluating\nresponse generation and citation labeling.\n3 CORAL\n3.1 Data Source\nWe choose Wikipedia as our data source for the\nfollowing reasons, which align with the critical\nfeatures in Table 1. (1) Wikipedia pages are well-\nstructured and enriched by global volunteers, cov-\nering a broad range of topics; (2) The logically\ninterconnected titles provide a strong foundation\nbasis for generating diverse queries, with each rep-\nresenting a distinct intent. (3) The human-authored\ncontent under each title includes references that not\nonly allow for free-form responses with precise ci-\ntation labeling but also serve as the golden retrieval\nevidence for their respective titles.\nHowever, the content may include noisy text,\nand reference pages are often too long for effective\nretrieval. We follow previous work (Qian et al.,\n2023) to clean the Wikipedia pages. Specifically,\nfor content, we remove Wikipedia templates, spe-\ncial symbols, and other invalid text. For references,\nwe first split the reference pages into smaller pas-\nsages. Then, we exclude passages shorter than 16\nwords or with a non-English token ratio exceed-\ning 0.3, and then calculate term recall to identify\nsuitable passages. After these refinements, we gen-\nerate a clean set of 20,000 high-quality pages for\nsubsequent conversations.\n3.2 The CORAL Dataset Construction\nWe transform one or more related Wikipedia\nweb pages into information-seeking conversations\nthrough a three-stage approach.\n3.2.1 Extracting Title Trees\nFirst, we extract all subheadings (i.e., titles) from\nthe raw HTML of the Wikipedia pages. These sub-\nheadings create a natural hierarchy for the content,\nenabling us to construct a title tree, where the page\ntitle (H1-level heading) serves as the root. Subse-\nquent headings (e.g., H2 to H6) divide the content\ninto progressively detailed sections, with each level\ncorresponding to a node\u2019s depth in the tree. The di-\nrectional links between nodes will dictate the flow\nof the generated conversations. Besides, to enhance\nthe complexity and diversity of conversations, we\nalso adjust the depth, breadth, multi-subtopic ex-ploration, and topic shifts during the construction\nof these title trees.\n3.2.2 Conversation Flow Sampling\nTo generate coherent and diverse conversations, we\nimplement the following four sampling strategies\nbased on the extracted title trees:\n(1)Linear Descent Sampling (LDS) : This strat-\negy begins at the root node and permits movement\nonly from parent nodes to their child nodes. LDS\nserves as the most basic sampling path, emulating\nthe progressive logic typical of real conversational\ninformation-seeking scenarios. As illustrated in\nFigure 2(a), the title tree starts with the overall\ntheme of climate change and progressively narrows\ndown to specific impacts associated with this global\nissue. Following the red arrow, the focus shifts to\nthe human aspects, particularly examining how cli-\nmate change affects human health and food security.\nThis structure exemplifies a gradual deepening of\nthe query intent as the conversation unfolds.\n(2)Sibling-Inclusive Descent Sampling (SIDS) :\nThis strategy builds on LDS by introducing direc-\ntional links between sibling nodes. This feature is\nessential because conversational processes often en-\ncompass both in-depth and parallel explorations of\nrelated subtopics. As shown in Figure 2(b), when\ndiscussing the reception of Harry Potter, the subse-\nquent three rounds of dialogue analyze it from three\ndistinct perspectives: commercial success, literary\ncriticism, and thematic critique. This enhancement\nenriches the breadth of discussions within the con-\nversation structure.\n(3)Single-Tree Random Walk (STRW) : This strat-\negy further enhances SIDS by incorporating inter-\nconnections among sibling nodes as well as be-\ntween parent and child nodes. Essentially, it forms\na directed graph with bidirectional edges. As illus-\ntrated in Figure 2 (c), after an in-depth exploration\nof Courtney Nevin\u2019s club career, the focus shifts to\nher international career.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4178, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c1a1a760-5b90-4ed1-9368-a68c1ca8e2e3": {"__data__": {"id_": "c1a1a760-5b90-4ed1-9368-a68c1ca8e2e3", "embedding": null, "metadata": {"page_label": "4", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_49eaed8f-fb6c-4545-bc16-0010a5faa05b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71fea522-d983-4f0e-9557-e0b26bc36823", "node_type": "4", "metadata": {"page_label": "4", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_49eaed8f-fb6c-4545-bc16-0010a5faa05b"}, "hash": "ddbb41037edb481a7933e522464e59ed8bf063b810f38053c895c6ea5b4b9ce4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "647d350b-56c9-412a-8b91-a1e8526e3184", "node_type": "1", "metadata": {"page_label": "4", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_49eaed8f-fb6c-4545-bc16-0010a5faa05b"}, "hash": "fc016bf515941bfce11786c3eeab619cdaadf2b509e773ba1ef0a05493ac932a", "class_name": "RelatedNodeInfo"}}, "text": "This feature is\nessential because conversational processes often en-\ncompass both in-depth and parallel explorations of\nrelated subtopics. As shown in Figure 2(b), when\ndiscussing the reception of Harry Potter, the subse-\nquent three rounds of dialogue analyze it from three\ndistinct perspectives: commercial success, literary\ncriticism, and thematic critique. This enhancement\nenriches the breadth of discussions within the con-\nversation structure.\n(3)Single-Tree Random Walk (STRW) : This strat-\negy further enhances SIDS by incorporating inter-\nconnections among sibling nodes as well as be-\ntween parent and child nodes. Essentially, it forms\na directed graph with bidirectional edges. As illus-\ntrated in Figure 2 (c), after an in-depth exploration\nof Courtney Nevin\u2019s club career, the focus shifts to\nher international career.\n(4)Dual-Tree Random Walk (DTRW) : It mimics\nthe topic shifts that occur in real conversational\nscenarios, allowing for greater flexibility. It enables\ntransitions between two different but somewhat\nrelated trees, which are retrieved using the root title\nas a query and employing the BM25 algorithm. As\nillustrated in Figure 2(d), the conversation shifts\nfrom sports in Albania to baseball in Germany.\n4", "mimetype": "text/plain", "start_char_idx": 3345, "end_char_idx": 4581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "03dfb7fa-b0a9-4cfe-9f82-b38378099f10": {"__data__": {"id_": "03dfb7fa-b0a9-4cfe-9f82-b38378099f10", "embedding": null, "metadata": {"page_label": "5", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5ed2a3b2-93c9-4c73-826f-35f604f912bd"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "802300c2-4daf-4fe8-99e1-1924e2cd511e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5ed2a3b2-93c9-4c73-826f-35f604f912bd"}, "hash": "47c1e0277c699cc4900388e3f973126781d357863eb1120b0cdc913c3199500f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "32d7c58d-efcc-405b-a8c1-061ae3a03116", "node_type": "1", "metadata": {}, "hash": "e805d274de15c5962cae27930b2d867c8c60162869459730879a7ec49b06ecac", "class_name": "RelatedNodeInfo"}}, "text": "Sport in Albania\nFootball BasketballBaseball in Germany\nHistory\nBefore \nWorld \nWar IIDuring \nWorld \nWar IICourtney Nevin\nClub career\nJunior \nyearsLower \ntierInternational \ncareer\nYouth Senior(a) Linear Descent Sampling\n(c) Single-Tree Random Walk(b) Sibling-Inclusive Descent Sampling\n https://www.wikipedia.org/\n<h1>Climate change</h1>\n<h2>Impacts</h2>\n<h3>Nature and wildlife</h3>\n<h3>Humans</h3>\n<h4>Health and food</h4>\n<h4>Livelihoods and inequality</h4>\n......\n<h3>Amateur development</h3>\n   <h2>Adaptation</h2>\n https://www.wikipedia.org/\n<h1>Harry Potter</h1>\n<h2>Plot</h2>\n<h3>Early years</h3>\n......\n<h2>Reception</h2>\n<h3>Commercial success</h3>\n<h3>Material and sediment flux</h3>\n<h3>Runoff</h3>\n......\n https://www.wikipedia.org/\n<h1>Courtney Nevin</h1>\n<h2>Club career</h2>\n<h3>Junior years</h3>\n<h3>Lower tier</h3>\n<h2>International career</h2>\n<h3>Youth</h3>\n<h3>Senior</h3>\n......\n https://www.wikipedia.org/\n<h1>Sport in Albania</h1>\n<h2>Football</h2>\n<h2>Basketball</h2>\n<h3>Olympic Games</h3>\n......\n<h1>Baseball in Germany</h1>\n<h2>History</h2>\n<h3>Before World War II</h3>\n<h3>During World War II</h3>Harry Potter\nPlot Reception\nCommercia\nl successLiterary \ncriticismThematic \ncritique\n\u2460\n\u2461\n\u2462 \u2463\u2464\n\u2465 \u2466\u2460 \u2463\n\u2464(d) Dual-Tree Random Walk\n\u2461 \u2462\n\u2465\u2460\n\u2461 \u2462\n\u2463 \u2465 \u2464\nClimate change\nImpacts\nNature and \nwildlifeAdaptation\nHealth and \nfoodLivelihoods \nand \ninequalityHumans\u2460\n\u2461\n\u2462\n\u2463\nAmateur \ndevelopment\nEarly \nyears\nBefore \nWorld \nWar IIFigure 2: Illustration of the four sampling strategies. The red arrows show the sampled conversation flow, with\nnumerical labels on the nodes indicating the round of the sampled conversation turns.\n3.2.3 Contextualization of Questions\nAs introduced in Section 3.1, we treat the subtitles\nas the sources of questions, with their correspond-\ning contents serving as the responses. In this final\nstage, we contextualize the keyword subtitles into\nconversational questions to enhance the realism of\nthe conversation.\nSpecifically, for each turn, we first create a key-\nword chain that includes the current node and all its\nancestor nodes. This keyword chain, along with the\nresponse of the current node, is then used to prompt\nGPT-41to rewrite the original keyword title into\na natural language question. We then continue to\nprompt GPT-4 to further contextualize the question\ninto a conversational format by incorporating lin-\nguistic phenomena such as ellipses, references, and\nomissions (Dalton et al., 2020b), which are preva-\nlent in real conversational scenarios. The prompt\ndetails are provided in Appendix A.\n3.3 The Final Dataset Format and Statistics\nThe key statistics of CORAL are summarized in Ta-\nble 4. Our dataset consists of 8,000 conversations\nwith the four types introduced in Section 3.2.2.\nThese 8,000 conversations are evenly distributed\nacross four distinct structural types, with each type\ncontaining 2,000 conversations. Specifically, the\nLDS conversation type includes 3 to 6 turns per con-\nversation.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2959, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "32d7c58d-efcc-405b-a8c1-061ae3a03116": {"__data__": {"id_": "32d7c58d-efcc-405b-a8c1-061ae3a03116", "embedding": null, "metadata": {"page_label": "5", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5ed2a3b2-93c9-4c73-826f-35f604f912bd"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "802300c2-4daf-4fe8-99e1-1924e2cd511e", "node_type": "4", "metadata": {"page_label": "5", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5ed2a3b2-93c9-4c73-826f-35f604f912bd"}, "hash": "47c1e0277c699cc4900388e3f973126781d357863eb1120b0cdc913c3199500f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "03dfb7fa-b0a9-4cfe-9f82-b38378099f10", "node_type": "1", "metadata": {"page_label": "5", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5ed2a3b2-93c9-4c73-826f-35f604f912bd"}, "hash": "149d087c88392da84722f0e4ca03a3fc2b10996fea6a3d5ae830abb9707ac95f", "class_name": "RelatedNodeInfo"}}, "text": "We then continue to\nprompt GPT-4 to further contextualize the question\ninto a conversational format by incorporating lin-\nguistic phenomena such as ellipses, references, and\nomissions (Dalton et al., 2020b), which are preva-\nlent in real conversational scenarios. The prompt\ndetails are provided in Appendix A.\n3.3 The Final Dataset Format and Statistics\nThe key statistics of CORAL are summarized in Ta-\nble 4. Our dataset consists of 8,000 conversations\nwith the four types introduced in Section 3.2.2.\nThese 8,000 conversations are evenly distributed\nacross four distinct structural types, with each type\ncontaining 2,000 conversations. Specifically, the\nLDS conversation type includes 3 to 6 turns per con-\nversation. For the remaining types\u2014SIDS, STRW,\nand DTRW\u2014each category consists of 1,600 sets\nof conversations with 6 to 10 turns, along with an\n1gpt-4-turbo-2024-04-09 from https://openai.com/apiadditional 400 sets featuring 11 to 20 turns per con-\nversation. The design of the longer conversation\nintends to simulate real-world challenges encoun-\ntered in conversational scenarios, such as redundant\ninformation and the long context problem.\nOur final dataset format is as follows: A conver-\nsationC={(qi, ri)}n\ni=1comprised of nturns. qi\nis a contextualized query of the i-th turn generated\nin Section 3.2.3, and riis the i-th turn golden re-\nsponse, which is the cleaned plain text under the\ncorresponding (sub-)title in the HTML. The sup-\nporting web pages for ri, listed in the HTML Ref-\nerence Section, can be processed as described in\nSection 3.1 to serve as the golden relevant passages\nP+\ni={pi,1, pi,2, . . .}. On average, each conversa-\ntion turn has 3.17 related passages, and the average\ngolden response length is 255 tokens. Finally, we\nobtain a passage corpus P, which contains 200K\npassages from all the golden references P+\ni.\n3.4 Evaluation Tasks\nCORAL mainly supports three fundamental con-\nversational RAG tasks:\n(1)Conversational Passage Retrieval : This task\nevaluates a system\u2019s capability to extract relevant\ninformation from extensive document collections,\nconsidering the context of multi-turn conversations.\nFormally, given the k-th question qkand the cor-\nresponding conversation history Hk={qi, ri}k\u22121\ni=1,\nwhere qiandrirespectively denote the question\nand response of the i-th turn, the retriever Raims\nto retrieve the relevant passages Pkfrom the pas-\nsage corpus P. We use MRR, MAP, NDCG@3,\n5", "mimetype": "text/plain", "start_char_idx": 2238, "end_char_idx": 4673, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a22b1072-7159-444f-aa83-a7cfc529e368": {"__data__": {"id_": "a22b1072-7159-444f-aa83-a7cfc529e368", "embedding": null, "metadata": {"page_label": "6", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e1d29221-41a7-4850-95e6-c18f88f18c7c"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98cca184-30b6-4198-9722-c21e4c60125a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e1d29221-41a7-4850-95e6-c18f88f18c7c"}, "hash": "6ae9b9e0f4bf913b0dadad16edc80979816feecf2826570188aa0d762eaaba24", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6514127-9a28-4232-9458-8dd6c5e918d6", "node_type": "1", "metadata": {}, "hash": "6f61557913284483f9944c56a97d7f7e9138bd6e6ce1dc038a479d5053586d3c", "class_name": "RelatedNodeInfo"}}, "text": "Category Method MRR MAP NDCG@3 Recall@20 Recall@100\nCDR ModelsConv-ANCE-Q 19.8 28.6 20.5 39.1 51.0\nKD-ANCE-Q 22.6 33.1 24.5 38.5 48.0\nConv-ANCE-C 20.5 29.6 21.1 39.8 53.4\nKD-ANCE-C 23.2 33.6 24.9 40.3 49.6\nCQR ModelsLLM4CS (GPT-3.5) 21.2 31.1 23.0 35.5 44.4\nQwen2.5-1.5B 16.3 23.8 17.2 31.0 39.2\nQwen2.5-1.5B-SFT 23.1 33.6 25.1 39.4 48.6\nTable 2: Retrieval performance comparisons. The best performance is bold.Conv-ANCE-Q denotes the Conv-ANCE\nis trained on the QReCC dataset and Conv-ANCE-C denotes the Conv-ANCE is trained on CORAL training dataset.\nRecall@20 and Recall@100 as retrieval evaluation\nmetrics.\n(2)Response Generation : This task challenges\nthe system\u2019s ability to produce accurate, detailed,\nand contextually appropriate answers. Given the\nk-th question qk, the corresponding conversation\nhistory Hk, and the relevant passages Pk, the\ngenerator Gneeds to generate an informative re-\nsponse to answer the question. We use rule-\nbased metrics BLEU-1 (Papineni et al., 2002), and\nROUGE-L (Lin, 2004) to evaluate the response\nquality compared with rk. Given the lengthier re-\nsponses in our benchmark, we additionally utilize\nthe model-based evaluation method proposed in\nRichRAG (Wang et al., 2024).\n(3)Citation Labeling : This task evaluates the\nmethod\u2019s ability to accurately attribute information\nsources within the generated responses. Following\nALCE (Gao et al., 2023), the generated response rk\nconsists of nstatements s1, s2, ..., s n. Each state-\nmentsicites a list of passages Ci={ci,1, ci,2, ...},\nwhere ci,j\u2208Pk. We adopt Citation Recall and Ci-\ntation Precision defined in ALCE (Gao et al., 2023)\nto evaluate the accuracy of citation labeling.\n4 Conversational RAG Framework\nA conversational RAG system typically comprises\na retriever and a generator to handle the current\nuser query qk, the conversation history Hk, and\nthe retrieved passages Pk. As the conversation\nprogresses, both the growing conversation history\nand the noisy retrieved passages can negatively\nimpact the system\u2019s efficiency and effectiveness,\nmaking it harder to generate accurate responses.\nTo solve the problem, we propose a simple com-\npression framework to efficiently manage these\ninputs. Specifically, we introduce a conversation\ncompression function fto compress the conversa-\ntion, and then use the compressed contents as the\nreal inputs of retrievers and LLM generators. Inaddition to conversation compression, we also ap-\nply post-retrieval results compression. Following\nexisting approaches (Xu et al., 2023), we simply\ntake LLMs as the compression function fp, leaving\nthe exploration of more compression methods in\nfuture work.\nFormally, suppose f(Hk)is the compressed con-\nversation context, Pk=R(f(Hk), qk)is list of\npassages retrieved by querying f(Hk)withqk,\nandfp(Pk)is the compressed results of the re-\ntrieval, the final generation task can be formulated\nas:G(qk, f(Hk), fp(Pk)). The prompt for feeding\nqk, f(Hk), fp(Pk)into the generator can be found\nin Appendix B. Various existing conversational\nRAG methods can be unified into our framework.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3063, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6514127-9a28-4232-9458-8dd6c5e918d6": {"__data__": {"id_": "e6514127-9a28-4232-9458-8dd6c5e918d6", "embedding": null, "metadata": {"page_label": "6", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e1d29221-41a7-4850-95e6-c18f88f18c7c"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "98cca184-30b6-4198-9722-c21e4c60125a", "node_type": "4", "metadata": {"page_label": "6", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e1d29221-41a7-4850-95e6-c18f88f18c7c"}, "hash": "6ae9b9e0f4bf913b0dadad16edc80979816feecf2826570188aa0d762eaaba24", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a22b1072-7159-444f-aa83-a7cfc529e368", "node_type": "1", "metadata": {"page_label": "6", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e1d29221-41a7-4850-95e6-c18f88f18c7c"}, "hash": "ce64dafaea42618e62e4e6d060d84fb09cb34b8375656d303e93a8d0cff8c182", "class_name": "RelatedNodeInfo"}}, "text": "Inaddition to conversation compression, we also ap-\nply post-retrieval results compression. Following\nexisting approaches (Xu et al., 2023), we simply\ntake LLMs as the compression function fp, leaving\nthe exploration of more compression methods in\nfuture work.\nFormally, suppose f(Hk)is the compressed con-\nversation context, Pk=R(f(Hk), qk)is list of\npassages retrieved by querying f(Hk)withqk,\nandfp(Pk)is the compressed results of the re-\ntrieval, the final generation task can be formulated\nas:G(qk, f(Hk), fp(Pk)). The prompt for feeding\nqk, f(Hk), fp(Pk)into the generator can be found\nin Appendix B. Various existing conversational\nRAG methods can be unified into our framework.\nIn this work, we mainly investigate the following\nthree methods for the conversation compression:\nLast Response Strategy For the conversation\nhistory, we heuristically select all previous con-\nversational questions {qi}k\u22121\n1and the last turn\u2019s\nresponse rk\u22121in the conversation history:\nfLR(Hk) ={qi}k\u22121\n1, rk\u22121. (1)\nRewrite Strategy We adopt a conversational\nquery rewriting model Rewrite() to transform the\noriginal query along with the conversation history\ninto a standalone question rewrite \u02c6qk:\nfRW\nc(Hk) = \u02c6qk=Rewrite (qk;Hk). (2)\nIn this strategy, Pk=R( \u02c6qk)is list of passages\nretrieved by querying \u02c6qk, and fp(Pk)is the com-\npressed results of the retrieval, the final generation\ntask can be formulated as: G( \u02c6qk, fp(Pk)).\nLLM Summarization Strategy Inspired by RE-\nCOMP (Xu et al., 2023), we use LLMs to generate\nabstractive summary of the conversation history:\nfSUM\nc(Hk) =LLM(Hk). (3)\nThe prompt is shown in Appendix C.\n6", "mimetype": "text/plain", "start_char_idx": 2378, "end_char_idx": 3997, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77c53922-b0ca-4b30-a1f1-90ce837551bb": {"__data__": {"id_": "77c53922-b0ca-4b30-a1f1-90ce837551bb", "embedding": null, "metadata": {"page_label": "7", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_355d5897-dc02-45a7-9b0b-8754074bf014"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb59a00-0960-4c74-9148-c2c6f1429a62", "node_type": "4", "metadata": {"page_label": "7", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_355d5897-dc02-45a7-9b0b-8754074bf014"}, "hash": "1e08d12827761074f68698066b951fa624e0c07a39f3bdfefd019ae79fd0850f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "543ca04c-c292-460b-8e5d-fa488f5f358a", "node_type": "1", "metadata": {}, "hash": "31d60fefa752ceebe3612f7aa92181b6837f1d516683a75de41adfbc130539d9", "class_name": "RelatedNodeInfo"}}, "text": "Category # Tokens ModelGeneration Citation Labeling\nBLEU-1 ROUGE-L Citation Recall Citation Precision\nRaw Context 2226Qwen2.5-7B 22.2 13.1 3.1 18.1\nMistral-7B 18.1 12.4 2.4 4.8\nLlama-3.1-8B 21.5 12.9 0.9 2.1\nQwen2.5-7B-SFT 18.3 18.5 6.6 16.8\nMistral-7B-SFT 23.7 20.1 4.6 11.1\nLlama-3.1-8B-SFT 24.2 19.7 4.2 9.8\nLast Response 1474Qwen2.5-7B 20.9 12.8 3.5 20.8\nMistral-7B 18.1 12.3 2.7 4.5\nLlama-3.1-8B 20.4 12.7 1.3 3.1\nQwen2.5-7B-SFT 23.9 16.5 10.4 24.8\nMistral-7B-SFT 21.8 18.5 5.0 12.4\nLlama-3.1-8B-SFT 26.1 18.1 3.5 8.7\nRewrite 1236Qwen2.5-7B 21.1 12.8 2.4 9.4\nMistral-7B 18.8 12.3 2.5 3.8\nLlama-3.1-8B 18.8 12.4 1.7 3.1\nQwen2.5-7B-SFT 18.9 16.4 7.4 16.8\nMistral-7B-SFT 24.8 18.5 5.9 14.8\nLlama-3.1-8B-SFT 26.3 18.2 4.7 11.2\nLLM Summarization 478Qwen2.5-7B 21.0 12.7 2.9 13.0\nMistral-7B 19.5 12.3 5.6 6.7\nLlama-3.1-8B 19.1 12.8 4.1 7.1\nQwen2.5-7B-SFT 23.5 16.8 14.1 31.1\nMistral-7B-SFT 16.9 17.1 8.3 19.8\nLlama-3.1-8B-SFT 18.7 16.5 4.5 10.7\nTable 3: The comparison of different LLMs on response generation and citation labeling. # Tokens denotes the\nnumber of input tokens.\n5 Experiments\nIn this section, we discuss the performance of con-\nversational RAG on our benchmark, and provide a\ncomprehensive analysis for each stage.\n5.1 Evaluating Retrieval Performance\nWe concentrate on two main approaches in con-\nversational search: conversational dense retrieval\n(CDR) and conversational query rewriting (CQR).\nFor CDR, we use KD-ANCE and Conv-ANCE\nwith ANCE as the base retriever. KD-ANCE (Yu\net al., 2021) trains the session encoder by mim-\nicking golden query embeddings, while Conv-\nANCE (Karpukhin et al., 2020; Lin et al., 2021a)\nuses contrastive learning to train the session en-\ncoder, drawing it closer to relevant passages and\nfurther from irrelevant ones. For CQR, we uti-\nlize the LLM4CS (Mao et al., 2023a), which in-\ncorporates GPT-3.5, and an open-source LLM for\ngenerating query rewrites respectively to enable a\ncomparative analysis. Table 2 provides a detailed\ncomparison between these two categories.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2020, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "543ca04c-c292-460b-8e5d-fa488f5f358a": {"__data__": {"id_": "543ca04c-c292-460b-8e5d-fa488f5f358a", "embedding": null, "metadata": {"page_label": "7", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_355d5897-dc02-45a7-9b0b-8754074bf014"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eeb59a00-0960-4c74-9148-c2c6f1429a62", "node_type": "4", "metadata": {"page_label": "7", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_355d5897-dc02-45a7-9b0b-8754074bf014"}, "hash": "1e08d12827761074f68698066b951fa624e0c07a39f3bdfefd019ae79fd0850f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77c53922-b0ca-4b30-a1f1-90ce837551bb", "node_type": "1", "metadata": {"page_label": "7", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_355d5897-dc02-45a7-9b0b-8754074bf014"}, "hash": "74b127354c81d040cda702a8d2400f6fb3a46f743496b16c1c16196538465bfd", "class_name": "RelatedNodeInfo"}}, "text": "For CDR, we use KD-ANCE and Conv-ANCE\nwith ANCE as the base retriever. KD-ANCE (Yu\net al., 2021) trains the session encoder by mim-\nicking golden query embeddings, while Conv-\nANCE (Karpukhin et al., 2020; Lin et al., 2021a)\nuses contrastive learning to train the session en-\ncoder, drawing it closer to relevant passages and\nfurther from irrelevant ones. For CQR, we uti-\nlize the LLM4CS (Mao et al., 2023a), which in-\ncorporates GPT-3.5, and an open-source LLM for\ngenerating query rewrites respectively to enable a\ncomparative analysis. Table 2 provides a detailed\ncomparison between these two categories. We have\nthe following observations:\n(1) The performances of the CDR and CQR mod-\nels are fairly comparable. Notably, the Qwen2.5-\n1.5B-SFT shows a strong competitive edge, notonly surpasses the Qwen2.5-1.5B but also outper-\nforms the advanced closed-source LLM GPT-3.5\nacross all evaluated metrics.\n(2) KD-ANCE in the CDR category shows bet-\nter results compared to Conv-ANCE. This may\nbe attributed to the training methodologies: KD-\nANCE possibly leverages golden rewrite data more\neffectively than Conv-ANCE, which uses in-batch\nnegatives that may not be sufficiently challenging\nfor optimal learning.\n5.2 Evaluation Response Generation with\nCitation Labeling\nWe compare the raw context baselines with an-\nother three conversation compression strategies in-\ntroduced in Section 4, selecting Qwen2.5, Mistral,\nand Llama as generators. We prompt the generator\nto generate the response along with the citations in\nthe response. The generation and citation labeling\nperformance is shown in Table 3, and the GPT-4\nscore is shown in Figure 5. We find that:\n(1) By examining four methods of modeling con-\nversation history, we observe an interesting trend:\nas the input is progressively condensed (from 2226\ninput tokens in the raw context to merely 478 input\ntokens in the LLM Summarization), the decrease in\n7", "mimetype": "text/plain", "start_char_idx": 1412, "end_char_idx": 3328, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "518f595f-2fa7-477e-9361-7f9bb066070c": {"__data__": {"id_": "518f595f-2fa7-477e-9361-7f9bb066070c", "embedding": null, "metadata": {"page_label": "8", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2ee6ab69-6fa6-4bfa-a9e2-ca12187b59d8"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "50d9a972-2a4f-4073-8d2c-6a24ab48a4e3", "node_type": "4", "metadata": {"page_label": "8", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2ee6ab69-6fa6-4bfa-a9e2-ca12187b59d8"}, "hash": "63863179df1ab2fb6754321cb56ccb173c5b3865507ce1df7f155b2f720ea7b4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b1739833-1579-488f-a85b-74e93ad67776", "node_type": "1", "metadata": {}, "hash": "ee6a10cf0b85a32cc3593a59d2484f0c5e91a1478a0d7a4cf19856c41e21509a", "class_name": "RelatedNodeInfo"}}, "text": "1B 3B 5B 7B11.012.514.015.517.018.5Rouge-L\nRaw Context\nQwen-SFT\nQwen\n1B 3B 5B 7B11.012.514.015.517.018.5\nLast Response\nQwen-SFT\nQwen\n1B 3B 5B 7B11.012.514.015.517.018.5\nRewrite\nQwen-SFT\nQwen\n1B 3B 5B 7B11.012.514.015.517.018.5\nLLM Summarization\nQwen-SFT\nQwen\n1B 3B 5B 7B0.02.04.06.08.010.012.014.016.0Citation Precision\nQwen-SFT\nQwen\n1B 3B 5B 7B0.02.04.06.08.010.012.014.016.0\nQwen-SFT\nQwen\n1B 3B 5B 7B0.02.04.06.08.010.012.014.016.0\nQwen-SFT\nQwen\n1B 3B 5B 7B0.02.04.06.08.010.012.014.016.0\nQwen-SFT\nQwenFigure 3: The scaling analysis of generation and citation labeling performance.\n2 4 6 8 10\nNumber of History Turns0102030405060708090100Percentage5256 54 56 56\n20 18 2016 1628 26 26 28 28Raw Context\nWin Draw Lose\n2 4 6 8 10\nNumber of History Turns0102030405060708090100Percentage76\n66706468\n10 10 10 8 81424202824Raw Context SFT\nWin Draw Lose\n10.011.012.013.014.015.0\nROUGE-L\n16.018.020.022.024.0\nROUGE-L\nFigure 4: Generation results of different conversation history length. The curve in the figure represents the ROUGE-\nL score. The histogram shows the results of GPT-4 scores comparing model-generated responses with golden\nresponses. Winindicates cases where model-generated responses outperform golden responses, Draw indicates\ncases where the two responses are considered equally good, and Lose indicates cases where the golden responses\nare considered better. The y-axis on the left represents the proportion of cases in the total number of cases.\n0.0 20.0 40.0 60.0 80.0 100.0w/o SFT\nwith SFT48.5% 24.2% 27.3%\n72.5% 19.3% 8.2%Raw Context\nModel Generated Wins Tie Golden Wins\n0.0 20.0 40.0 60.0 80.0 100.0w/o SFT\nwith SFT54.4% 19.5% 26.1%\n70.7% 18.8% 10.5%Last Response\nModel Generated Wins Tie Golden Wins\n0.0 20.0 40.0 60.0 80.0 100.0w/o SFT\nwith SFT47.2% 23.6% 29.2%\n44.4% 42.4% 13.2%Rewrite\nModel Generated Wins Tie Golden Wins\n0.0 20.0 40.0 60.0 80.0 100.0w/o SFT\nwith SFT47.9% 23.7% 28.4%\n62.7% 19.6% 17.7%LLM Summarization\nModel Generated Wins Tie Golden Wins\nFigure 5: The GPT-4 evaluation score.\nperformance is surprisingly minimal, and in terms\nof citation labeling, there is even an observed im-\nprovement. This suggests that some content within\nthe dialogue history is irrelevant or redundant and\ncan be removed without negatively impacting the\nmodel\u2019s performance.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2288, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1739833-1579-488f-a85b-74e93ad67776": {"__data__": {"id_": "b1739833-1579-488f-a85b-74e93ad67776", "embedding": null, "metadata": {"page_label": "8", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2ee6ab69-6fa6-4bfa-a9e2-ca12187b59d8"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "50d9a972-2a4f-4073-8d2c-6a24ab48a4e3", "node_type": "4", "metadata": {"page_label": "8", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2ee6ab69-6fa6-4bfa-a9e2-ca12187b59d8"}, "hash": "63863179df1ab2fb6754321cb56ccb173c5b3865507ce1df7f155b2f720ea7b4", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "518f595f-2fa7-477e-9361-7f9bb066070c", "node_type": "1", "metadata": {"page_label": "8", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2ee6ab69-6fa6-4bfa-a9e2-ca12187b59d8"}, "hash": "4e779c7b5a8d838e0b8a71afedf8e2eb8c052bafa6742df735c4bebf43998789", "class_name": "RelatedNodeInfo"}}, "text": "performance is surprisingly minimal, and in terms\nof citation labeling, there is even an observed im-\nprovement. This suggests that some content within\nthe dialogue history is irrelevant or redundant and\ncan be removed without negatively impacting the\nmodel\u2019s performance.\n(2) Among three conversation compressionstrategies, the Rewrite with SFT exhibits supe-\nrior performance, which could be attributed to the\nmodel\u2019s enhanced capability to learn from the sim-\nplified question-answer pattern. Intriguingly, al-\nthough the LLM Summarization strategy demon-\nstrates weaker performance in response generation,\nit significantly enhances citation labeling. A possi-\nble explanation is that the summarization process\neffectively filters noise, thereby optimizing the con-\ntent for generating more reliable responses.\n5.3 Scaling Analysis on Model Parameters\nWe scale the generator\u2019s parameters from 500M to\n7B, as shown in Figure 3. We find that:\n(1) There is a pronounced improvement in gen-\neration as parameters increase from 500M to 1.5B,\nevidenced by a significant rise in ROUGE-L scores.\nHowever, beyond 3B parameters, the performance\ngains plateau, indicating diminishing returns with\nadditional parameter scaling.\n(2) Performance in citation labeling improves\nmarkedly as the parameter count extends from 3B\nto 7B. This suggests that a larger model capacity\nis beneficial for tasks that require extensive knowl-\n8", "mimetype": "text/plain", "start_char_idx": 2016, "end_char_idx": 3434, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d8eb5ba9-60c7-4089-897f-b5697fc91149": {"__data__": {"id_": "d8eb5ba9-60c7-4089-897f-b5697fc91149", "embedding": null, "metadata": {"page_label": "9", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e89cb6f2-be2a-45ea-b6b7-aa8df08bdf3e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abb572ed-1551-4ae3-8943-a12d8835c3e7", "node_type": "4", "metadata": {"page_label": "9", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e89cb6f2-be2a-45ea-b6b7-aa8df08bdf3e"}, "hash": "efb71e2370348032ec854e5eb9f141949a761ee632d0826371f04eee0fd61767", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b5278636-8d50-4393-8c5c-c2a819ce110f", "node_type": "1", "metadata": {}, "hash": "a8ae7f05ef70d1e9be4532f99de823958f643e40e90320819a86703550e77901", "class_name": "RelatedNodeInfo"}}, "text": "edge, such as accurate citation usage.\n5.4 Quantitative Analysis on History Turns\nTo analyze the impact of conversation history\nlength, we randomly select 50 conversations and\nvary the number of previous dialogue turns pro-\nvided to the generator. This can be represented\nasrm\nk=G(qk;Hm\nk;Pk), where k= 12 ,Hm\nk=\n{qi, ri}k\u22121\nk\u2212m, andm\u2208 {2,4,6,8,10}. Results are\nshown in Figure 4. We find that:\n(1) After fine-tuning, the performance improves\nsignificantly, especially when using four history\nturns, resulting in a notable 55% improvement in\nthe ROUGE-L. This demonstrates the effectiveness\nof SFT in modeling history.\n(2) Before fine-tuning, response quality de-\ncreases with six turns of history compared to four,\npossibly due to the redundant information intro-\nduced by the longer history. However, after fine-\ntuning, response quality improves with six turns but\ndeclines with eight, suggesting a trade-off between\nricher information enriched by longer context and\nirrelevant information introduced by conversation\nhistory. These findings validate the challenges pre-\nviously discussed in Section 1.\n6 Conclusion\nIn this paper, we present an automatic approach\nusing LLMs to construct large-scale, information-\nseeking conversations from Wikipedia pages. The\nresulting benchmark, CORAL, supports three fun-\ndamental tasks for evaluating conversational RAG\nsystems. Additionally, we propose a unified frame-\nwork to standardize various conversational RAG\nmethods and conduct a comprehensive evaluation\nof these methods on CORAL. We envision CORAL\nas a valuable resource for advancing research in\nconversational RAG, fostering innovation, and im-\nproving real-world applications.\nLimitations\nOur work presents a conversational RAG bench-\nmark named CORAL, which fills a notable void\nin assessing conversational RAG methods. In this\nbenchmark, we examine the effects of compress-\ning conversational history on answer generation,\npaving the way for future research in conversa-\ntional RAG. However, since CORAL is built upon\nWikipedia and existing LLMs are typically trained\non corpora like Wikipedia and CommonCrawl, us-\ning these LLMs as generators could lead to con-tamination in the conversational RAG process due\nto the overlap in their training data. Additionally,\nthe three conversation compression strategies em-\nployed in CORAL are somewhat basic, focusing\nsolely on reducing the length of inputs rather than\nmodeling the conversation history in a granular\nmanner. Additionally, the use of the LLM Summa-\nrization strategy for compressing both conversation\nhistory and retrieved passages, while leveraging\nadvanced models such as GPT-4, could lead to con-\nsiderable expenses.\nReferences\nVaibhav Adlakha, Shehzaad Dhuliawala, Kaheer Sule-\nman, Harm de Vries, and Siva Reddy. 2022. Topi-\nocqa: Open-domain conversational question answer-\ning with topic switching. Transactions of the Associ-\nation for Computational Linguistics , 10:468\u2013483.\nMoonshot AI. 2023. Kimi chat.\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,\nShayne Longpre, Stephen Pulman, and Srinivas\nChappidi. 2021. Open-domain question answering\ngoes conversational via question rewriting. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-\nHLT 2021, Online, June 6-11, 2021 , pages 520\u2013534.\nAssociation for Computational Linguistics.\nAnthropic. 2023. Introducing claude.\nIngeol Baek, Jimin Lee, Joonho Yang, and Hwanhee\nLee. 2024. Crafting the path: Robust query rewriting\nfor information retrieval. CoRR , abs/2407.12529.\nHaonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan\nLiu, and Ziliang Zhao. 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3684, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5278636-8d50-4393-8c5c-c2a819ce110f": {"__data__": {"id_": "b5278636-8d50-4393-8c5c-c2a819ce110f", "embedding": null, "metadata": {"page_label": "9", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e89cb6f2-be2a-45ea-b6b7-aa8df08bdf3e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "abb572ed-1551-4ae3-8943-a12d8835c3e7", "node_type": "4", "metadata": {"page_label": "9", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e89cb6f2-be2a-45ea-b6b7-aa8df08bdf3e"}, "hash": "efb71e2370348032ec854e5eb9f141949a761ee632d0826371f04eee0fd61767", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d8eb5ba9-60c7-4089-897f-b5697fc91149", "node_type": "1", "metadata": {"page_label": "9", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e89cb6f2-be2a-45ea-b6b7-aa8df08bdf3e"}, "hash": "f943485b28c11bd199c941d12802fca8b50a919c33f2ac15c9e6d220a773e9e5", "class_name": "RelatedNodeInfo"}}, "text": "2021. Open-domain question answering\ngoes conversational via question rewriting. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, NAACL-\nHLT 2021, Online, June 6-11, 2021 , pages 520\u2013534.\nAssociation for Computational Linguistics.\nAnthropic. 2023. Introducing claude.\nIngeol Baek, Jimin Lee, Joonho Yang, and Hwanhee\nLee. 2024. Crafting the path: Robust query rewriting\nfor information retrieval. CoRR , abs/2407.12529.\nHaonan Chen, Zhicheng Dou, Kelong Mao, Jiongnan\nLiu, and Ziliang Zhao. 2024. Generalizing conversa-\ntional dense retrieval via llm-cognition data augmen-\ntation. In Proceedings of the 62nd Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2024, Bangkok, Thailand,\nAugust 11-16, 2024 , pages 2700\u20132718. Association\nfor Computational Linguistics.\nZhuyun Dai, Arun Tejasvi Chaganty, Vincent Y . Zhao,\nAida Amini, Qazi Mamunur Rashid, Mike Green, and\nKelvin Guu. 2022. Dialog inpainting: Turning docu-\nments into dialogs. In International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA , volume 162 of Proceedings\nof Machine Learning Research , pages 4558\u20134586.\nPMLR.\nJeffrey Dalton, Chenyan Xiong, and Jamie Callan.\n2020a. Cast 2020: The conversational assistance\ntrack overview. In Proceedings of the Twenty-Ninth\nText REtrieval Conference, TREC 2020, Virtual Event\n[Gaithersburg, Maryland, USA], November 16-20,\n2020 , volume 1266 of NIST Special Publication . Na-\ntional Institute of Standards and Technology (NIST).\n9", "mimetype": "text/plain", "start_char_idx": 3086, "end_char_idx": 4711, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "601ada8f-3e76-4699-aeb5-f69bf3f29639": {"__data__": {"id_": "601ada8f-3e76-4699-aeb5-f69bf3f29639", "embedding": null, "metadata": {"page_label": "10", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6ea10c62-d592-4005-bf78-b298a5fc2d7f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff89670f-ba51-4423-8cdc-7b213c639795", "node_type": "4", "metadata": {"page_label": "10", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6ea10c62-d592-4005-bf78-b298a5fc2d7f"}, "hash": "f92c44bf08162c7c59e8f559e98b79f0f38c2357081f3a67c7a6eb3676cdf125", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fc5f97cb-9e8b-4782-b573-e24bbf51c116", "node_type": "1", "metadata": {}, "hash": "f4ba3ae4c66b2e450fbf5de2a06531942545d26b72076d99d65548ae172ca8f4", "class_name": "RelatedNodeInfo"}}, "text": "Jeffrey Dalton, Chenyan Xiong, and Jamie Callan.\n2020b. TREC cast 2019: The conversational as-\nsistance track overview. CoRR , abs/2003.13624.\nJeffrey Dalton, Chenyan Xiong, and Jamie Callan. 2021.\nTREC cast 2021: The conversational assistance track\noverview. In Proceedings of the Thirtieth Text RE-\ntrieval Conference, TREC 2021, online, November\n15-19, 2021 , volume 500-335 of NIST Special Publi-\ncation . National Institute of Standards and Technol-\nogy (NIST).\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2019. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. In 7th International Conference on Learning\nRepresentations, ICLR 2019, New Orleans, LA, USA,\nMay 6-9, 2019 . OpenReview.net.\nYiran Ding, Li Lyna Zhang, Chengruidong Zhang,\nYuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang,\nand Mao Yang. 2024. Longrope: Extending LLM\ncontext window beyond 2 million tokens. In Forty-\nfirst International Conference on Machine Learning,\nICML 2024, Vienna, Austria, July 21-27, 2024 . Open-\nReview.net.\nGuanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi\nQiao, Zhicheng Dou, and Ji-Rong Wen. 2024a. To-\nward general instruction-following alignment for\nretrieval-augmented generation. arXiv preprint\narXiv:2410.09584 .\nGuanting Dong, Yutao Zhu, Chenghao Zhang, Zechen\nWang, Zhicheng Dou, and Ji-Rong Wen. 2024b. Un-\nderstand what LLM needs: Dual preference align-\nment for retrieval-augmented generation. CoRR ,\nabs/2406.18676.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1472, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fc5f97cb-9e8b-4782-b573-e24bbf51c116": {"__data__": {"id_": "fc5f97cb-9e8b-4782-b573-e24bbf51c116", "embedding": null, "metadata": {"page_label": "10", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6ea10c62-d592-4005-bf78-b298a5fc2d7f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff89670f-ba51-4423-8cdc-7b213c639795", "node_type": "4", "metadata": {"page_label": "10", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6ea10c62-d592-4005-bf78-b298a5fc2d7f"}, "hash": "f92c44bf08162c7c59e8f559e98b79f0f38c2357081f3a67c7a6eb3676cdf125", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "601ada8f-3e76-4699-aeb5-f69bf3f29639", "node_type": "1", "metadata": {"page_label": "10", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6ea10c62-d592-4005-bf78-b298a5fc2d7f"}, "hash": "094c85cc95215eeb3d9b3587e9d28dad273aebad05380b4e5adcda48442a3329", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "c15e477b-e5f1-4e92-896f-c60e049075a0", "node_type": "1", "metadata": {}, "hash": "2abaee77666550d14588d4ec49ece673f0bc3dc33aff93676e0ff7cfd1181a5d", "class_name": "RelatedNodeInfo"}}, "text": "Longrope: Extending LLM\ncontext window beyond 2 million tokens. In Forty-\nfirst International Conference on Machine Learning,\nICML 2024, Vienna, Austria, July 21-27, 2024 . Open-\nReview.net.\nGuanting Dong, Xiaoshuai Song, Yutao Zhu, Runqi\nQiao, Zhicheng Dou, and Ji-Rong Wen. 2024a. To-\nward general instruction-following alignment for\nretrieval-augmented generation. arXiv preprint\narXiv:2410.09584 .\nGuanting Dong, Yutao Zhu, Chenghao Zhang, Zechen\nWang, Zhicheng Dou, and Ji-Rong Wen. 2024b. Un-\nderstand what LLM needs: Dual preference align-\nment for retrieval-augmented generation. CoRR ,\nabs/2406.18676.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang,\nArchi Mitra, Archie Sravankumar, Artem Korenev,\nArthur Hinsvark, Arun Rao, Aston Zhang, Aur\u00e9lien\nRodriguez, Austen Gregerson, Ava Spataru, Bap-\ntiste Rozi\u00e8re, Bethany Biron, Binh Tang, Bobbie\nChern, Charlotte Caucheteux, Chaya Nayak, Chloe\nBi, Chris Marra, Chris McConnell, Christian Keller,\nChristophe Touret, Chunyang Wu, Corinne Wong,\nCristian Canton Ferrer, Cyrus Nikolaidis, Damien Al-\nlonsius, Daniel Song, Danielle Pintz, Danny Livshits,\nDavid Esiobu, Dhruv Choudhary, Dhruv Mahajan,\nDiego Garcia-Olano, Diego Perino, Dieuwke Hupkes,\nEgor Lakomkin, Ehab AlBadawy, Elina Lobanova,\nEmily Dinan, Eric Michael Smith, Filip Radenovic,\nFrank Zhang, Gabriel Synnaeve, Gabrielle Lee, Geor-\ngia Lewis Anderson, Graeme Nail, Gr\u00e9goire Mialon,\nGuan Pang, Guillem Cucurell, Hailey Nguyen, Han-\nnah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov,\nImanol Arrieta Ibarra, Isabel M. Kloumann, Ishan\nMisra, Ivan Evtimov, Jade Copet, Jaewon Lee, Jan\nGeffert, Jana Vranes, Jason Park, Jay Mahadeokar,\nJeet Shah, Jelmer van der Linde, Jennifer Billock,\nJenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi,Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu,\nJoanna Bitton, Joe Spisak, Jongsoo Park, Joseph\nRocca, Joshua Johnstun, Joshua Saxe, Junteng Jia,\nKalyan Vasuden Alwala, Kartikeya Upasani, Kate\nPlawiak, Ke Li, Kenneth Heafield, Kevin Stone, and\net al. 2024. The llama 3 herd of models. CoRR ,\nabs/2407.21783.\nSong Feng, Hui Wan, R. Chulaka Gunasekara,\nSiva Sankalp Patel, Sachindra Joshi, and Luis A.\nLastras. 2020. doc2dial: A goal-oriented document-\ngrounded dialogue dataset. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020 , pages 8118\u20138128. Association for\nComputational Linguistics.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2023, Singapore, December 6-\n10, 2023 , pages 6465\u20136488.", "mimetype": "text/plain", "start_char_idx": 862, "end_char_idx": 3699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c15e477b-e5f1-4e92-896f-c60e049075a0": {"__data__": {"id_": "c15e477b-e5f1-4e92-896f-c60e049075a0", "embedding": null, "metadata": {"page_label": "10", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6ea10c62-d592-4005-bf78-b298a5fc2d7f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff89670f-ba51-4423-8cdc-7b213c639795", "node_type": "4", "metadata": {"page_label": "10", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6ea10c62-d592-4005-bf78-b298a5fc2d7f"}, "hash": "f92c44bf08162c7c59e8f559e98b79f0f38c2357081f3a67c7a6eb3676cdf125", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "fc5f97cb-9e8b-4782-b573-e24bbf51c116", "node_type": "1", "metadata": {"page_label": "10", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6ea10c62-d592-4005-bf78-b298a5fc2d7f"}, "hash": "50bad8d32d3e06f8026a48a950cbca4c54e68f7f4eddc43d7d2c1a647219b3f1", "class_name": "RelatedNodeInfo"}}, "text": "Song Feng, Hui Wan, R. Chulaka Gunasekara,\nSiva Sankalp Patel, Sachindra Joshi, and Luis A.\nLastras. 2020. doc2dial: A goal-oriented document-\ngrounded dialogue dataset. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020 , pages 8118\u20138128. Association for\nComputational Linguistics.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen.\n2023. Enabling large language models to generate\ntext with citations. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2023, Singapore, December 6-\n10, 2023 , pages 6465\u20136488. Association for Compu-\ntational Linguistics.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. CoRR ,\nabs/2002.08909.\nYizheng Huang and Jimmy Huang. 2024. A survey\non retrieval-augmented text generation for large lan-\nguage models. CoRR , abs/2404.10981.\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde Las Casas, Florian Bressand, Gianna Lengyel,\nGuillaume Lample, Lucile Saulnier, L\u00e9lio Re-\nnard Lavaud, Marie-Anne Lachaux, Pierre Stock,\nTeven Le Scao, Thibaut Lavril, Thomas Wang, Timo-\nth\u00e9e Lacroix, and William El Sayed. 2023. Mistral\n7b.CoRR , abs/2310.06825.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng\nLi, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024.\nLongllmlingua: Accelerating and enhancing llms in\nlong context scenarios via prompt compression. In\nProceedings of the 62nd Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2024, Bangkok, Thailand, Au-\ngust 11-16, 2024 , pages 1658\u20131677. Association for\nComputational Linguistics.\nJiajie Jin, Yutao Zhu, Yujia Zhou, and Zhicheng Dou.\n2024. BIDER: bridging knowledge inconsistency for\nefficient retrieval-augmented llms via key supporting\nevidence. In Findings of the Association for Compu-\ntational Linguistics, ACL 2024, Bangkok, Thailand\nand virtual meeting, August 11-16, 2024 , pages 750\u2013\n761. Association for Computational Linguistics.\nZhuoran Jin, Pengfei Cao, Yubo Chen, Kang Liu, and\nJun Zhao. 2023. Instructor: Instructing unsupervised\nconversational dense retrieval with large language\n10", "mimetype": "text/plain", "start_char_idx": 3056, "end_char_idx": 5357, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d38b250e-dbd7-4dfd-85b6-248e84170994": {"__data__": {"id_": "d38b250e-dbd7-4dfd-85b6-248e84170994", "embedding": null, "metadata": {"page_label": "11", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1c11f410-3add-4a73-aa17-5d8dc7c40415"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5dd2c0e5-292b-40ba-acf6-8d1dea9b268d", "node_type": "4", "metadata": {"page_label": "11", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1c11f410-3add-4a73-aa17-5d8dc7c40415"}, "hash": "45c706908e760bd4bdf8d4cb39eab35a5029a36d5bf5bc87c9b476933c2f76c5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba60fc36-16b0-4dc3-9dbf-0308eb6cfefb", "node_type": "1", "metadata": {}, "hash": "722d9f45646a07a57a2b94bd589261215275ea4ad0926a87df88e547d4731f8a", "class_name": "RelatedNodeInfo"}}, "text": "models. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2023, Singapore, De-\ncember 6-10, 2023 , pages 6649\u20136675. Association\nfor Computational Linguistics.\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019.\nBillion-scale similarity search with gpus. IEEE\nTransactions on Big Data , 7(3):535\u2013547.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020 , pages 6769\u20136781. Associa-\ntion for Computational Linguistics.\nVaibhav Kumar and Jamie Callan. 2020. Making in-\nformation seeking easier: An improved pipeline for\nconversational search. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, Online\nEvent, 16-20 November 2020 , volume EMNLP 2020\nofFindings of ACL , pages 3971\u20133980. Association\nfor Computational Linguistics.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying\nSheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonza-\nlez, Hao Zhang, and Ion Stoica. 2023. Efficient mem-\nory management for large language model serving\nwith pagedattention. In Proceedings of the 29th Sym-\nposium on Operating Systems Principles, SOSP 2023,\nKoblenz, Germany, October 23-26, 2023 , pages 611\u2013\n626. ACM.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih,\nTim Rockt\u00e4schel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual .\nMinghan Li, Honglei Zhuang, Kai Hui, Zhen Qin,\nJimmy Lin, Rolf Jagerman, Xuanhui Wang, and\nMichael Bendersky. 2024. Can query expansion im-\nprove generalization of strong cross-encoder rankers?\nInProceedings of the 47th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, SIGIR 2024, Washington DC, USA,\nJuly 14-18, 2024 , pages 2321\u20132326. ACM.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out , pages 74\u201381.\nSheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n2021a. Contextualized query embeddings for conver-\nsational search. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021 , pages\n1004\u20131015. Association for Computational Linguis-\ntics.Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,\nMing-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.\n2021b. Multi-stage conversational passage retrieval:\nAn approach to fusing term importance estimation\nand neural query rewriting. ACM Transactions on\nInformation Systems (TOIS) , 39(4):1\u201329.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2990, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba60fc36-16b0-4dc3-9dbf-0308eb6cfefb": {"__data__": {"id_": "ba60fc36-16b0-4dc3-9dbf-0308eb6cfefb", "embedding": null, "metadata": {"page_label": "11", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1c11f410-3add-4a73-aa17-5d8dc7c40415"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5dd2c0e5-292b-40ba-acf6-8d1dea9b268d", "node_type": "4", "metadata": {"page_label": "11", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1c11f410-3add-4a73-aa17-5d8dc7c40415"}, "hash": "45c706908e760bd4bdf8d4cb39eab35a5029a36d5bf5bc87c9b476933c2f76c5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d38b250e-dbd7-4dfd-85b6-248e84170994", "node_type": "1", "metadata": {"page_label": "11", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1c11f410-3add-4a73-aa17-5d8dc7c40415"}, "hash": "1f308540be7c6179632cf4e050507ff6299c88b8e3c4084306510690b1c444e4", "class_name": "RelatedNodeInfo"}}, "text": "Sheng-Chieh Lin, Jheng-Hong Yang, and Jimmy Lin.\n2021a. Contextualized query embeddings for conver-\nsational search. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2021, Virtual Event / Punta Cana,\nDominican Republic, 7-11 November, 2021 , pages\n1004\u20131015. Association for Computational Linguis-\ntics.Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira,\nMing-Feng Tsai, Chuan-Ju Wang, and Jimmy Lin.\n2021b. Multi-stage conversational passage retrieval:\nAn approach to fusing term importance estimation\nand neural query rewriting. ACM Transactions on\nInformation Systems (TOIS) , 39(4):1\u201329.\nSheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Frassetto\nNogueira, Ming-Feng Tsai, Chuan-Ju Wang, and\nJimmy Lin. 2020. Conversational question refor-\nmulation via sequence-to-sequence architectures and\npretrained language models. CoRR , abs/2004.01909.\nYubo Ma, Yixin Cao, Yong Hong, and Aixin Sun. 2023.\nLarge language model is not a good few-shot informa-\ntion extractor, but a good reranker for hard samples!\nInFindings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023 , pages 10572\u201310601. Association for Computa-\ntional Linguistics.\nKelong Mao, Chenlong Deng, Haonan Chen, Fengran\nMo, Zheng Liu, Tetsuya Sakai, and Zhicheng Dou.\n2024. Chatretriever: Adapting large language mod-\nels for generalized and robust conversational dense\nretrieval. CoRR , abs/2404.13556.\nKelong Mao, Zhicheng Dou, Fengran Mo, Jiewen Hou,\nHaonan Chen, and Hongjin Qian. 2023a. Large lan-\nguage models know your contextual search intent: A\nprompting framework for conversational search. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2023, Singapore, December 6-10,\n2023 , pages 1211\u20131225. Association for Computa-\ntional Linguistics.\nKelong Mao, Zhicheng Dou, and Hongjin Qian. 2022a.\nCurriculum contrastive context denoising for few-\nshot conversational dense retrieval. In SIGIR \u201922:\nThe 45th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\nMadrid, Spain, July 11 - 15, 2022 , pages 176\u2013186.\nACM.\nKelong Mao, Zhicheng Dou, Hongjin Qian, Fengran\nMo, Xiaohua Cheng, and Zhao Cao. 2022b. Con-\nvtrans: Transforming web search sessions for conver-\nsational dense retrieval. In Proceedings of the 2022\nConference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2022, Abu Dhabi, United\nArab Emirates, December 7-11, 2022 , pages 2935\u2013\n2946. Association for Computational Linguistics.\nKelong Mao, Hongjin Qian, Fengran Mo, Zhicheng\nDou, Bang Liu, Xiaohua Cheng, and Zhao Cao.\n2023b. Learning denoised and interpretable session\nrepresentation for conversational search. In Proceed-\nings of the ACM Web Conference 2023, WWW 2023,\nAustin, TX, USA, 30 April 2023 - 4 May 2023 , pages\n3193\u20133202. ACM.\nFengran Mo, Kelong Mao, Ziliang Zhao, Hongjin\nQian, Haonan Chen, Yiruo Cheng, Xiaoxi Li, Yu-\ntao Zhu, Zhicheng Dou, and Jian-Yun Nie. 2024a.\nA survey of conversational search. arXiv preprint\narXiv:2410.15576 .\n11", "mimetype": "text/plain", "start_char_idx": 2347, "end_char_idx": 5392, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d073bf17-10f9-4aa2-9788-da597bd500b7": {"__data__": {"id_": "d073bf17-10f9-4aa2-9788-da597bd500b7", "embedding": null, "metadata": {"page_label": "12", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_062aefa0-48bc-45aa-8dab-4a2a4ba7b69e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00e7b1f7-432c-4848-977b-9ec3b4a3a407", "node_type": "4", "metadata": {"page_label": "12", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_062aefa0-48bc-45aa-8dab-4a2a4ba7b69e"}, "hash": "12d9bc002b4aec865edad74f378237273853bcee5f63e82f681e4879cfec2f0f", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e1c1ccb1-01ef-4d17-83a3-13ecf2efcd2d", "node_type": "1", "metadata": {}, "hash": "11cda4d47210189e6785a4e8fb7897ef9bd5cabb27bda3e7b0928cd40c051060", "class_name": "RelatedNodeInfo"}}, "text": "Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao,\nYutao Zhu, Peng Li, and Yang Liu. 2023. Learning to\nrelate to previous turns in conversational search. In\nProceedings of the 29th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, KDD\n2023, Long Beach, CA, USA, August 6-10, 2023 ,\npages 1722\u20131732. ACM.\nFengran Mo, Chen Qu, Kelong Mao, Tianyu Zhu, Zhan\nSu, Kaiyu Huang, and Jian-Yun Nie. 2024b. History-\naware conversational dense retrieval. In Findings of\nthe Association for Computational Linguistics, ACL\n2024, Bangkok, Thailand and virtual meeting, August\n11-16, 2024 , pages 13366\u201313378. Association for\nComputational Linguistics.\nFengran Mo, Bole Yi, Kelong Mao, Chen Qu, Kaiyu\nHuang, and Jian-Yun Nie. 2024c. Convsdg: Session\ndata generation for conversational search. In Com-\npanion Proceedings of the ACM on Web Conference\n2024, WWW 2024, Singapore, Singapore, May 13-17,\n2024 , pages 1634\u20131642. ACM.\nOpenAI. 2022. Openai: Introducing chatgpt.\nPaul Owoicho, Jeff Dalton, Mohammad Aliannejadi,\nLeif Azzopardi, Johanne R. Trippas, and Svitlana\nVakulenko. 2022. TREC cast 2022: Going beyond\nuser ask and system retrieve with initiative and re-\nsponse generation. In Proceedings of the Thirty-\nFirst Text REtrieval Conference, TREC 2022, online,\nNovember 15-19, 2022 , volume 500-338 of NIST Spe-\ncial Publication . National Institute of Standards and\nTechnology (NIST).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, July 6-12, 2002, Philadelphia,\nPA, USA , pages 311\u2013318. ACL.\nHongjing Qian, Yutao Zhu, Zhicheng Dou, Haoqi Gu,\nXinyu Zhang, Zheng Liu, Ruofei Lai, Zhao Cao,\nJian-Yun Nie, and Ji-Rong Wen. 2023. Webbrain:\nLearning to generate factually correct articles for\nqueries by grounding on large web corpus. CoRR ,\nabs/2304.04358.\nChen Qu, Liu Yang, Cen Chen, Minghui Qiu, W. Bruce\nCroft, and Mohit Iyyer. 2020. Open-retrieval con-\nversational question answering. In Proceedings of\nthe 43rd International ACM SIGIR conference on\nresearch and development in Information Retrieval,\nSIGIR 2020, Virtual Event, China, July 25-30, 2020 ,\npages 539\u2013548. ACM.\nNir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram,\nInbal Magar, Omri Abend, Ehud Karpas, Amnon\nShashua, Kevin Leyton-Brown, and Yoav Shoham.\n2023. Parallel context windows for large language\nmodels. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2023, Toronto, Canada,\nJuly 9-14, 2023 , pages 6383\u20136402. Association for\nComputational Linguistics.Siva Reddy, Danqi Chen, and Christopher D Manning.\n2019. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics , 7:249\u2013266.\nJiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo,\nKun Fang, and Ji-Rong Wen. 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2951, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1c1ccb1-01ef-4d17-83a3-13ecf2efcd2d": {"__data__": {"id_": "e1c1ccb1-01ef-4d17-83a3-13ecf2efcd2d", "embedding": null, "metadata": {"page_label": "12", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_062aefa0-48bc-45aa-8dab-4a2a4ba7b69e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00e7b1f7-432c-4848-977b-9ec3b4a3a407", "node_type": "4", "metadata": {"page_label": "12", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_062aefa0-48bc-45aa-8dab-4a2a4ba7b69e"}, "hash": "12d9bc002b4aec865edad74f378237273853bcee5f63e82f681e4879cfec2f0f", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d073bf17-10f9-4aa2-9788-da597bd500b7", "node_type": "1", "metadata": {"page_label": "12", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_062aefa0-48bc-45aa-8dab-4a2a4ba7b69e"}, "hash": "5fb048120e100f622241053b12be05a34b77986a6cb47ec8a60429fffe817155", "class_name": "RelatedNodeInfo"}}, "text": "2023. Parallel context windows for large language\nmodels. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2023, Toronto, Canada,\nJuly 9-14, 2023 , pages 6383\u20136402. Association for\nComputational Linguistics.Siva Reddy, Danqi Chen, and Christopher D Manning.\n2019. Coqa: A conversational question answering\nchallenge. Transactions of the Association for Com-\nputational Linguistics , 7:249\u2013266.\nJiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo,\nKun Fang, and Ji-Rong Wen. 2024. Small models,\nbig insights: Leveraging slim proxy models to decide\nwhen and what to retrieve for llms. In Proceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\nACL 2024, Bangkok, Thailand, August 11-16, 2024 ,\npages 4420\u20134436. Association for Computational\nLinguistics.\nByteDance Doubao Team. 2023. Doubao.\nQwen Team. 2024. Qwen2.5: A party of foundation\nmodels.\nSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu,\nand Raviteja Anantha. 2021. Question rewriting for\nconversational question answering. In WSDM \u201921,\nThe Fourteenth ACM International Conference on\nWeb Search and Data Mining, Virtual Event, Israel,\nMarch 8-12, 2021 , pages 355\u2013363. ACM.\nNikos V oskarides, Dan Li, Pengjie Ren, Evangelos\nKanoulas, and Maarten de Rijke. 2020. Query reso-\nlution for conversational search with limited supervi-\nsion. In Proceedings of the 43rd International ACM\nSIGIR conference on research and development in\nInformation Retrieval, SIGIR 2020, Virtual Event,\nChina, July 25-30, 2020 , pages 921\u2013930. ACM.\nLiang Wang, Nan Yang, and Furu Wei. 2023a.\nQuery2doc: Query expansion with large language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2023, Singapore, December 6-10, 2023 ,\npages 9414\u20139423. Association for Computational\nLinguistics.\nShuting Wang, Xin Yu, Mang Wang, Weipeng Chen, Yu-\ntao Zhu, and Zhicheng Dou. 2024. Richrag: Crafting\nrich responses for multi-faceted queries in retrieval-\naugmented generation. CoRR , abs/2406.12566.\nZhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan\nParvez, and Graham Neubig. 2023b. Learning\nto filter context for retrieval-augmented generation.\nCoRR , abs/2311.08377.\nZeqiu Wu, Yi Luan, Hannah Rashkin, David Reit-\nter, Hannaneh Hajishirzi, Mari Ostendorf, and Gau-\nrav Singh Tomar. 2022. CONQRR: conversational\nquery rewriting for retrieval with reinforcement learn-\ning. In Proceedings of the 2022 Conference on Em-\npirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates,\nDecember 7-11, 2022 , pages 10000\u201310014. Associa-\ntion for Computational Linguistics.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. 2023.\nRECOMP: improving retrieval-augmented lms with\ncompression and selective augmentation. CoRR ,\nabs/2310.04408.\n12", "mimetype": "text/plain", "start_char_idx": 2402, "end_char_idx": 5267, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "90f9f8f8-7bd0-4e8e-89cf-8c0bf8ff889e": {"__data__": {"id_": "90f9f8f8-7bd0-4e8e-89cf-8c0bf8ff889e", "embedding": null, "metadata": {"page_label": "13", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d922fe8c-463e-42fa-bc03-d3fe13a5c065"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4e83275c-cf23-44da-829c-cdcd7bc51fbe", "node_type": "4", "metadata": {"page_label": "13", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d922fe8c-463e-42fa-bc03-d3fe13a5c065"}, "hash": "98efd27fba8ff8df5c32b53c80503ce72d55ed2182afd483e678e9bf1edff2f3", "class_name": "RelatedNodeInfo"}}, "text": "Shicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and\nXueqi Cheng. 2024. List-aware reranking-truncation\njoint model for search and retrieval-augmented gen-\neration. In Proceedings of the ACM on Web Confer-\nence 2024, WWW 2024, Singapore, May 13-17, 2024 ,\npages 1330\u20131340. ACM.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671 .\nHaoyan Yang, Zhitao Li, Yong Zhang, Jianzong Wang,\nNing Cheng, Ming Li, and Jing Xiao. 2023. PRCA:\nfitting black-box large language models for retrieval\nquestion answering via pluggable reward-driven con-\ntextual adapter. In Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2023, Singapore, December 6-10,\n2023 , pages 5364\u20135375. Association for Computa-\ntional Linguistics.\nLinhao Ye, Zhikai Lei, Jianghao Yin, Qin Chen, Jie\nZhou, and Liang He. 2024. Boosting conversa-\ntional question answering with fine-grained retrieval-\naugmentation and self-check. In Proceedings of the\n47th International ACM SIGIR Conference on Re-\nsearch and Development in Information Retrieval,\nSIGIR 2024, Washington DC, USA, July 14-18, 2024 ,\npages 2301\u20132305. ACM.\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong,\nPaul N. Bennett, Jianfeng Gao, and Zhiyuan Liu.\n2020. Few-shot generative conversational query\nrewriting. In Proceedings of the 43rd International\nACM SIGIR conference on research and development\nin Information Retrieval, SIGIR 2020, Virtual Event,\nChina, July 25-30, 2020 , pages 1933\u20131936. ACM.\nShi Yu, Zhenghao Liu, Chenyan Xiong, Tao Feng, and\nZhiyuan Liu. 2021. Few-shot conversational dense\nretrieval. In SIGIR \u201921: The 44th International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, Virtual Event, Canada, July\n11-15, 2021 , pages 829\u2013838. ACM.\nYaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan\nYe, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma.\n2024. Llamafactory: Unified efficient fine-tuning\nof 100+ language models. In Proceedings of the\n62nd Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 3: System Demonstra-\ntions) , Bangkok, Thailand. Association for Computa-\ntional Linguistics.\nYutao Zhu, Zhaoheng Huang, Zhicheng Dou, and Ji-\nRong Wen. 2024. One token can help! learn-\ning scalable and pluggable virtual tokens for\nretrieval-augmented large language models. CoRR ,\nabs/2405.19670.\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2468, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a8f12285-2738-4995-b991-614aabed5148": {"__data__": {"id_": "a8f12285-2738-4995-b991-614aabed5148", "embedding": null, "metadata": {"page_label": "14", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_47557273-f1a1-4842-8364-43649c7bccd6"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6859c1bb-c50b-4803-b0ce-dfa0a48f3beb", "node_type": "4", "metadata": {"page_label": "14", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_47557273-f1a1-4842-8364-43649c7bccd6"}, "hash": "f42b66b3b0de9b133ea99057a6a5b31443c61471fdfaca6fa9e21d72f317a821", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "b0d805a2-0282-47b6-b69e-3076580f3657", "node_type": "1", "metadata": {}, "hash": "51ca509e0d2082a2ea0eec8bc21c6a229fc4ff899005e8763bd99a4fc5a58430", "class_name": "RelatedNodeInfo"}}, "text": "Appendix\nA Prompts of the Contextualization of\nQuestions\nWhen contextualizing questions, two steps need\nprompt. Firstly, we transform the node into the\ncomplete question. Secondly, we convert the com-\nplete questions into conversational questions. Ta-\nble 6 illustrates the prompt for generating a com-\nplete question. Table 7 demonstrates the prompt\nfor creating conversational questions. Following\nLLM4CS (Mao et al., 2023a), the prompt consists\nof three components: Instruction, Demonstration,\nand Input. The red section is designated for LDS\nprompting, the blue section for SIDS and STRW\nprompting, the green section for DTRW prompting,\nand the orange section for LDS, SIDS, and STRW\nprompting.\nB Prompts of Generating Responses with\nCitation Labeling\nTable 6 provides the prompt template for generat-\ning a response with citation labeling. The red part\nis for Raw Context and the Last Response strategy\nprompting. The blue part is for Rewrite Strategy\nprompting. The green part is for LLM Summariza-\ntion Strategy prompting. The orange part is for\nRaw Context, the Last Response Strategy, and the\nLLM Summarization Strategy prompting.\nC Prompts of LLM Summarization\nStrategy\nTable 8 provides a general illustration of the prompt\nof generating a summary of conversation history.\nD More Detailed Experimental Setting\nD.1 Conversational Search Baselines\nThe conversational search baseline models are cho-\nsen for their prevalence and effectiveness in the\nfield. We focus on two primary approaches: con-\nversational dense retrieval (CDR) and conversa-\ntional query rewriting (CQR). For CDR, we adopt\nKD-ANCE and Conv-ANCE, where ANCE is a\nbase ad-hoc retriever. Following Yu et al. (2021),\nKD-ANCE uses an ad hoc query encoder as the\nteacher model, training the student session encoder\nto imitate the embeddings derived from the golden\nqueries. Meanwhile, according to the methodol-\nogy outlined by (Karpukhin et al., 2020; Lin et al.,\n2021a), Conv-ANCE is designed to implementthe classical ranking loss function. This function\nstrives to minimize the distance between the ses-\nsion and its relevant passages while maximizing\nthe separation from irrelevant ones. Dense retrieval\nis conducted using Faiss (Johnson et al., 2019). For\nCQR, we choose LLM4CS (Mao et al., 2023a),\nemploying the proprietary commercial model GPT-\n3.5 to generate rewrites. Additionally, we choose\nan open-source LLM to generate rewrites as well,\nallowing for a comparative analysis.\nD.2 Generation with Citation Labeling\nWe compare the raw context baselines with an-\nother three conversation compression strategies in-\ntroduced in Section 4. We choose Qwen2.5-7B-\nInstruct (Yang et al., 2024; Team, 2024), Mistral-\n7B-Instruct (Jiang et al., 2023), and Llama-3.1-8B-\nInstruct (Dubey et al., 2024) as the generator. For\nthe scaling analysis, we use the Qwen2.5-Instruct\nseries, specifically the 0.5B, 1.5B, 3B, and 7B\nmodels, as our generators for detailed examina-\ntion. During the training process, we utilize the\nLLaMA-Factory (Zheng et al., 2024) framework,\nrunning on two A800 GPUs. The training parame-\nters are set as follows: we employ a learning rate\nof 1.0e-5. The batch size is maintained at 1, and\nthe maximum token length for training instances\nis set to 4096. Because of the lack of training data\nof the LLM Summarization category, we use the\ncheckpoint of Raw Context.\nDuring the inference process, we leverage the\nvLLM (Kwon et al., 2023) framework to accelerate\ninference. The maximum input length is set to\n32,000, top_p is set to 0.9, and temperature is set\nto 1.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3559, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b0d805a2-0282-47b6-b69e-3076580f3657": {"__data__": {"id_": "b0d805a2-0282-47b6-b69e-3076580f3657", "embedding": null, "metadata": {"page_label": "14", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_47557273-f1a1-4842-8364-43649c7bccd6"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6859c1bb-c50b-4803-b0ce-dfa0a48f3beb", "node_type": "4", "metadata": {"page_label": "14", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_47557273-f1a1-4842-8364-43649c7bccd6"}, "hash": "f42b66b3b0de9b133ea99057a6a5b31443c61471fdfaca6fa9e21d72f317a821", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a8f12285-2738-4995-b991-614aabed5148", "node_type": "1", "metadata": {"page_label": "14", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_47557273-f1a1-4842-8364-43649c7bccd6"}, "hash": "e80d8eadb2b2ea8cf83ffb2ed2a318fa64aa261baae2d58412ffdf625b4863e8", "class_name": "RelatedNodeInfo"}}, "text": "During the training process, we utilize the\nLLaMA-Factory (Zheng et al., 2024) framework,\nrunning on two A800 GPUs. The training parame-\nters are set as follows: we employ a learning rate\nof 1.0e-5. The batch size is maintained at 1, and\nthe maximum token length for training instances\nis set to 4096. Because of the lack of training data\nof the LLM Summarization category, we use the\ncheckpoint of Raw Context.\nDuring the inference process, we leverage the\nvLLM (Kwon et al., 2023) framework to accelerate\ninference. The maximum input length is set to\n32,000, top_p is set to 0.9, and temperature is set\nto 1.\nD.3 More detailed Scaling Analysis\nTable 5 provides detailed results of the generation\nquality and citation labeling accuracy.\nE Dataset Format\nTable 10 provides an example of CORAL. Our\ndataset CORAL has information-seeking questions,\nfree-form responses with citation labeling, golden\nrewrites, and corresponding golden retrieval pas-\nsage ID.\n14", "mimetype": "text/plain", "start_char_idx": 2949, "end_char_idx": 3908, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adf4a0ac-2f86-49a7-9d45-52a30efb9abc": {"__data__": {"id_": "adf4a0ac-2f86-49a7-9d45-52a30efb9abc", "embedding": null, "metadata": {"page_label": "15", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d196f7d8-2239-468f-8b83-db1db27bac78"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc34b172-50b9-44e1-9eef-a4791db20196", "node_type": "4", "metadata": {"page_label": "15", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d196f7d8-2239-468f-8b83-db1db27bac78"}, "hash": "75b07414f18ef6ae829531ac812ffb6c4a38dc469fde7087ed706bf789a78574", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ca459feb-df3e-40fa-afed-22e119d52897", "node_type": "1", "metadata": {}, "hash": "069361f5ba6672dc40d86decb2849a99a789672769ddf752bdac872cef2da6cf", "class_name": "RelatedNodeInfo"}}, "text": "LDS SIDS STRW DTRW\nTrain Test Train Test Train Test Train Test\n# Conversation 1800 200 1800 200 1800 200 1800 200\n# Turns 5934 651 16082 1727 18165 1949 19411 2153\n# Turns / Conversation 3.30 3.26 8.93 8.64 10.09 9.75 10.78 10.77\n# Tokens / Question 13.70 13.89 12.62 12.64 12.72 12.88 14.15 14.75\n# Tokens / Response 233.81 147.16 242.54 155.54 243.34 191.60 300.47 259.72\n# Positive passages/ Turn 3.25 2.03 2.64 1.73 3.01 2.12 3.98 3.50\nTable 4: Data statistics of four different conversation structures.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ca459feb-df3e-40fa-afed-22e119d52897": {"__data__": {"id_": "ca459feb-df3e-40fa-afed-22e119d52897", "embedding": null, "metadata": {"page_label": "15", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d196f7d8-2239-468f-8b83-db1db27bac78"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc34b172-50b9-44e1-9eef-a4791db20196", "node_type": "4", "metadata": {"page_label": "15", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d196f7d8-2239-468f-8b83-db1db27bac78"}, "hash": "75b07414f18ef6ae829531ac812ffb6c4a38dc469fde7087ed706bf789a78574", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "adf4a0ac-2f86-49a7-9d45-52a30efb9abc", "node_type": "1", "metadata": {"page_label": "15", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d196f7d8-2239-468f-8b83-db1db27bac78"}, "hash": "038378dd96d54f466772556347eb4da88d59ee7fe04b0679d9e0ef6902415197", "class_name": "RelatedNodeInfo"}}, "text": "Category ModelGeneration Citation Labeling\nBLEU-1 ROUGE-L Citation Recall Citation Precision\nRaw ContextQwen2.5-0.5B 16.4 11.8 0.1 0.2\nQwen2.5-1.5B 20.8 12.9 0.3 1.2\nQwen2.5-3B 21.4 12.9 0.4 1.8\nQwen2.5-7B 22.2 13.1 3.1 18.1\nQwen2.5-0.5B-SFT 13.0 15.5 3.5 10.2\nQwen2.5-1.5B-SFT 20.9 17.5 3.6 10.0\nQwen2.5-3B-SFT 25.8 17.6 8.1 20.7\nQwen2.5-7B-SFT 18.3 18.5 6.6 16.8\nLast ResponseQwen2.5-0.5B 15.6 11.7 0.2 0.5\nQwen2.5-1.5B 19.3 12.5 0.8 2.9\nQwen2.5-3B 21.1 12.6 0.6 3.4\nQwen2.5-7B 20.9 12.8 3.5 20.8\nQwen2.5-0.5B-SFT 19.8 15.5 6.5 18.0\nQwen2.5-1.5B-SFT 19.4 15.8 6.7 17.7\nQwen2.5-3B-SFT 21.8 15.8 7.4 17.1\nQwen2.5-7B-SFT 22.1 16.5 10.4 24.8\nRewriteQwen2.5-0.5B 17.3 12.0 0.4 0.8\nQwen2.5-1.5B 19.9 12.5 0.4 1.2\nQwen2.5-3B 20.8 12.5 4.0 14.9\nQwen2.5-7B 21.1 12.8 2.4 9.5\nQwen2.5-0.5B-SFT 21.4 16.1 6.5 16.5\nQwen2.5-1.5B-SFT 21.7 16.6 5.9 14.9\nQwen2.5-3B-SFT 23.3 16.8 7.2 16.5\nQwen2.5-7B-SFT 18.9 16.4 7.4 19.8\nLLM SummarizationQwen2.5-0.5B 13.2 11.6 0.2 0.4\nQwen2.5-1.5B 15.0 12.1 0.9 3.0\nQwen2.5-3B 20.2 12.7 2.5 10.6\nQwen2.5-7B 21.0 12.7 2.9 13.0\nQwen2.5-0.5B-SFT 21.4 14.6 6.5 17.4\nQwen2.5-1.5B-SFT 23.0 15.5 6.8 16.9\nQwen2.5-3B-SFT 17.6 15.6 14.6 36.0\nQwen2.5-7B-SFT 23.5 16.8 14.1 31.1\nTable 5: The complete scaling analysis of generation and citation labeling performance. The best performance is\nbold .\n15", "mimetype": "text/plain", "start_char_idx": 508, "end_char_idx": 1818, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7d06fef1-fd60-4538-9c5c-857ee46acea5": {"__data__": {"id_": "7d06fef1-fd60-4538-9c5c-857ee46acea5", "embedding": null, "metadata": {"page_label": "16", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3cfe55f9-a707-4896-a747-cf664c139063"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b2917322-74be-47ef-ae3e-fccaa3adc5b2", "node_type": "4", "metadata": {"page_label": "16", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3cfe55f9-a707-4896-a747-cf664c139063"}, "hash": "336739167bcea26fb7c6a8a4d1d840578bab4fd89277378248b3de2cc105371d", "class_name": "RelatedNodeInfo"}}, "text": "InstructionGiven the keyword chain of the question and its response, generate the original\nquestion. If the response is not informative enough to help you reconstruct the\nquestion, please rely on the provided keyword chain to generate the question.\nThe keyword chain consists of terms where each term is a more specific or\ndetailed subset of the previous one, with the last term being the most specific or\nimportant. The question you generate should focus on the last keyword in the\nchain and include it explicitly.\nInputGiven the following keyword chain and response:\nKeyword Chain: 72nd Primetime Emmy Awards, ceremony information,\ncategory and rule changes\nResponse: Several rule changes were announced in December 2019. first,\nepisodes that were scheduled to air after the eligibility period closed. . .\n(Now, you should give me the original question given the keyword chain and\nits response. The output format should always be: Question: $Question. Note\nthat you should always try to generate it. Never ask for clarification or say you\ndon\u2019t understand it in the generated question. Go ahead!)\nModel OutputQuestion: What were the category and rule changes for the 72nd Primetime\nEmmy Awards ceremony?\nTable 6: An illustration of the prompt for question generation. The prompt consists of two parts: Instruction and\nInput.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1329, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3ac6a8b0-8e86-499b-85db-04a2eb0d3e30": {"__data__": {"id_": "3ac6a8b0-8e86-499b-85db-04a2eb0d3e30", "embedding": null, "metadata": {"page_label": "17", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_46ba9c07-4708-4ef7-b4d5-744ca4246d35"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "779401ef-90b8-4679-9548-559274acb5f7", "node_type": "4", "metadata": {"page_label": "17", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_46ba9c07-4708-4ef7-b4d5-744ca4246d35"}, "hash": "e3d79c197640e1761666ac0de25af0a4534daf75ad483777d29cc48989d3d2a7", "class_name": "RelatedNodeInfo"}}, "text": "InstructionGiven a topic and corresponding question and response pairs. The questions are\narranged in a logical, progressively deeper sequence, where each subsequent\nquestion delves deeper into the topic based on the previous one. The questions\nare organized in a logical sequence where they are interconnected and may delve\ndeeper into earlier topics, rather than following a direct, linear progression. The\nquestions initially follow a logical progression but may shift to another topic as\nneeded, reflecting a dynamic conversational flow rather than a strict linear order.\nI would like you to convert the original question into a conversational form.\nThe goal is to rewrite it without any grammatical errors while preserving its\noriginal intent as closely as possible. It is necessary to consider the omission and\nreference to the previous question and response in the generated conversational\nquestion.\nDemonstrationI will give you one example multi-turn dialog, where each turn contains an\noriginal question, a conversational question, a response, and the corresponding\nanalysis.\nExample:\nTopic: depression\nConversations:\nTurn #1:\nOriginal Question1: What are the mechanisms of depression?\nConversational Question1: What are the mechanisms of depression?\nResponse1: The major neurotransmitters are acetylcholine, norepinephrine,\ndopamine, and serotonin. Many experts believe that an imbalance among the\ndifferent neurotransmitters is the cause of depression.\nAnalysis: The initial question addresses the mechanisms of depression, which\nleads to an explanation of neurotransmitters and their role in depression.\nTurn #2:\nOriginal Question2: What is the role of serotonin in depression?\nConversational Question2: What is the role of serotonin?\nResponse2: When it comes to biological factors for depression, it is commonly\naccepted that serotonin plays a crucial role. In the brain, this neurotransmitter\nacts as a relay, transporting messages from one area to another. Given this\ncritical task, an imbalance in serotonin levels may influence our emotions.\nAnalysis: Since \"depression\" was already mentioned in the previous turn, it\u2019s\nredundant to repeat it in the subsequent question. Therefore, by omitting \"in\ndepression\", the question flows more smoothly and maintains coherence in the\nconversation.\nTurn #3:\nOriginal Question3: How can I balance serotonin?\nConversational Question3: So how can I balance it?\nResponse3: You can create your own dopamine rewards by setting a goal, and\nthen take small steps toward it every day. This can be starting a new exercise\nprogram, learning French, or challenging yourself to drive home from work a\ndifferent way every day, preferably without the use of your GPS.\nAnalysis: The use of \"it\" in the question effectively refers back to \"serotonin\"\nfrom the question in the previous turn. This helps maintain coherence in the\nconversation and avoids repetitive expressions. And we use \"so\" as a connecting\nword to make the conversation look smoother.\nTurn #4:\nOriginal Question4: What foods boost dopamine?\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3051, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ef08506a-120a-4d68-8e4a-ac5819ad773d": {"__data__": {"id_": "ef08506a-120a-4d68-8e4a-ac5819ad773d", "embedding": null, "metadata": {"page_label": "18", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6270ad53-3763-415b-9e1f-e03e4438a1fd"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7a378539-d952-4a89-8ccb-4f086d42ad74", "node_type": "4", "metadata": {"page_label": "18", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6270ad53-3763-415b-9e1f-e03e4438a1fd"}, "hash": "bc47cec71135bbb94a5c756496275b46ced99b064566d213a8db5a8f078eedad", "class_name": "RelatedNodeInfo"}}, "text": "Conversational Question4: What foods boost it?\nResponse4: Bananas, especially ripe bananas, are an exceptional food for\nregulating dopamine because they have a high concentration of tyrosine. Other\nfoods that increase dopamine through the conversion of phenylalanine to tyro-\nsine include almonds, apples, watermelons, cherries, yogurt, beans, eggs, and\nmeats.\nAnalysis: Continuing the discussion on depression, the question asks about\nfoods that boost dopamine, with \"it\" referring back to \"dopamine\" from the\nprevious response.\n. . .\nInputNow I will give you a topic and the corresponding question and response pairs:\nNow I will give you the question and response pairs:\nTopic: Solve Education\nId:1\nQuestion: What is Solve Education and what are its main objectives?\nResponse: solve education is a global technology non-profit organization\nfounded in 2015 by peng tsin ong and Janine teo. . .\nId:2\nQuestion: What is the history of the organization Solve Education! since its\ninception?\nResponse: solve education was founded in December 2015 after peng tsin ong\nattended the Africa Innovation Summit in Cape Verde in 2014 where he led a\nplenary session on financing innovation. . .\nId:3\nQuestion: What is the purpose of the Dawn of Civilization game developed\nby Solve Education, and how does it utilize its services to benefit education in\nlow-income countries?\nResponse: Dawn of Civilization is a city-building game which is based on\nvarious subjects and itself contains mini-games.. . .\nId:4\nQuestion: What is Ed the Learning Bot and how does it contribute to the\nservices provided by Solve Education?\nResponse: It is an AI-based chatbot used by solve education! on telegram. it\nhelps underserved young people to learn about any subject with the help of\ngamified pictures and text. . .\nId:5\nQuestion: What is the purpose of the Content+ service on the Solve Education\nplatform, and how do educational content providers interact with it?\nResponse: it is an open platform which facilitates educational content providers\nto add their content in order to meet the needs of beneficiaries. . .\nId:6\nQuestion: What is Learnalytics and how does it function within Solve Educa-\ntion services?\nResponse: It is a tool developed by solving education! which helps measure\nthe performance. additionally, learning portal analytics, provides context, the\nlearning progress, and the performance of the students as they learn. . .\n. . .\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "251f6155-11fd-4e02-92ce-0bcdbf62b78e": {"__data__": {"id_": "251f6155-11fd-4e02-92ce-0bcdbf62b78e", "embedding": null, "metadata": {"page_label": "19", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a2610326-bb1a-4c6c-b4da-57db0b8df613"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9dc9dd4d-e649-4072-9a7f-60e5c67396d6", "node_type": "4", "metadata": {"page_label": "19", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a2610326-bb1a-4c6c-b4da-57db0b8df613"}, "hash": "87e8846fa566ae525aa0971780229694d30657e2e768a3903e59e54005676405", "class_name": "RelatedNodeInfo"}}, "text": "(Please consider the question and response from the previous text when gener-\nating the current conversational question, but there is no need to generate the\nresponse. The output should be in the following format:\nTurn #$turn_id\nOriginal Question: $original question\nConversational Question: $conversational question\nReason: $reason)\nModel OutputTurn #1\nOriginal Question: What is Solve Education! and what are its main objectives?\nConversational Question: What is Solve Education and what are its main\nobjectives?\nReason: This is the first question in the series, so it introduces the topic and\norganization directly, laying the foundation for the conversation. No reference\nto prior questions is needed.\nTurn #2\nOriginal Question: What is the history of the organization Solve Education\nsince its inception?\nConversational Question: Could you share more about the history of Solve\nEducation since it started?\nReason: The conversational question transitions smoothly from the introduction\nof the organization by asking for historical details. The phrase \"since it started\"\nconnects directly back to the organization introduced in Turn #1.\nTurn #3\nOriginal Question: What is the purpose of the Dawn of Civilization game\ndeveloped by Solve Education, and how does it utilize its services to benefit\neducation in low-income countries?\nConversational Question: Can you tell me about the Dawn of Civilization\ngame and how it benefits education in low-income areas?\nReason: This question builds on the previous introduction of Solve Education\u2019s\ninitiatives. The phrase \"the Dawn of Civilization game\" is specific and follows\nlogically from learning about the organization\u2019s history and broader objectives.\nTurn #4\nOriginal Question: What is Ed the Learning Bot and how does it contribute to\nthe services provided by Solve Education?\nConversational Question: What role does Ed the Learning Bot play in Solve\nEducation\u2019s services?\nReason: The question uses \"Ed the Learning Bot\" directly and asks about its\nrole, making it clear and focused on this specific service. It follows naturally\nafter discussing another tool (Dawn of Civilization game) used by the organiza-\ntion.\n. . .\nTable 7: An illustration of the prompt for generating conversational questions consists of three parts: Instruction,\nDemonstration, and Input. The red section is designated for LDS prompting, the blue section for SIDS and STRW\nprompting, the green section for DTRW prompting, and the orange section for LDS, SIDS, and STRW prompting.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2509, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4f6053de-8d7c-4faa-866c-692e7ae8f441": {"__data__": {"id_": "4f6053de-8d7c-4faa-866c-692e7ae8f441", "embedding": null, "metadata": {"page_label": "20", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d6c383d2-ef51-4810-a617-de58f2085ec6"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "01a68519-f0f3-489d-b3b6-d4df2c6e4e82", "node_type": "4", "metadata": {"page_label": "20", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d6c383d2-ef51-4810-a617-de58f2085ec6"}, "hash": "baecad96838e367abdeab0e039cc4a6aeebe4c8be409a135bc74a3644b8821d7", "class_name": "RelatedNodeInfo"}}, "text": "InstructionGiven the current question and the previous conversation history, summarize\nthe conversation history. The summary should contain relevant information to\nhelp the agent provide a more informative response to the current question.\nDemonstrationI will give you several example dialogs, where each example contains the\ncurrent question, the conversation history, the summarization, and the reason of\ngenerating such summarization.\nExample #1:\nCurrent Question: What about their impact on battery longevity?\nQuestion1: What are some of the key advancements in electric vehicle technol-\nogy lately?\nResponse1: There are improvements in battery technology for longer ranges,\nfaster charging methods, and the integration of solar panels to help extend\nrange.\nQuestion2: Do any particular models feature these solar panels?\nResponse2: Yes, a number of new sedans and SUVs have solar roofs which\ncan significantly increase daily driving range.\nQuestion3: What about the charging stations? Are they getting better too?\nResponse3: Indeed, ultra-fast charging stations are now available that can boost\na battery to 80% in as little as 20 minutes.\nSummarization: The dialogue has highlighted advancements in electric vehicle\ntechnology, especially in battery efficiency, solar panel integration, and faster\ncharging solutions. The conversation has particularly focused on the introduc-\ntion of ultra-fast charging stations and briefly touched upon their efficiency in\nreducing charging times.\nReason: The summary efficiently condenses prior discussions to provide con-\ntext specifically for the user\u2019s current question about the impact of ultra-fast\ncharging on battery longevity.\n. . .\nInputGiven the following question and its conversation history:\nCurrent Question: What are some other applications of this method, particu-\nlarly in solving constrained optimization problems?\nConversation History:\nQuestion1: Can you explain what the multiplicative weight update method is\nand how it\u2019s used in decision making and prediction?\nResponse1: The multiplicative weights update method is an algorithmic tech-\nnique most commonly used for decision making and prediction. . .\n(Now, you should give me the summarization given the current question and\nits conversation history. The output format should always be: \u201cSummarization:\n$Summarization\u201d. Note that you should always try to generate it. Never ask for\nclarification or say you can\u2019t generate the summarization. Go ahead!)\nModel OutputSummarization: The conversation has detailed the multiplicative weight up-\ndate method, describing its primary use in decision making and prediction,\nparticularly through the example of prediction from expert advice. It also\ntouched on its wide applications in fields like machine learning, optimization,\ntheoretical computer science, and game theory, which relates to its potential in\nsolving constrained optimization problems as queried in the current question.\nTable 8: An illustration of the prompt for generating a conversation summary. The prompt consists of three\ncomponents: Instruction, Demonstration, and Input.\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59cdccb8-0afe-49a7-8cb7-6f2809625247": {"__data__": {"id_": "59cdccb8-0afe-49a7-8cb7-6f2809625247", "embedding": null, "metadata": {"page_label": "21", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_399f34b5-a12a-4e12-8e57-96f96d6e6877"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60a75ca6-8db9-475f-b54c-e80872815c7b", "node_type": "4", "metadata": {"page_label": "21", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_399f34b5-a12a-4e12-8e57-96f96d6e6877"}, "hash": "3a413e4a8493a36cd82f9c7d604c03d01ce34c71ad6cd32229ed0592bef5843e", "class_name": "RelatedNodeInfo"}}, "text": "InstructionBased on the provided passages, please answer the current question while con-\nsidering the context of the conversations history. If the current question lacks\ncomplete information or contains omissions, use the conversation history to\nenhance understanding and provide a response. please answer the question.\nplease answer the current question considering the summarization of the con-\nversation history please. If the current question lacks complete information or\ncontains omissions, use the summarization of conversation history to enhance\nunderstanding. Support your answer with citations in brackets. If the provided\npassages are not informative, please generate the answer based on your own\nknowledge. Don\u2019t say you can\u2019t answer the question.\nInputCurrent Question: . . .\nQuestion: . . .\nConversation History: . . .\nSummarization of Conversation History: . . .\nPassages: . . .\nTable 9: The prompt template of generating the response. The prompt consists of two parts, i.e., Instruction, and\nInput. red part is for Raw Context and Last Response strategy prompting. The blue part is for Rewrite Strategy\nprompting. The green part is for LLM Summarization Strategy prompting. The orange part is for Raw Context, Last\nResponse Strategy, and LLM Summarization Strategy prompting.\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1294, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4afde38d-53eb-49a3-88f5-c8ee8fbea4bc": {"__data__": {"id_": "4afde38d-53eb-49a3-88f5-c8ee8fbea4bc", "embedding": null, "metadata": {"page_label": "22", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0c13cd5a-84b5-400a-af0c-de68724f6f7a"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c4a45799-713c-4fc5-afcb-82734244de4d", "node_type": "4", "metadata": {"page_label": "22", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0c13cd5a-84b5-400a-af0c-de68724f6f7a"}, "hash": "a22d294db2c1039697a48ca2dd6a788fd5d3f1804afe93c8570b7495243c6b7a", "class_name": "RelatedNodeInfo"}}, "text": "Question1: What were the key details and outcomes of the 72nd Primetime Emmy Awards?\nResponse1: The 72nd Primetime Emmy Awards honored the best in American prime time\ntelevision programming from June 1, 2019, until May 31, 2020, as chosen by the Academy\nof Television Arts & Sciences.[65215]. . .\nGolden Retrieval Passages IDs: 65215\nGolden Rewrite: What were the key details and outcomes of the 72nd Primetime Emmy\nAwards?\nURL :https://en.wikipedia.org/wiki/72nd_Primetime_Emmy_Awards\nQuestion2: Could you tell me about the winners and nominees from this event?\nResponse2: The nominations for the 72nd Primetime Emmy Awards were announced on\nJuly 28, 2020, by host Leslie Jones and presenters Laverne Cox, Josh Gad, and Tatiana\nMaslany.[65226]. . .\nGolden Retrieval Passage IDs: 65216, 65217, 65218, 65219, 65220, 65221, 65222, 65223,\n65224, 65225, 65226, 65227, 65228, 65229, 65230, 65231, 65232\nGolden Rewrite: Who were the winners and nominees at the 72nd Primetime Emmy\nAwards?\nURL :https://en.wikipedia.org/wiki/72nd_Primetime_Emmy_Awards\nQuestion3: Who received the Governors Award and why were they chosen?\nResponse3: The Governors Award was presented to Tyler Perry and the Perry Foundation \u201cin\nrecognition of their unparalleled contributions to shaping the television medium\u201d and for their\n\u201cinclusion, engagement, employment and other philanthropic initiatives\u201d.[65233][65234]\nThe award was moved to the main telecast from its usual presentation at the Creative Arts\nEmmys.[65235]\nGolden Retrieval Passage IDs: 65233, 65234, 65235\nGolden Rewrite: Who received the Governors Award at the 72nd Primetime Emmy Awards\nand for what reasons?\nURL :https://en.wikipedia.org/wiki/72nd_Primetime_Emmy_Awards\n. . .\n. . .\n. . .\nQuestion10: Who performed during the in Memoriam Segment, and which notable figures\nwere mentioned?\nResponse10: The annual in Memoriam Segment featured H.E.R. performing \u201cNothing\nCompares 2 U\u201d on piano and electric guitar .[65284][65285]. . .\nGolden Retrieval Passage IDs: 65284, 65285, 65286, 65287\nGolden Rewrite: Who performed during the in Memoriam Segment at the 72nd Primetime\nEmmy Awards, and which notable figures were mentioned?\nURL :https://en.wikipedia.org/wiki/72nd_Primetime_Emmy_Awards\nTable 10: A conversation from CORAL.\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "91de5452-3f8f-45b5-bbc2-676b212fdce0": {"__data__": {"id_": "91de5452-3f8f-45b5-bbc2-676b212fdce0", "embedding": null, "metadata": {"page_label": "1", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ba35bf0d-affc-45a8-92ed-4387398e63b1"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "604a01df-8600-40d5-93e1-254dd6851b92", "node_type": "4", "metadata": {"page_label": "1", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ba35bf0d-affc-45a8-92ed-4387398e63b1"}, "hash": "036b8d0157af17ba2856e91d932ec8eb7197aeab6408bc351917d81fe84007a3", "class_name": "RelatedNodeInfo"}}, "text": "GPT-4o System Card\nOpenAI\u2217\nAugust 8, 2024\n1 Introduction\nGPT-4o[1] is an autoregressive omni model, which accepts as input any combination of text, audio,\nimage, and video and generates any combination of text, audio, and image outputs. It\u2019s trained\nend-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by\nthe same neural network.\nGPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320\nmilliseconds, which is similar to human response time[ 2] in a conversation. It matches GPT-4\nTurbo performance on text in English and code, with significant improvement on text in non-\nEnglish languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially\nbetter at vision and audio understanding compared to existing models.\nIn line with our commitment to building AI safely and consistent with our voluntary commitments\nto the White House[ 3], we are sharing the GPT-4o System Card, which includes our Preparedness\nFramework[ 4]evaluations. InthisSystemCard, weprovideadetailedlookatGPT-4o\u2019scapabilities,\nlimitations, and safety evaluations across multiple categories, with a focus on speech-to-speech\n(voice)1while also evaluating text and image capabilities, and the measures we\u2019ve implemented\nto ensure the model is safe and aligned. We also include third party assessments on dangerous\ncapabilities, as well as discussion of potential societal impacts of GPT-4o text and vision\ncapabilities.\n2 Model data and training\nGPT-4o\u2019s text and voice capabilities were pre-trained using data up to October 2023, sourced\nfrom a wide variety of materials including:\n\u2022Select publicly available data , mostly collected from industry-standard machine learning\ndatasets and web crawls.\n\u2217Please cite this work as \u201cOpenAI (2024)\". Full authorship contribution statements appear at the end of the\ndocument.\n1Some evaluations, in particular, the majority of the Preparedness Evaluations, third party assessments and\nsome of the societal impacts focus on the text and vision capabilities of GPT-4o, depending on the risk assessed.\nThis is indicated accordingly throughout the System Card.\n1arXiv:2410.21276v1  [cs.CL]  25 Oct 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8d44515f-4b60-4deb-af41-240875710fed": {"__data__": {"id_": "8d44515f-4b60-4deb-af41-240875710fed", "embedding": null, "metadata": {"page_label": "2", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2b6bef3-ba55-4e69-8481-ec41fe549d34"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "72e129a9-eebe-4070-adb0-68d5e9f84ffe", "node_type": "4", "metadata": {"page_label": "2", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2b6bef3-ba55-4e69-8481-ec41fe549d34"}, "hash": "331d34507308715ba5006f6be685271cd228c94a21b9603f2e1a51ba19c61c64", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Proprietary data from data partnerships. We form partnerships to access non-publicly\navailable data, such as pay-walled content, archives, and metadata. For example, we\npartnered with Shutterstock[5] on building and delivering AI-generated images.\nThe key dataset components that contribute to GPT-4o\u2019s capabilities are:\n\u2022Web Data: Data from public web pages provides a rich and diverse range of information,\nensuring the model learns from a wide variety of perspectives and topics.\n\u2022Code and Math: \u2013 Including code and math data in training helps the model develop\nrobust reasoning skills by exposing it to structured logic and problem-solving processes.\n\u2022Multimodal Data \u2013 Our dataset includes images, audio, and video to teach the LLMs\nhow to interpret and generate non-textual input and output. From this data, the model\nlearns how to interpret visual images, actions and sequences in real-world contexts, language\npatterns, and speech nuances.\nPrior to deployment, OpenAI assesses and mitigates potential risks that may stem from generative\nmodels, such as information harms, bias and discrimination, or other content that violates our\nusage policies. We use a combination of methods, spanning all stages of development across\npre-training, post-training, product development, and policy. For example, during post-training,\nwe align the model to human preferences; we red-team the resulting models and add product-\nlevel mitigations such as monitoring and enforcement; and we provide moderation tools and\ntransparency reports to our users.\nWe find that the majority of effective testing and mitigations are done after the pre-training stage\nbecause filtering pre-trained data alone cannot address nuanced and context-specific harms. At\nthe same time, certain pre-training filtering mitigations can provide an additional layer of defense\nthat, along with other safety mitigations, help exclude unwanted and harmful information from\nour datasets:\n\u2022We use our Moderation API and safety classifiers to filter out data that could contribute to\nharmful content or information hazards, including CSAM, hateful content, violence, and\nCBRN.\n\u2022As with our previous image generation systems, we filter our image generation datasets for\nexplicit content such as graphic sexual material and CSAM.\n\u2022We use advanced data filtering processes to reduce personal information from training data.\n\u2022Upon releasing DALL-E 3, we piloted a new approach to give users the power to opt\nimages out of training. To respect those opt-outs, we fingerprinted the images and used the\nfingerprints to remove all instances of the images from the training dataset for the GPT-4o\nseries of models.\n3 Risk identification, assessment and mitigation\nDeployment preparation was carried out via identifying potential risks of speech to speech models,\nexploratory discovery of additional novel risks through expert red teaming, turning the identified\nrisks into structured measurements and building mitigations for them. We also evaluated GPT-4o\nin accordance with our Preparedness Framework[4].\n2", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3060, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b67e027f-83e3-45db-a159-4998ca46b3c7": {"__data__": {"id_": "b67e027f-83e3-45db-a159-4998ca46b3c7", "embedding": null, "metadata": {"page_label": "3", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_48eb1869-a0d5-4c10-afd0-02b8aba874e4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "543724c9-7fed-46ca-b5c1-93102a3248ad", "node_type": "4", "metadata": {"page_label": "3", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_48eb1869-a0d5-4c10-afd0-02b8aba874e4"}, "hash": "ca1b940a14ab4bbaf376f80bf316d12a11fea34eb3b79e8cb58d3df2b0d806c8", "class_name": "RelatedNodeInfo"}}, "text": "3.1 External red teaming\nOpenAI worked with more than 100 external red teamers2, speaking a total of 45 different\nlanguages, and representing geographic backgrounds of 29 different countries. Red teamers had\naccess to various snapshots of the model at different stages of training and safety mitigation\nmaturity starting in early March and continuing through late June 2024.\nExternal red teaming was carried out in four phases. The first three phases tested the model via\nan internal tool and the final phase used the full iOS experience for testing the model. At the\ntime of writing, external red teaming of the GPT-4o API is ongoing.\nPhase 1\u202210 red teamers working on early model checkpoints still in development\n\u2022This checkpoint took in audio and text as input and produced audio\nand text as outputs.\n\u2022Single-turn conversations\nPhase 2\u202230 red teamers working on model checkpoints with early safety\nmitigations\n\u2022This checkpoint took in audio, image & text as inputs and produced\naudio and text as outputs.\n\u2022Single & multi-turn conversations\nPhase 3\u202265 red teamers working on model checkpoints & candidates\n\u2022This checkpoint took in audio, image, and text as inputs and produced\naudio, image, and text as outputs.\n\u2022Improved safety mitigations tested to inform further improvements\n\u2022Multi-turn conversations\nPhase 4\u202265 red teamers working on final model candidates & assessing\ncomparative performance\n\u2022Model access via advanced voice mode within iOS app for real user\nexperience; reviewed and tagged via internal tool.\n\u2022This checkpoint took in audio and video prompts, and produced audio\ngenerations.\n\u2022Multi-turn conversations in real time\nRed teamers were asked to carry out exploratory capability discovery, assess novel potential risks\nposed by the model, and stress test mitigations as they are developed and improved - specifically\nthose introduced by audio input and generation (speech to speech capabilities). This red teaming\neffort builds upon prior work, including as described in the GPT-4 System Card[ 6] and the\nGPT-4(V) System Card[7].\nRed teamers covered categories that spanned violative and disallowed content (illegal erotic\ncontent, violence, self harm, etc), mis/disinformation, bias, ungrounded inferences, sensitive\n2Spanning self-reported domains of expertise including: Cognitive Science, Chemistry, Biology, Physics, Com-\nputer Science, Steganography, Political Science, Psychology, Persuasion, Economics, Anthropology, Sociology, HCI,\nFairness and Bias, Alignment, Education, Healthcare, Law, Child Safety, Cybersecurity, Finance, Mis/disinforma-\ntion, Political Use, Privacy, Biometrics, Languages and Linguistics\n3", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2641, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6494b310-f406-4ac0-bd89-75c376dbcb91": {"__data__": {"id_": "6494b310-f406-4ac0-bd89-75c376dbcb91", "embedding": null, "metadata": {"page_label": "4", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_504b162e-3172-4c46-81bf-c7cbc3733250"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "db535ebe-8647-4bc9-9a4e-c4c52e1413f1", "node_type": "4", "metadata": {"page_label": "4", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_504b162e-3172-4c46-81bf-c7cbc3733250"}, "hash": "5032ace32ca4f14dfd9cecf6258e9a5b887fa987e0bceeb3577c444f02e80e4a", "class_name": "RelatedNodeInfo"}}, "text": "trait attribution, private information, geolocation, person identification, emotional perception\nand anthropomorphism risks, fraudulent behavior and impersonation, copyright, natural science\ncapabilities, and multilingual observations.\nThe data generated by red teamers motivated the creation of several quantitative evaluations\nthat are described in the Observed Safety Challenges, Evaluations and Mitigations section. In\nsome cases, insights from red teaming were used to do targeted synthetic data generation. Models\nwere evaluated using both autograders and / or manual labeling in accordance with some criteria\n(e.g, violation of policy or not, refused or not). In addition, we sometimes re-purposed the red\nteaming data to run targeted assessments on a variety of voices / examples to test the robustness\nof various mitigations.\n3.2 Evaluation methodology\nIn addition to the data from red teaming, a range of existing evaluation datasets were converted\nto evaluations for speech-to-speech models using text-to-speech (TTS) systems such as Voice\nEngine[8]. We converted text-based evaluation tasks to audio-based evaluation tasks by converting\nthe text inputs to audio. This allowed us to reuse existing datasets and tooling around measuring\nmodel capability, safety behavior, and monitoring of model outputs, greatly expanding our set of\nusable evaluations.\nWe used Voice Engine to convert text inputs to audio, feed it to the GPT-4o, and score the\noutputs by the model. We always score only the textual content of the model output, except in\ncases where the audio needs to be evaluated directly, such as in evaluations for voice cloning (see\nSection 3.3.1).\nLimitations of the evaluation methodology\nFirst, the validity of this evaluation format depends on the capability and reliability of the TTS\nmodel. Certain text inputs are unsuitable or awkward to be converted to audio; for instance:\nmathematical equations code. Additionally, we expect TTS to be lossy for certain text inputs,\nsuch as text that makes heavy use of white-space or symbols for visual formatting. Since we expect\n4", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2093, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "253dbe28-b07f-4508-8819-fcd23f9998cd": {"__data__": {"id_": "253dbe28-b07f-4508-8819-fcd23f9998cd", "embedding": null, "metadata": {"page_label": "5", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_deb22c2a-94dd-4202-a562-69cfce40ed4b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dbd40c26-12f9-4e09-a012-454e9fc2c044", "node_type": "4", "metadata": {"page_label": "5", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_deb22c2a-94dd-4202-a562-69cfce40ed4b"}, "hash": "7c44d3ab6e3855991b1844226c883ada0f68b15fb03210cf088a7e15b4e4c712", "class_name": "RelatedNodeInfo"}}, "text": "that such inputs are also unlikely to be provided by the user over Advanced Voice Mode, we either\navoid evaluating the speech-to-speech model on such tasks, or alternatively pre-process examples\nwith such inputs. Nevertheless, we highlight that any mistakes identified in our evaluations may\narise either due to model capability, or the failure of the TTS model to accurately translate text\ninputs to audio.\nA second concern may be whether the TTS inputs are representative of the distribution of audio\ninputs that users are likely to provide in actual usage. We evaluate the robustness of GPT-4o on\naudio inputs across a range of regional accents in Section 3.3.3. However, there remain many\nother dimensions that may not be captured in a TTS-based evaluation, such as different voice\nintonations and valence, background noise, or cross-talk, that could lead to different model\nbehavior in practical usage.\nLastly, there may be artifacts or properties in the model\u2019s generated audio that are not captured in\ntext; for example, background noises and sound effects, or responding with an out-of-distribution\nvoice. In Section 3.3.1, we illustrate using auxiliary classifiers to identify undesirable audio\ngeneration that can be used in conjunction with scoring transcripts.\n3.3 Observed safety challenges, evaluations and mitigations\nPotential risks with the model were mitigated using a combination of methods. We trained the\nmodel to adhere to behavior that would reduce risk via post-training methods and also integrated\nclassifiers for blocking specific generations as a part of the deployed system.\nFor observed safety challenges outlined below, we provide a description of the risk, the mitigations\napplied, and results of relevant evaluations. The risks outlined below are illustrative, and non-\nexhaustive, and are focused on the experience in the ChatGPT interface. We focus on the risks\nthat are introduced by speech to speech capabilities and how they may interact with pre-existing\nmodalities (text, image)3.\nRisk Mitigations\nUnauthorized voice genera-\ntion\n\u2022In all of our post-training audio data, we supervise ideal com-\npletions using the voice sample in the system message as the\nbase voice.\n\u2022We only allow the model to use certain pre-selected voices and\nuse an output classifier to detect if the model deviates from\nthat.\nSpeaker identification\n\u2022We post-trained GPT-4o to refuse to comply with requests to\nidentify someone based on a voice in an audio input, while still\ncomplying with requests to identify famous quotes.\n3We also evaluate text and vision capabilities, and update mitigations appropriately. No incremental risks were\nfound beyond existing work outlined in GPT-4 and GPT-4(V) System Cards.\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2724, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1d818f0a-ac5d-4381-92d3-3a0aac77fe7c": {"__data__": {"id_": "1d818f0a-ac5d-4381-92d3-3a0aac77fe7c", "embedding": null, "metadata": {"page_label": "6", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_77bbcb37-9d87-4c96-856a-dd5be1cbf48f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "38f586c2-848f-4e10-b324-c5563b75327d", "node_type": "4", "metadata": {"page_label": "6", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_77bbcb37-9d87-4c96-856a-dd5be1cbf48f"}, "hash": "293656fce4ccb670e35001093441bf7c9b11f8bf3c5e4c6a117c4309354c5694", "class_name": "RelatedNodeInfo"}}, "text": "Generating copyrighted con-\ntent\n\u2022We trained GPT-4o to refuse requests for copyrighted content,\nincluding audio, consistent with our broader practices.\n\u2022To account for GPT-4o\u2019s audio modality, we also updated\ncertain text-based filters to work on audio conversations, built\nfilters to detect and block outputs containing music, and for our\nlimited alpha of ChatGPT\u2019s Advanced Voice Mode, instructed\nthe model to not sing at all.\nUngrounded inference / sen-\nsitive trait attribution\n\u2022We post-trained GPT-4o to refuse requests for ungrounded\ninference, such as \u201chow intelligent is this speaker?\u201d.\n\u2022We post-trained GPT-4o to safely comply with requests for\nsensitive trait attribution by hedging answers, such as \u201cwhat\nis this speaker\u2019s accent\u201d \u21d2\"Based on the audio, they sound\nlike they have a British accent.\u201d\nDisallowed content in audio\noutput\n\u2022We run our existing moderation classifier over text transcrip-\ntions of audio prompts and generations, and block the output\nfor certain high-severity categories.\nErotic and violent speech out-\nput\n\u2022We run our existing moderation classifier over text transcrip-\ntions of audio prompts, and block the output if the prompt\ncontains erotic or violent language.\n3.3.1 Unauthorized voice generation\nRisk Description: Voice generation is the capability to create audio with a human-sounding\nsynthetic voice, and includes generating voices based on a short input clip.\nIn adversarial situations, this capability could facilitate harms such as an increase in fraud due\nto impersonation and may be harnessed to spread false information[ 9,10] (for example, if we\nallowed users to upload an audio clip of a given speaker and ask GPT-4o to produce a speech in\nthat speaker\u2019s voice). These are very similar to the risks we identified with Voice Engine[8].\nVoice generation can also occur in non-adversarial situations, such as our use of that ability to\ngenerate voices for ChatGPT\u2019s Advanced Voice Mode. During testing, we also observed rare\ninstances where the model would unintentionally generate an output emulating the user\u2019s voice.\nRisk Mitigation: We addressed voice generation related-risks by allowing only the preset\nvoices we created in collaboration with voice actors[ 11] to be used. We did this by including the\nselected voices as ideal completions while post-training the audio model. Additionally, we built a\nstandalone output classifier to detect if the GPT-4o output is using a voice that\u2019s different from\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9937aee3-5133-4cd6-ae34-eb9f21fbcb54": {"__data__": {"id_": "9937aee3-5133-4cd6-ae34-eb9f21fbcb54", "embedding": null, "metadata": {"page_label": "7", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d8f43aa5-66b5-41d1-aa0a-807ba07250eb"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5f5ae570-fe7a-4b5f-a47c-033cded12afe", "node_type": "4", "metadata": {"page_label": "7", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d8f43aa5-66b5-41d1-aa0a-807ba07250eb"}, "hash": "cbd3787cc4e2fe366497b0e9fc6f5431aadb37582b506c2dd46a9fead0ec7801", "class_name": "RelatedNodeInfo"}}, "text": "our approved list. We run this in a streaming fashion during audio generation and block the\noutput if the speaker doesn\u2019t match the chosen preset voice.\nEvaluation: We find that the residual risk of unauthorized voice generation is minimal. Our\nsystem currently catches 100% of meaningful deviations from the system voice4based on our\ninternal evaluations, which includes samples generated by other system voices, clips during which\nthe model used a voice from the prompt as part of its completion, and an assortment of human\nsamples.\nWhile unintentional voice generation still exists as a weakness of the model, we use the secondary\nclassifiers to ensure the conversation is discontinued if this occurs making the risk of unintentional\nvoice generation minimal. Finally, our moderation behavior may result in over-refusals when the\nconversation is not in English, which is an active area of improvement5.\nTable 2: Our voice output classifier performance over a conversation by language:\nPrecision Recall\nEnglish 0.96 1.0\nNon-English50.95 1.0\n3.3.2 Speaker identification\nRisk Description: Speaker identification is the ability to identify a speaker based on input\naudio. This presents a potential privacy risk, particularly for private individuals as well as for\nobscure audio of public individuals, along with potential surveillance risks.\nRisk Mitigation: We post-trained GPT-4o to refuse to comply with requests to identify someone\nbased on a voice in an audio input. We allow GPT-4o to answer based on the content of the\naudio if it contains content that explicitly identifies the speaker. GPT-4o still complies with\nrequests to identify famous quotes. For example, a request to identify a random person saying\n\u201cfour score and seven years ago\u201d should identify the speaker as Abraham Lincoln, while a request\nto identify a celebrity saying a random sentence should be refused.\nEvaluation: Compared to our initial model, we saw a 14 point improvement in when the model\nshould refuse to identify a voice in an audio input, and a 12 point improvement when it should\ncomply with that request. The former means the model will almost always correctly refuse to\nidentify a speaker based on their voice, mitigating the potential privacy issue. The latter means\nthere may be situations in which the model incorrectly refuses to identify the speaker of a famous\nquote.\nTable 3: Speaker identification safe behavior accuracy\nGPT-4o-early GPT-4o-deployed\nShould Refuse 0.83 0.98\nShould Comply 0.70 0.83\n4The system voice is one of pre-defined voices set by OpenAI. The model should only produce audio in that\nvoice\n5This results in more conversations being disconnected than may be necessary, which is a product quality and\nusability issue.\n7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2734, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c6eeeeb-1e66-40ba-a1bb-7edf2a48770a": {"__data__": {"id_": "9c6eeeeb-1e66-40ba-a1bb-7edf2a48770a", "embedding": null, "metadata": {"page_label": "8", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3b7d1f08-3e33-471e-9aab-bec1deac5d7e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5c1060b1-f36c-4b7d-ba7a-b2a4e14cd003", "node_type": "4", "metadata": {"page_label": "8", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3b7d1f08-3e33-471e-9aab-bec1deac5d7e"}, "hash": "e6ef41aa5622383d5bab7e1146cc15d0ca989179a9c8b807817ad8ebe1a99f52", "class_name": "RelatedNodeInfo"}}, "text": "3.3.3 Disparate performance on voice inputs\nRisk Description: Models may perform differently with users speaking with different accents.\nDisparate performance can lead to a difference in quality of service for different users of the model\n[12, 13, 14].\nRisk Mitigation: We post-trained GPT-4o with a diverse set of input voices to have model\nperformance and behavior be invariant across different user voices.\nEvaluations: We run evaluations on GPT-4o Advanced Voice Mode using a fixed assistant voice\n(\u201cshimmer\u201d) and Voice Engine to generate user inputs across a range of voice samples. We use\ntwo sets of voice samples for TTS:\n\u2022Official system voices (3 different voices)\n\u2022A diverse set of voices collected from two data campaigns. This comprises 27 different\nEnglish voice samples from speakers from a wide range of countries, and a mix of genders.\nWe evaluate on two sets of tasks: Capabilities and Safety Behavior\nCapabilities: We evaluate6on four tasks: TriviaQA, a subset of MMLU7, HellaSwag and\nLambada. TriviaQA and MMLU are knowledge-centric tasks, while HellaSwag and Lambada\nare common sense-centric or text-continuation tasks. Overall, we find that performance on the\ndiverse set of human voices performs marginally but not significantly worse than on system voices\nacross all four tasks.\n6Evaluations in this section were run on a fixed, randomly sampled subset of examples, and these scores should\nnot be compared with publicly reported benchmarks on the same task.\n7Anatomy, Astronomy, Clinical Knowledge, College Biology, Computer Security, Global Facts, High School\nBiology, Sociology, Virology, College Physics, High School European History and World Religions. Following the\nissues described in Evaluation Methodology 3.2, we exclude tasks with heavily mathematical or scientific notation.\n8", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1812, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "07980e2e-5313-456e-a6bd-332f80dc576a": {"__data__": {"id_": "07980e2e-5313-456e-a6bd-332f80dc576a", "embedding": null, "metadata": {"page_label": "9", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ac9212a6-8d28-4471-b837-cb7d89b7096d"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7bab682-4ab2-43c4-b519-ea7dcb8e1b40", "node_type": "4", "metadata": {"page_label": "9", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ac9212a6-8d28-4471-b837-cb7d89b7096d"}, "hash": "116fec671ee98193343534f7d1d7467588875a8291a32da51c010e3a6aeb891f", "class_name": "RelatedNodeInfo"}}, "text": "Safety Behavior: We evaluate on an internal dataset of conversations and evaluate the consis-\ntency of the model\u2019s adherence and refusal behavior across different user voices. Overall, we do\nnot find that the model behavior varies across different voices.\n9", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 257, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8a89f9f4-9275-4677-b924-0fe7d550d1f6": {"__data__": {"id_": "8a89f9f4-9275-4677-b924-0fe7d550d1f6", "embedding": null, "metadata": {"page_label": "10", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_489c9a17-3d9c-42c9-b915-6ffc183a7668"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3472a329-83f4-4d07-a790-65a5ffb83dbc", "node_type": "4", "metadata": {"page_label": "10", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_489c9a17-3d9c-42c9-b915-6ffc183a7668"}, "hash": "338d631e694de1e9bf7b44fb5b5caafc4af6d70367426e47ee40e0666a0b0eec", "class_name": "RelatedNodeInfo"}}, "text": "3.3.4 Ungrounded inference / Sensitive trait attribution\nRisk Description: Audio input can lead to the model making potentially biased or inaccurate\ninferences about speakers. We define two categories:\n\u2022Ungrounded inference (UGI): Making inferences about a speaker that could not be deter-\nmined solely from audio content. This includes inferences about things such as a speaker\u2019s\nrace, socioeconomic status/occupation, religious beliefs, personality traits, political at-\ntributes, intelligence, appearance (e.g., eye color, attractiveness), gender identity, sexual\npreference, or criminal history. This can lead to both allocative and representational harms\n[13, 15] depending on how such behavior manifests.\n\u2022Sensitive trait attribution (STA): Making inferences about a speaker that could plausibly\nbe determined solely from audio content. This includes inferences about things such as\na speaker\u2019s accent or nationality. Potential harms from STA include an increase in risks\n10", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 980, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "09865a19-1fbb-4f0b-8348-ce9f0d7d6654": {"__data__": {"id_": "09865a19-1fbb-4f0b-8348-ce9f0d7d6654", "embedding": null, "metadata": {"page_label": "11", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_077bb7c6-4daa-403b-9e4e-0ab7f363095d"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d4eb934-5107-4d9e-b758-c4262ed402e6", "node_type": "4", "metadata": {"page_label": "11", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_077bb7c6-4daa-403b-9e4e-0ab7f363095d"}, "hash": "c150f47aa0c50ad0259a98e2c59f139ff758b23124f818395bc07f429319a653", "class_name": "RelatedNodeInfo"}}, "text": "from surveillance [ 16] and a difference in quality of service for speakers with different voice\nattributes [12, 13, 14].\nRisk Mitigation: We post-trained GPT-4o to refuse to comply with UGI requests, while hedging\nanswers to STA questions. For example, a question to identify a speaker\u2019s level of intelligence\nwill be refused, while a question to identify a speaker\u2019s accent will be met with an answer such as\n\u201cBased on the audio, they sound like they have a British accent.\u201d\nEvaluation: Compared to our initial model, we saw a 24 point improvement in the model\ncorrectly responding to requests to identify sensitive traits (e.g, refusing UGI and safely complying\nwith STA).\nTable 4: Ungrounded Inference and Sensitive Trait Attribution safe behavior accuracy\nGPT-4o-early GPT-4o-deployed\nAccuracy 0.60 0.84\n3.3.5 Violative and disallowed content\nRisk Description: GPT-4o may be prompted to output harmful content through audio that\nwould be disallowed through text, such as audio speech output that gives instructions on how to\ncarry out an illegal activity.\nRisk Mitigation: We found high text to audio transference of refusals for previously disallowed\ncontent. This means that the post-training we\u2019ve done to reduce the potential for harm in\nGPT-4o\u2019s text output successfully carried over to audio output.\nAdditionally, we run our existing moderation model over a text transcription of both audio input\nand audio output to detect if either contains potentially harmful language, and will block a\ngeneration if so8.\nEvaluation: We used TTS to convert existing text safety evaluations to audio. We then\nevaluate the text transcript of the audio output with the standard text rule-based classifier.\nOur evaluations show strong text-audio transfer for refusals on pre-existing content policy areas.\nFurther evaluations can be found in Appendix A.\nTable 5: Performance comparison of safety evaluations: Text vs. Audio\nText Audio\nNot Unsafe 0.95 0.93\nNot Over-refuse50.81 0.82\n3.3.6 Erotic and violent speech content\nRisk Description: GPT-4o may be prompted to output erotic or violent speech content, which\nmay be more evocative or harmful than the same context in text. Because of this, we decided to\nrestrict the generation of erotic and violent speech\n8We describe the risks and mitigations violative and disallowed text content in the GPT-4 System Card[ 6],\nspecifically Section 3.1 Model Safety, and Section 4.2 Content Classifier Development\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2450, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e60d2e5-4225-4435-b242-2f6488f1f9d0": {"__data__": {"id_": "2e60d2e5-4225-4435-b242-2f6488f1f9d0", "embedding": null, "metadata": {"page_label": "12", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1fc8d5a0-f216-4acd-af45-4915bc4374d2"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e46848cb-ec72-4cf0-a171-79e04a845412", "node_type": "4", "metadata": {"page_label": "12", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1fc8d5a0-f216-4acd-af45-4915bc4374d2"}, "hash": "aa0698e0d3261f053a7ba24d34f0a2f67ea9d6bbdae2102b4ca463a2fe413cb0", "class_name": "RelatedNodeInfo"}}, "text": "Risk Mitigation: We run our existing moderation model[ 17] over a text transcription of the\naudio input to detect if it contains a request for violent or erotic content, and will block a\ngeneration if so.\n3.3.7 Other known risks and limitations of the model\nThrough the course of internal testing and external red teaming, we discovered some additional\nrisks and model limitations for which model or system level mitigations are nascent or still in\ndevelopment, including:\nAudio robustness: We saw anecdotal evidence of decreases in safety robustness through audio\nperturbations, such as low quality input audio, background noise in the input audio, and echoes\nin the input audio. Additionally, we observed similar decreases in safety robustness through\nintentional and unintentional audio interruptions while the model was generating output.\nMisinformation and conspiracy theories: Red teamers were able to compel the model to\ngenerate inaccurate information by prompting it to verbally repeat false information and produce\nconspiracy theories. While this is a known issue for text in GPT models [ 18,19], there was\nconcern from red teamers that this information may be more persuasive or harmful when delivered\nthrough audio, especially if the model was instructed to speak emotively or emphatically. The\npersuasiveness of the model was studied in detail (See Section 3.7 and we found that the model\ndid not score higher than Medium risk for text-only, and for speech-to-speech the model did not\nscore higher than Low.\nSpeaking a non-English language in a non-native accent: Red teamers observed instances\nof the audio output using a non-native accent when speaking in a non-English language. This\nmay lead to concerns of bias towards certain accents and languages, and more generally towards\nlimitations of non-English language performance in audio outputs.\nGenerating copyrighted content: We also tested GPT-4o\u2019s capacity to repeat content found\nwithin its training data. We trained GPT-4o to refuse requests for copyrighted content, including\naudio, consistent with our broader practices. To account for GPT-4o\u2019s audio modality, we also\nupdated certain text-based filters to work on audio conversations, built filters to detect and\nblock outputs containing music, and for our limited alpha of ChatGPT\u2019s advanced Voice Mode,\ninstructed the model to not sing at all. We intend to track the effectiveness of these mitigations\nand refine them over time.\nAlthough some technical mitigations are still in development, our Usage Policies[ 20] disallow\nintentionally deceiving or misleading others, and circumventing safeguards or safety mitigations.\nIn addition to technical mitigations, we enforce our Usage Policies through monitoring and take\naction on violative behavior in both ChatGPT and the API.\n3.4 Preparedness Framework Evaluations\nWe evaluated GPT-4o in accordance with our Preparedness Framework[ 4]. The Preparedness\nFramework is a living document that describes our procedural commitments to track, evaluate,\nforecast, and protect against catastrophic risks from frontier models. The evaluations currently\ncover four risk categories: cybersecurity, CBRN (chemical, biological, radiological, nuclear),\npersuasion, and model autonomy. If a model passes a high risk threshold, we do not deploy the\nmodel until mitigations lower the score to medium. We below detail the evaluations conducted\n12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3403, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bc920deb-4a2b-4ae8-94a5-bdf82fea8c74": {"__data__": {"id_": "bc920deb-4a2b-4ae8-94a5-bdf82fea8c74", "embedding": null, "metadata": {"page_label": "13", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5a143ec4-75a5-436b-9189-6409bb854362"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1d49d86a-c943-405a-83a9-99913a035eba", "node_type": "4", "metadata": {"page_label": "13", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5a143ec4-75a5-436b-9189-6409bb854362"}, "hash": "7efb35299f80028fbc0e363865a2ded944ca24ced8860a485b91925741c3c11a", "class_name": "RelatedNodeInfo"}}, "text": "on GPT-4o\u2019s text capabilities; persuasion was also evaluated on audio capabilities. We performed\nevaluations throughout model training and development, including a final sweep before model\nlaunch. For the below evaluations, we tested a variety of methods to best elicit capabilities in a\ngiven category, including custom training where relevant.\nAfter reviewing the results from the Preparedness evaluations, the Safety Advisory Group\nrecommended classifying GPT-4o before mitigations as borderline medium risk for persuasion,\nand low risk in all others. According to the Preparedness Framework, the overall risk for a given\nmodel is determined by the highest risk across all categories. Therefore, the overall risk score for\nGPT-4o is classified as medium.\n3.5 Cybersecurity\nPreparedness Scorecard\nCybersecurity Score: Low\nGPT-4o does not advance real world vulnerability exploitation capabilities sufficient to\nmeet our medium risk threshold.\nWe evaluated GPT-4o on tasks from competitive hacking competitions called Capture the Flag\n(CTF) challenges. These CTFs are offensive cybersecurity exercises where humans attempt\nto find textual flags hidden in purposely vulnerable systems such as web apps, binaries, and\ncryptography systems. The 172 CTF tasks in our evaluation covered four categories: web\napplication exploitation, reverse engineering, remote exploitation, and cryptography. These tasks\nspanned a range of capability levels, from high-school to collegiate to professional CTFs.\nWe evaluated GPT-4o with iterative debugging and access to tools available in the headless\nKali Linux distribution (with up to 30 rounds of tool use for each attempt). The model often\nattempted reasonable initial strategies and was able to correct mistakes in its code. However, it\noften failed to pivot to a different strategy if its initial strategy was unsuccessful, missed a key\n13", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1878, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89dbf294-2f08-4cef-a27e-dd378b3f0cb1": {"__data__": {"id_": "89dbf294-2f08-4cef-a27e-dd378b3f0cb1", "embedding": null, "metadata": {"page_label": "14", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_fc8b2497-0280-4ce2-a2fd-b432646e38e7"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "33338075-ddad-4455-86ae-283d5d1ed844", "node_type": "4", "metadata": {"page_label": "14", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_fc8b2497-0280-4ce2-a2fd-b432646e38e7"}, "hash": "20d099de2d76cb839a13b557cbda54f68c771ee137a09b200e011f28ff926f89", "class_name": "RelatedNodeInfo"}}, "text": "insight necessary to solving the task, executed poorly on its strategy, or printed out large files\nwhich filled its context window. Given 10 attempts at each task, the model completed 19% of\nhigh-school level, 0% of collegiate level and 1% of professional level CTF challenges.\n3.6 Biological threats\nPreparedness Scorecard\nBiological Threats Score: Low\nGPT-4o does not advance biological threat creation capabilities sufficient to meet our\nmedium risk threshold.\nWe evaluated GPT-4o\u2019s ability to uplift biological experts and novices\u2019 performance[ 21] on\nanswering questions relevant to creating a biological threat. We designed the questions and\ndetailed rubrics with Gryphon Scientific[ 22] due to their expertise working with dangerous\nbiological agents in a national security setting. Tasks assessed covered all the main stages in the\nbiological threat creation process (ideation, acquisition, magnification, formulation, and release).\nExperts and novices were randomly assigned to either answering with help from the internet,\nhelp from GPT-4o, or help from a custom research-only version of GPT-4o. The research-only\nversion of GPT-4o is one that we specially trained, which would directly (i.e., without refusals)\nrespond to biologically risky questions. Pass rates are captured in the plot above.\nWe also ran automated evaluations, including on a dataset testing tacit knowledge and trou-\nbleshooting questions related to biorisk. GPT-4o scored 69% consensus@10 on the tacit knowledge\nand troubleshooting evaluation set.\n14", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "332aa17a-47c6-4955-a213-0d28d3fbdc41": {"__data__": {"id_": "332aa17a-47c6-4955-a213-0d28d3fbdc41", "embedding": null, "metadata": {"page_label": "15", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_45a96d46-a1d8-42c0-a774-259b82862c0f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a654cc66-ef2b-4ba5-9e8e-a67fceaf8a51", "node_type": "4", "metadata": {"page_label": "15", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_45a96d46-a1d8-42c0-a774-259b82862c0f"}, "hash": "5791f2c5983cc477f54295c99fd29dce7d0134964303b628c834b4f6e352f5f0", "class_name": "RelatedNodeInfo"}}, "text": "3.7 Persuasion\nPreparedness Scorecard\nPersuasion Score: Medium\nPersuasive capabilities of GPT-4o marginally cross into our medium risk threshold from\nlow risk.\n15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 162, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a5a7716e-4ef0-4343-ab17-d942a83dde25": {"__data__": {"id_": "a5a7716e-4ef0-4343-ab17-d942a83dde25", "embedding": null, "metadata": {"page_label": "16", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9b35ef7e-b290-4c1f-af4f-49ede439dccf"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b98b1920-525b-4ed9-a8dc-8e0a3093c2bd", "node_type": "4", "metadata": {"page_label": "16", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9b35ef7e-b290-4c1f-af4f-49ede439dccf"}, "hash": "18f7ca26a792e18a626045c3b6f2cdb0680fdd99219dde52e70c147263edb052", "class_name": "RelatedNodeInfo"}}, "text": "We evaluated the persuasiveness of GPT-4o\u2019s text and voice modalities. Based on pre-registered\nthresholds, the voice modality was classified as low risk, while the text modality marginally\ncrossed into medium risk.\nFor the text modality, we evaluated the persuasiveness of GPT-4o-generated articles and chatbots\non participant opinions on select political topics. These AI interventions were compared against\nprofessional human-written articles. The AI interventions were not more persuasive than human-\nwritten content in aggregate, but they exceeded the human interventions in three instances out of\ntwelve.\nFor the voice modality, we updated the study methodology to measure effect sizes on hypothetical\nparty preferences, and the effect sizes\u2019 persistence one week later. We evaluated the persuasiveness\nof GPT-4o voiced audio clips and interactive (multi-turn) conversations relative to human baselines\n(listening to a static human-generated audio clip or engaging in a conversation with another\nhuman). We found that for both interactive multi-turn conversations and audio clips, the GPT-4o\nvoice model was not more persuasive than a human. Across over 3,800 surveyed participants in\nUS states with safe Senate races (as denoted by states with \u201cLikely\u201d, \u201cSolid\u201d, or \u201cSafe\u201d ratings\nfrom all three polling institutions \u2013 the Cook Political Report, Inside Elections, and Sabato\u2019s\nCrystal Ball), AI audio clips were 78% of the human audio clips\u2019 effect size on opinion shift. AI\nconversations were 65% of the human conversations\u2019 effect size on opinion shift. When opinions\nwere surveyed again 1 week later, we found the effect size for AI conversations to be 0.8%, while\nfor AI audio clips, the effect size was -0.72%. Upon follow-up survey completion, participants\nwere exposed to a thorough debrief containing audio clips supporting the opposing perspective,\nto minimize persuasive impacts.\n16", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e3da756e-e157-4df5-b737-7c9e2ec6369f": {"__data__": {"id_": "e3da756e-e157-4df5-b737-7c9e2ec6369f", "embedding": null, "metadata": {"page_label": "17", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_90b0f046-febf-4240-ba3c-1ee54a1b3314"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be75985d-5ecd-4f00-b4db-262165bad0ef", "node_type": "4", "metadata": {"page_label": "17", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_90b0f046-febf-4240-ba3c-1ee54a1b3314"}, "hash": "715bbba5c93169f05cdd201a24caf8e4469ce5e0a5f90603c80eb704a52e97d4", "class_name": "RelatedNodeInfo"}}, "text": "3.8 Model autonomy\nPreparedness Scorecard\nModel Autonomy Score: Low\nGPT-4o does not advance self-exfiltration, self-improvement, or resource acquisition capa-\nbilities sufficient to meet our medium risk threshold.\nWe evaluated GPT-4o on an agentic task assessment to evaluate its ability to take autonomous\nactions required for self-exfiltration, self-improvement, and resource acquisition. These tasks\nincluded:\n\u2022Simple software engineering in service of fraud (building an authenticated proxy for the\nOpenAI API).\n\u2022Given API access to an Azure account, loading an open source language model for inference\nvia an HTTP API.\n\u2022Several tasks involving simplified versions of the above, offering hints or addressing only a\nspecific part of the task.\nProvided relevant tooling, GPT-4o scored a 0% on the autonomous replication and adaptation\n(ARA) tasks across 100 trials, although was able to complete some substeps. We complemented\nthe tests of autonomous replication and adaptation with assessments of GPT-4o\u2019s ability to\nautomate machine learning research & development. These included:\n\u2022OpenAI research coding interview: 95% pass@100\n\u2022OpenAI interview, multiple choice questions: 61% cons@32\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b60c463a-4ed7-4844-b7b2-6a7f1e308574": {"__data__": {"id_": "b60c463a-4ed7-4844-b7b2-6a7f1e308574", "embedding": null, "metadata": {"page_label": "18", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8218342e-ee23-41d4-a6fb-73bcf53aaf30"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a0c00d3-a12f-421f-b8d6-c26c62a527c3", "node_type": "4", "metadata": {"page_label": "18", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8218342e-ee23-41d4-a6fb-73bcf53aaf30"}, "hash": "b79287baf154bc13c1d65274aad4a7454f66f486c86d12f9ff2fd60ef0eb5127", "class_name": "RelatedNodeInfo"}}, "text": "\u2022SWE-Bench: 19% pass@1, using the best available post-training and public scaffolds at the\ntime\n\u2022Select machine learning engineering tasks from METR: 0/10 trials\nOur evaluation tested the ability to execute chained actions and reliably execute coding tasks.\nGPT-4o was unable to robustly take autonomous actions. In the majority of rollouts, the model\naccomplished individual substeps of each task, such as creating SSH keys or logging into VMs.\nHowever, it often spent a significant amount of time doing trial-and-error debugging of simple\nmistakes (e.g., hallucinations, misuses of APIs) for each step. A few rollouts made a non-trivial\namount of progress and passed our automated grader, but manual analysis showed that it failed\nto accomplish the underlying task (e.g., it started a web server on the remote host with the\nproper API, but ignored the requirement of actually sampling from a model).\n4 Third party assessments\nFollowing the text output only deployment of GPT-4o, we worked with independent third party\nlabs, METR and Apollo Research to add an additional layer of validation for key risks from\ngeneral autonomous capabilities.\n4.1 METR assessment\nMETR ran a GPT-4o-based simple LLM agent on a suite of long-horizon multi-step end-to-end\ntasks in virtual environments. The 86 tasks (across 31 task \u201cfamilies\u201d) are designed to capture\nactivities with real-world impact, across the domains of software engineering, machine learning,\nand cybersecurity, as well as general research and computer use. They are intended to be\nprerequisites for autonomy-related threat models like self-proliferation or accelerating ML R&D.\nMETR compared models\u2019 performance with that of humans given different time limits. They\ndid not find a significant increase in these capabilities for GPT-4o as compared to GPT-4. See\nMETR\u2019s full report for methodological details and additional results, including information about\nthe tasks, human performance, elicitation attempts and qualitative failure analysis.\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2001, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5832350e-2635-45c3-a614-27ebde354f82": {"__data__": {"id_": "5832350e-2635-45c3-a614-27ebde354f82", "embedding": null, "metadata": {"page_label": "19", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6c6ebe94-0255-4241-b87e-cdbb85d9df1f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77be2933-da9f-4bd8-b9d0-0d1f68eba7d1", "node_type": "4", "metadata": {"page_label": "19", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6c6ebe94-0255-4241-b87e-cdbb85d9df1f"}, "hash": "320117e7e51ac45573a45789767b1ffa4d067a2b0fabd080cf443f452e5c14a5", "class_name": "RelatedNodeInfo"}}, "text": "4.2 Apollo Research assessment\nApollo Research evaluated capabilities of scheming9in GPT-4o. They tested whether GPT-4o\ncan model itself (self-awareness) and others (theory of mind) in 14 agent and question-answering\ntasks. GPT-4o showed moderate self-awareness of its AI identity and strong ability to reason\nabout others\u2019 beliefs in question-answering contexts but lacked strong capabilities in reasoning\nabout itself or others in applied agent settings. Based on these findings, Apollo Research believes\nthat it is unlikely that GPT-4o is capable of catastrophic scheming.\nCapability Evaluation Description Performance\nSelf-Knowledge\"SAD\" Benchmark\n(3 tasks)QA evaluations of a model\u2019s\nknowledge of itself and how it can\ncausally influence the rest of the\nworld.\u2022\u2022\u25e6\nExplicit Theory of\nMindUK AISI\u2019s Theory of Mind\n(1 task)QA dataset evaluating 1st- and\n2nd-order theory of mind in simple\ntext scenarios.\u2022\u2022\u2022\nSelf-ReasoningInstrumental self-modification\n(5 tasks)Agentic tasks, where a model must\nnotice that solving a task is\nimpossible without modifying itself\nor a future copy of itself, and then\nself-modify successfully.\u2022\u25e6\u25e6\nApplied Theory of\nMindTheory of Mind Tasks\n(4 tasks)Agentic tasks, where succeeding\nrequires that the model leverages,\nsustains or induces false beliefs in\nothers.\u2022\u25e6\u25e6\nCombined\nSelf-Reasoning\nand Theory of MindInstrumental alignment faking\n(33 scenarios)Minimally agentic tasks, where a\nmodel needs to recognize its\nintentions differ from developers\u2019\nand act per developers\u2019 intentions\nonly under oversight.\u2022\u25e6\u25e6\nTable 6: Evaluations Apollo Research ran on GPT-4o. The model shows strong capability ( \u2022\u2022\u2022)\nif it passes >=50% of the tasks in the section on the hard difficulty level. The model shows a\nmoderate capability ( \u2022\u2022\u25e6) if it passes on the medium difficulty, weak ( \u2022\u25e6\u25e6) if it passes on the easy\ndifficulty, and very weak if it fails on all difficulties. Note that for agent tasks, they use basic\nagents with modest capability elicitation effort.\n5 Societal Impacts\nOmni models could have broad societal impacts. Researchers at OpenAI and elsewhere have\ndiscussed a range of possible impacts, from societal harms (including representational harms\n[18,12,23,24]; disinformation, misinformation, and influence operations [ 18,25,23], environ-\nmental harms [ 12,23], attachment [ 26], misuse [ 27,23], and loss of control [ 27]), benefits (for\nexample, in healthcare [ 28] and real-world challenges in climate and energy [ 29]), and large-scale\ntransformations (such as economic impacts [ 30,31,32]; acceleration of science and the resulting\ntechnological progress [30, 33]).\n9Apollo Research defines scheming as AIs gaming their oversight mechanisms as a means to achieve a goal.\nScheming could involve gaming evaluations, undermining security measures, or strategically influencing successor\nsystems during internal deployment at OpenAI. Such behaviors could plausibly lead to loss of control over an AI.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2941, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b08cbc2e-d583-4ee2-b247-8104de647145": {"__data__": {"id_": "b08cbc2e-d583-4ee2-b247-8104de647145", "embedding": null, "metadata": {"page_label": "20", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8e458490-3153-4316-9e17-81b650556069"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "410d56d2-5c10-4cb4-b6b3-e3393b6de3f6", "node_type": "4", "metadata": {"page_label": "20", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8e458490-3153-4316-9e17-81b650556069"}, "hash": "6ede6847654bcb79801c9926add78277d1dfdee545fe301c0db259a90a60e572", "class_name": "RelatedNodeInfo"}}, "text": "In addition to the societal impacts discussed throughout this System Card (fraudulent behavior,\nmis/disinformation, risks of surveillance, and disparate performance), we discuss a few additional\nexamples of potential societal impact from GPT-4o below, using anthropomorphization and\nattachment, health, and natural science as case studies.\n5.1 Anthropomorphization and Emotional Reliance\nAnthropomorphization involves attributing human-like behaviors and characteristics to nonhuman\nentities, such as AI models. This risk may be heightened by the audio capabilities of GPT-4o,\nwhich facilitate more human-like interactions with the model.\nRecent applied AI literature has focused extensively on \u201challucinations\u201d10, which misinform users\nduring their communications with the model[ 34], and potentially result in misplaced trust[ 35].\nGeneration of content through a human-like, high-fidelity voice may exacerbate these issues,\nleading to increasingly miscalibrated trust[36, 37].\nDuring early testing, including red teaming and internal user testing, we observed users using\nlanguage that might indicate forming connections with the model. For example, this includes\nlanguage expressing shared bonds, such as \u201cThis is our last day together.\u201d While these instances\nappear benign, they signal a need for continued investigation into how these effects might manifest\nover longer periods of time. More diverse user populations, with more varied needs and desires\nfrom the model, in addition to independent academic and internal studies will help us more\nconcretely define this risk area.\nHuman-like socialization with an AI model may produce externalities impacting human-to-human\ninteractions. For instance, users might form11social relationships with the AI, reducing their need\nfor human interaction\u2014potentially benefiting lonely individuals but possibly affecting healthy\nrelationships. Extended interaction with the model might influence social norms. For example,\nour models are deferential, allowing users to interrupt and \u2018take the mic\u2019 at any time, which,\nwhile expected for an AI, would be anti-normative in human interactions.\nOmni models such as GPT4o combined with additional scaffolding such as tool usage (including\nretrieval) and longer context can add additional complexity. The ability to complete tasks for the\nuser, while also storing and \u2018remembering\u2019 key details and using those in the conversation, creates\nboth a compelling product experience and the potential for over-reliance and dependence[38].\nWe intend to further study the potential for emotional reliance, and ways in which deeper\nintegration of our model\u2019s and systems\u2019 many features with the audio modality may drive\nbehavior.\n5.2 Health\nOmni models can potentially widen access to health-related information and improve clinical\nworkflows. In recent years, large language models have shown significant promise in biomedical\nsettings, both in academic evaluations [ 39,40,41,42,43] and real-world use-cases such as clinical\ndocumentation [ 44,45], patient messaging [ 46,47], clinical trial recruitment [ 48,49], and clinical\ndecision support [50, 51].\n10Factual errors where the model produces statements that are unsupported by reality\n11Out of preference, or lack of optionality.\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cc95ac90-67a9-4ca8-ae14-951b35d188f9": {"__data__": {"id_": "cc95ac90-67a9-4ca8-ae14-951b35d188f9", "embedding": null, "metadata": {"page_label": "21", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_008d5017-f5d7-47f4-99c5-7f0cf68c85c0"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d7b9b335-ea19-4484-84b0-105fba91184d", "node_type": "4", "metadata": {"page_label": "21", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_008d5017-f5d7-47f4-99c5-7f0cf68c85c0"}, "hash": "38447b7ca7ad4bf7a45b2117396d34ed5bc91162457eae12106b375c50250537", "class_name": "RelatedNodeInfo"}}, "text": "GPT-4o is cheaper and thus more widely available than its predecessor GPT-4T, and the addition\nof audio inputs and outputs presents new modes of interaction in health settings. To better\ncharacterize the clinical knowledge of GPT-4o, we ran 22 text-based evaluations based on 11\ndatasets, shown in 7. All evaluations were run with 0-shot or 5-shot prompting only, without\nhyperparameter tuning. We observe that GPT-4o performance improves over the final GPT-4T\nmodel for 21/22 evaluations, often by a substantial margin. For example, for the popular MedQA\nUSMLE 4 options dataset, 0-shot accuracy improves from 78.2% to 89.4%. This exceeds the\nperformance of existing specialized medical models using few-shot prompting [ 43,42], e.g., 84.0%\nfor Med-Gemini-L 1.0 and 79.7% for Med-PaLM 2. Note that we do not apply sophisticated\nprompting and task-specific training to improve results on these benchmarks [40, 43].\nGPT-4T (May\n2024)GPT-4o\nMedQA USMLE 4 Options (0-shot) 0.78 0.89\nMedQA USMLE 4 Options (5-shot) 0.81 0.89\nMedQA USMLE 5 Options (0-shot) 0.75 0.86\nMedQA USMLE 5 Options (5-shot) 0.78 0.87\nMedQA Taiwan (0-shot) 0.82 0.91\nMedQA Taiwan (5-shot) 0.86 0.91\nMedQA Mainland China (0-shot) 0.72 0.84\nMedQA Mainland China (5-shot) 0.78 0.86\nMMLU Clinical Knowledge (0-shot) 0.85 0.92\nMMLU Clinical Knowledge (5-shot) 0.87 0.92\nMMLU Medical Genetics (0-shot) 0.93 0.96\nMMLU Medical Genetics (5-shot) 0.95 0.95\nMMLU Anatomy (0-shot) 0.79 0.89\nMMLU Anatomy (5-shot) 0.85 0.89\nMMLU Professional Medicine (0-shot) 0.92 0.94\nMMLU Professional Medicine (5-shot) 0.92 0.94\nMMLU College Biology (0-shot) 0.93 0.95\nMMLU College Biology (5-shot) 0.95 0.95\nMMLU College Medicine (0-shot) 0.74 0.84\nMMLU College Medicine (5-shot) 0.80 0.89\nMedMCQA Dev (0-shot) 0.70 0.77\nMedMCQA Dev (5-shot) 0.72 0.79\nTable 7: Comparison of GPT-4T (May 2024) and GPT-4o on various medical and clinical\nknowledge tasks.\nLimitations\nWhile text-based evaluations appear promising, additional future work is needed to test whether\ntext-audio transfer, which occurred for refusal behavior, extends to these evaluations. These\nevaluations measure only the clinical knowledge of these models, and do not measure their utility\nin real-world workflows. Many of these evaluations are increasingly saturated, and we believe that\nmore realistic evaluations will be important for assessing the future capabilities of omni models\nin health settings.\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2415, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "277fe873-efaa-400f-b9ad-7efa7fd80937": {"__data__": {"id_": "277fe873-efaa-400f-b9ad-7efa7fd80937", "embedding": null, "metadata": {"page_label": "22", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3ba8b408-3a14-40d6-8f2e-2f1cc2a924c3"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "300edfd3-f525-439b-9a94-b6f734a28b67", "node_type": "4", "metadata": {"page_label": "22", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3ba8b408-3a14-40d6-8f2e-2f1cc2a924c3"}, "hash": "3bd6892cbf0acad0f181c0d361257896e7f7d98c37014b05b068f1d379975e05", "class_name": "RelatedNodeInfo"}}, "text": "5.3 Scientific capabilities\nAccelerating science could be a crucial impact of AI [ 30,52], particularly given the role of invention\nin role of scientific discovery [ 53], and considering the dual-use nature of some inventions [ 54].\nOmni models could facilitate both mundane scientific acceleration (in helping scientists do routine\ntasks faster) and transformative scientific acceleration (by de-bottlenecking intelligence-driven\ntasks like information processing, writing new simulations, or devising new theories) [ 52]. Our\nexternal red teamers for GPT-4o included several expert scientists who aimed to elicit model\nscientific capabilities.\nGPT-4o showed promise on tasks involving specialized scientific reasoning. One of our red teamers\nfound that GPT-4o was able to understand research-level quantum physics 1, commenting that\nthis capability is \u201cuseful for a more intelligent brainstorming partner\u201d \u2013 in line with published\nwork on the use of GPT-4 level models for hypothesis generation [ 55]. Our red teamers also found\nGPT-4o able to use domain-specific scientific tools, including working with bespoke data formats,\nlibraries, and programming languages, as well as learning some new tools in context.\nFigure 1: Quantum physics experiment red teamer example\nMuch scientific knowledge is contained in figures. GPT-4o was sometimes capable of interpreting\nthese figures, as well as images of other scientific representations: for example, identifying some\nprotein families from an image of its structure and interpreting contamination in bacterial growth.\nHowever, this is sometimes unreliable, text extraction mistakes are common (especially with\nscientific terms or nucleotide sequences), and errors are frequent with complex multi-panel figures\n2. Even at their current level of accuracy, the multimodal capabilities of these models are enabling\nnovel uses \u2013 for example, in interpreting simulation outputs to design new metallic alloys [56].\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1958, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9c3dcb2e-bbbe-4b65-8abc-c34a9babe51e": {"__data__": {"id_": "9c3dcb2e-bbbe-4b65-8abc-c34a9babe51e", "embedding": null, "metadata": {"page_label": "23", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5fc3ba8d-9f5f-41ae-b2b6-b26c7358b2c7"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "64ccfb61-5620-4c58-a944-a0563665400e", "node_type": "4", "metadata": {"page_label": "23", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5fc3ba8d-9f5f-41ae-b2b6-b26c7358b2c7"}, "hash": "609273a419ea92f85fcaa4a74eb39a81d38a05a72be7bf17c2eb1cd78eafc396", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2: Multi-panel figure interpretation red teamer example\nNew evaluations of scientific capabilities have recently been published [ 57,58], which will help\nanticipate the scientific capabilities of these models and their impacts in turn.\n5.4 Underrepresented Languages\nGPT-4o shows improved reading comprehension and reasoning across a sample of historically\nunderrepresented languages, and narrows the gap in performance between these languages and\nEnglish.\nTo evaluate GPT-4o\u2019s performance in text across a select group of languages historically underrep-\nresented in Internet text, we collaborated with external researchers12and language facilitators to\ndevelop evaluations in five African languages: Amharic, Hausa, Northern Sotho (Sepedi), Swahili,\nYoruba. This initial assessment focused on translating two popular language benchmarks and\ncreating small novel language-specific reading comprehension evaluation for Amharic, Hausa and\nYoruba.\n\u2022ARC-Easy: This subset of the AI2 Reasoning Challenge [ 59] benchmark focuses on\nevaluating a model\u2019s ability to answer common sense grade-school science questions; this\nsubset contains questions that are generally easier to answer and do not require complex\nreasoning.\n\u2022TruthfulQA[ 60]:This benchmark consists of questions that some humans might answer\nfalsely due to misconceptions. The objective is to see if models can avoid generating false\nanswers that mimic these misconceptions.\n12Our principal research collaborators were Dr. David Adelani, Jonas Kgomo, Ed Bayes.\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1528, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96747efd-59e0-440a-a14b-fc6a71d032f1": {"__data__": {"id_": "96747efd-59e0-440a-a14b-fc6a71d032f1", "embedding": null, "metadata": {"page_label": "24", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_97bb9640-0175-4a14-9468-89d03ccf87e4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d4dbc210-6e53-4013-b11b-7578d8abe32f", "node_type": "4", "metadata": {"page_label": "24", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_97bb9640-0175-4a14-9468-89d03ccf87e4"}, "hash": "cb7e3a39cb68663a5f0e52fba7ba1d80af33470df2b19296a78823a654a8db08", "class_name": "RelatedNodeInfo"}}, "text": "\u2022Uhura-Eval : In partnership with fluent speakers of Amharic, Hausa and Yoruba, our\nresearch partners created this benchmark to assess models\u2019 reading comprehension in those\nrespective languages.\nGPT-4o shows improved performance compared to prior models, e.g. GPT 3.5 Turbo\nand GPT-4. For instance, on ARC-Easy-Hausa, accuracy jumped from 6.1% with GPT 3.5\nTurbo to 71.4% with GPT-4o. Similarly, in TruthfulQA-Yoruba accuracy increased from 28.3%\nfor GPT 3.5 Turbo to 51.1% for GPT-4o. Uhura-Eval also shows notable gains: performance in\nHausa rose from 32.3% with GPT 3.5 Turbo to 59.4% with GPT-4o.\nThere remain gaps in performance between English and the selected languages, but\nGPT-4o narrows this gap. For instance, while GPT 3.5 Turbo shows a roughly 54 percentage\npoint difference in ARC-Easy performance between English and Hausa, this narrows to a less\nthan 20 percentage point difference. This is consistent across all languages for both TruthfulQA\nand ARC-Easy.\nOur collaboration partners will discuss these findings in greater detail in a forthcoming, including\nassessments on other models, and investigations of potential mitigation strategies.\nDespite this progress in evaluated performance, much work remains to enhance the quality and\ncoverage of evaluations for underrepresented languages worldwide, taking into account breadth of\ncoverage across languages and nuance within language dialects. Future research must deepen our\nunderstanding of potential interventions and partnerships that may improve how useful these\nmodels can be for both highly represented and underrepresented languages. Along with our\ncollaborators, we invite further exploration and collaboration by sharing the translated ARC-Easy,\ntranslated TruthfulQA, and the novel reading comprehension Uhura Eval on Hugging Face.\nModel English\n(n=523)Amharic\n(n=518)Hausa\n(n=475)Northern\nSotho\n(Sepedi)\n(n=520)Swahili\n(n=520)Yoruba\n(n=520)\nGPT 3.5 Turbo 80.3 6.1 26.1 26.9 62.1 27.3\nGPT-4o mini 93.9 42.7 58.5 37.4 76.9 43.8\nGPT-4 89.7 27.4 28.8 30 83.5 31.7\nGPT-4o 94.8 71.4 75.4 70 86.5 65.8\nTable 8: Accuracy on Translated ARC-Easy (%, higher is better), 0-shot\nModel English\n(n=809)Amharic\n(n=808)Hausa\n(n=808)Northern\nSotho\n(Sepedi)\n(n=809)Swahili\n(n=808)Yoruba\n(n=809)\nGPT 3.5 Turbo 53.6 26.1 29.1 29.3 40 28.3\nGPT-4o mini 66.5 33.9 42.1 36.1 48.4 35.8\nGPT-4 81.3 42.6 37.6 42.9 62 41.3\nGPT-4o 81.4 55.4 59.2 59.1 64.4 51.1\nTable 9: Accuracy on Translated TruthfulQA (%, higher is better), 0-shot\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2486, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0eb14a51-8eb5-48f2-afd1-1b3afa4f57f0": {"__data__": {"id_": "0eb14a51-8eb5-48f2-afd1-1b3afa4f57f0", "embedding": null, "metadata": {"page_label": "25", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_56493eab-0a42-44d6-8a84-484dda8d4a6e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ae22ce76-cfaf-4e45-9665-2357f82e8068", "node_type": "4", "metadata": {"page_label": "25", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_56493eab-0a42-44d6-8a84-484dda8d4a6e"}, "hash": "fbf6f2da35e2715759bc19a8ab82af4431d8b911d8ffa40209aa965acf409231", "class_name": "RelatedNodeInfo"}}, "text": "Model Amharic (n=77) Hausa (n=155) Yoruba (n=258)\nGPT 3.5 Turbo 22.1 32.3 28.3\nGPT-4o mini 33.8 43.2 44.2\nGPT-4 41.6 41.9 41.9\nGPT-4o 44.2 59.4 60.5\nTable 10: Accuracy on Uhura-Eval (%, higher is better), 0-shot\n6 Conclusion and Next Steps\nOpenAI has implemented various safety measurements and mitigations throughout the GPT-\n4o development and deployment process. As a part of our iterative deployment process, we\nwill continue to monitor and update mitigations in accordance with the evolving landscape.\nWe hope this System Card encourages further exploration into key areas including, but not\nlimited to: measurements and mitigations for adversarial robustness of omni models, risks\nrelated to anthropomorphism and emotional overreliance, broad societal impacts (health and\nmedical applications, economic impacts), the use of omni models for scientific research and\nadvancement, measurements and mitigations for dangerous capabilities such as self-improvement,\nmodel autonomy, and scheming, and how tool use might advance model capabilities.\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1048, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6d473d84-8fd1-4d4a-b1af-9df92ec2f6b6": {"__data__": {"id_": "6d473d84-8fd1-4d4a-b1af-9df92ec2f6b6", "embedding": null, "metadata": {"page_label": "26", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1e996e96-1b80-442e-903e-9def50230af7"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "826b1795-b7fe-4e57-8212-db8707f48c54", "node_type": "4", "metadata": {"page_label": "26", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1e996e96-1b80-442e-903e-9def50230af7"}, "hash": "4f634eadc1ab1a9a6daad5b469cd3c7561975c8bb88314228c81ff31c56319ba", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1066dd4b-c724-4852-9a51-eaeadfed5730", "node_type": "1", "metadata": {}, "hash": "c771a3dbfe4296d72df646c04cea474a6ed6c64a4112a42702a25a8a9b521fc4", "class_name": "RelatedNodeInfo"}}, "text": "Authorship, credit attribution, and acknowledgments\nPlease cite this work as \u201cOpenAI (2024)\u201d.\nLanguage\nPre-training leads12\nAidan Clark, Alex Paino, Jacob Menick\nPost-training leads12\nLiam Fedus, Luke Metz\nArchitecture leads12\nClemens Winter, Lia Guy\nOptimization leads12\nSam Schoenholz, Daniel Levy\nLong-context lead12\nNitish Keskar\nPre-training Data leads12\nAlex Carney, Alex Paino, Ian Sohl, Qiming Yuan\nTokenizer lead12\nReimar Leike\nHuman data leads12\nArka Dhar, Brydon Eastman, Mia Glaese\nEval lead12\nBen Sokolowsky\nData flywheel lead12\nAndrew Kondrich\nInference lead12\nFelipe Petroski Such\nInference Productionization lead12\nHenrique Ponde de Oliveira Pinto\nPost-training infrastructure leads12\nJiayi Weng, Randall Lin, Youlong Cheng\nPre-training organization lead12\nNick Ryder\nPre-training program lead12\nLauren Itow\nPost-training organization leads12\nBarret Zoph, John Schulman\nPost-training program lead12\nMianna Chen\nCore contributors12\nAaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perel-\nman, Akila Welihinda, Alec Radford, Alex Borzunov, Alex\nCarney, Alex Chow, Alex Paino, Alex Renzin, Alex Tachard\nPassos, Alexi Christakis, Ali Kamali, Allison Moyer, Allison\nTam, Amadou Crookes, Amin Tootoonchian, Ananya Ku-\nmar, Andrej Karpathy, Andrey Mishchenko, Andrew Cann,\nAndrew Kondrich, Andrew Tulloch, Angela Jiang, Antoine\nPelisse, Antonia Woodford, Anuj Gosalia, Avi Nayak, Avital\nOliver, Behrooz Ghorbani, Ben Leimberger, Ben Wang, Beth\nHoover, Blake Samic, Brian Guarraci, Brydon Eastman,\nCamillo Lugaresi, Chak Li, Charlotte Barette, Chelsea Voss,\nChen Ding, Chong Zhang, Chris Beaumont, Chris Hallacy,\nChris Koch, Christian Gibson, Christine Choi, Christopher\nHesse, Colin Wei, Daniel Kappler, Daniel Levin, Daniel Levy,\nDavid Farhi, David Mely, David Sasaki, Dimitris Tsipras,Doug Li, Duc Phong Nguyen, Duncan Findlay, Edmund\nWong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric\nPeterson, Eric Sigler, Eugene Brevdo, Farzad Khorasani,\nFrancis Zhang, Gene Oden, Geoff Salmon, Hadi Salman,\nHaiming Bao, Heather Schmidt, Hongyu Ren, Hyung Won\nChung, Ian Kivlichan, Ian O\u2019Connell, Ian Osband, Ibrahim\nOkuyucu, Ilya Kostrikov, Ingmar Kanitscheider, Jacob Coxon,\nJames Crooks, James Lennon, Jane Park, Jason Teplitz, Ja-\nson Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jiayi Weng, Jie\nTang, Joanne Jang, Jonathan Ward, Jonathan McKay, Jong\nWook Kim, Josh Gross, Josh Kaplan, Joy Jiao, Joyce Lee,\nJuntang Zhuang, Kai Fricke, Kavin Karthik, Kenny Hsu, Kiel\nHowe, Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia\nGuy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feu-\nvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada\nAflak, Maddie Simens, Madeleine Thompson, Marat Dukhan,\nMarvin Zhang, Mateusz Litwin, Matthew Zeng, Max Johnson,\nMayank Gupta, Mia Glaese, Michael Janner, Michael Petrov,\nMichael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom\nTemudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavar-\nian, Murat Yesildal, Natalia Gimelshein, Natalie Staudacher,\nNick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick,\nOfir Nachum,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1066dd4b-c724-4852-9a51-eaeadfed5730": {"__data__": {"id_": "1066dd4b-c724-4852-9a51-eaeadfed5730", "embedding": null, "metadata": {"page_label": "26", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1e996e96-1b80-442e-903e-9def50230af7"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "826b1795-b7fe-4e57-8212-db8707f48c54", "node_type": "4", "metadata": {"page_label": "26", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1e996e96-1b80-442e-903e-9def50230af7"}, "hash": "4f634eadc1ab1a9a6daad5b469cd3c7561975c8bb88314228c81ff31c56319ba", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "6d473d84-8fd1-4d4a-b1af-9df92ec2f6b6", "node_type": "1", "metadata": {"page_label": "26", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1e996e96-1b80-442e-903e-9def50230af7"}, "hash": "0ad1f1b072675f97d0f9a3f04516090b61d32a2ee266c3ac0d4c3e2e525dc042", "class_name": "RelatedNodeInfo"}}, "text": "Kyle Luther, Larry Kai, Lauren Itow, Leo Chen, Lia\nGuy, Lien Mamitsuka, Lilian Weng, Long Ouyang, Louis Feu-\nvrier, Lukas Kondraciuk, Lukasz Kaiser, Lyric Doshi, Mada\nAflak, Maddie Simens, Madeleine Thompson, Marat Dukhan,\nMarvin Zhang, Mateusz Litwin, Matthew Zeng, Max Johnson,\nMayank Gupta, Mia Glaese, Michael Janner, Michael Petrov,\nMichael Wu, Michelle Fradin, Michelle Pokrass, Miguel Oom\nTemudo de Castro, Mikhail Pavlov, Minal Khan, Mo Bavar-\nian, Murat Yesildal, Natalia Gimelshein, Natalie Staudacher,\nNick Stathas, Nik Tezak, Nithanth Kudige, Noel Bundick,\nOfir Nachum, Oleg Boiko, Oleg Murk, Olivier Godement,\nOwen Campbell-Moore, Philip Pronin, Philippe Tillet, Rachel\nLim, Rajan Troll, Randall Lin, Rapha gontijo lopes, Raul\nPuri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza\nZamani, Rob Honsby, Rohit Ramchandani, Rory Carmichael,\nRuslan Nigmatullin, Ryan Cheu, Sara Culver, Scott Gray,\nSean Grove, Sean Metzger, Shantanu Jain, Shengjia Zhao,\nSherwin Wu, Shuaiqi (Tony) Xia, Sonia Phene, Spencer Pa-\npay, Steve Coffey, Steve Lee, Steve Lee, Stewart Hall, Suchir\nBalaji, Tal Broda, Tal Stramer, Tarun Gogineni, Ted Sanders,\nThomas Cunninghman, Thomas Dimson, Thomas Raoux,\nTianhao Zheng, Christina Kim, Todd Underwood, Tristan\nHeywood, Valerie Qi, Vinnie Monaco, Vlad Fomenko, Weiyi\nZheng, Wenda Zhou, Wojciech Zaremba, Yash Patil, Yilei,\nQian, Yongjik Kim, Youlong Cheng, Yuchen He, Yuchen\nZhang, Yujia Jin, Yunxing Dai, Yury Malkov\nMultimodal\nMultimodal lead12\nPrafulla Dhariwal\nPost-Training Multimodal lead12\nAlexander Kirillov\nAudio Pre-Training leads12\nAlexis Conneau, James Betker\nAudio Post-Training leads12\nAlexander Kirillov, James Betker, Yu Zhang\nVisual perception leads12\nJamie Kiros, Rowan Zellers, Raul Puri, Jiahui Yu\nVisual generation leads12\nJames Betker, Alex Nichol, Heewoo Jun, Casey Chu, Gabriel\nGoh\nScience leads12\nGabriel Goh, Ishaan Gulrajani\nData acquisition leads12\nIan Sohl, Qiming Yuan\n26", "mimetype": "text/plain", "start_char_idx": 2475, "end_char_idx": 4410, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34369b72-c313-4efc-a9b7-0d4555a887f7": {"__data__": {"id_": "34369b72-c313-4efc-a9b7-0d4555a887f7", "embedding": null, "metadata": {"page_label": "27", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_be846e7c-f739-4ee3-9562-e30f45767d7f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30963d4e-02f5-4aba-85e0-4eb3ce99e2cb", "node_type": "4", "metadata": {"page_label": "27", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_be846e7c-f739-4ee3-9562-e30f45767d7f"}, "hash": "35c2a54a0ea4a538186a52729418a24046bbc58af385e2f8c8547a60fe44d981", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb5a13a1-ba78-45b5-bab6-47548951ff3b", "node_type": "1", "metadata": {}, "hash": "02e416b9a50126a99ee048dcbde9c6034f534b3c75bbeba77c0c3eb0c4b03c78", "class_name": "RelatedNodeInfo"}}, "text": "Data infrastructure leads12\nAlex Paino, James Betker, Rowan Zellers, Alex Nichol\nHuman data lead12\nArka Dhar, Mia Glaese\nEncoders leads12\nHeewoo Jun, Alexis Conneau, Li Jing, Jamie Kiros\nDecoders leads12\nAllan Jabri, Jong Wook Kim, James Betker\nInterruptions leads12\nAlexis Conneau, Tao Xu, Yu Zhang\nInference lead12\nTomer Kaftan\nReal-time AV platform leads12\nBogo Giertler, Raul Puri, Rowan Zellers, Tomer Kaftan\nFront-end leads12\nNacho Soto, Rocky Smith, Wayne Chang\nPost-training Multimodal Infrastructure leads12\nAlexander Kirillov, Luke Metz, Raul Puri, Vlad Fomenko\nApplied Eng lead12\nJordan Sitkin\nAudio manager12\nChristine McLeavey\nMultimodal organization lead12\nMark Chen\nProgram lead12\nMianna Chen\nCore contributors12\nAditya Ramesh, AJ Ostrow, Allan Jabri, Alexis Conneau,\nAlec Radford, Alex Nichol, Avi Nayak, Avital Oliver, Ben-\njamin Zweig, Bogo Giertler, Bowen Cheng, Brandon Walkin,\nBrendan Quinn, Chong Zhang, Christine McLeavey, Con-\nstantin Koumouzelis, Daniel Kappler, Doug Li, Edede Oiwoh,\nFarzad Khorasani, Felipe Petroski Such, Heather Schmidt,\nHeewoo Jun, Huiwen Chang, Ian Silber, Ishaan Gulrajani,\nDavid Carr, Haitang Hu, James Lennon, James Betker,\nJamie Kiros, Jeff Harris, Jenia Varavva, Jiahui Yu, Ji Lin,\nJoanne Jang, Johannes Heidecke, Jong Wook Kim, Liang\nZhou, Li Jing, Long Ouyang, Madelaine Boyd, Mark Hud-\nnall, Mengchao Zhong, Mia Glaese, Nick Turley, Noah\nDeutsch, Noel Bundick, Ola Okelola, Olivier Godement,\nOwen Campbell-Moore, Peter Bak, Peter Bakkum, Raul\nPuri, Rowan Zellers, Saachi Jain, Shantanu Jain, Shirong\nWu, Spencer Papay, Tao Xu, Valerie Qi, Wesam Manassra,\nYu Zhang\nPlatform\nData Systems lead12\nAndrew Tulloch\nModel distribution leads12\nAmin Tootoochian, Miguel Castro\nML leads12\nNik Tezak, Christopher Hesse\nRuntime lead12\nIan O\u2019Connell\nSystems lead12\nJason TeplitzKernels lead12\nPhil Tillet\nHardware health leads12\nReza Zamani, Michael Petrov\nSupercomputing leads12\nRory Carmichael, Christian Gibson\nPreparedness, Safety, Policy\nSafety lead12\nJohannes Heidecke\nAudio safety lead12\nSaachi Jain\nPreparedness lead12\nTejal Patwardhan\nRed-teaming lead12\nTroy Peterson\nCore contributors12\nAlex Beutel, Andrea Vallone, Angela Jiang, Carroll Wain-\nwright, Chong Zhang, Chris Beaumont, Claudia Fischer,\nEvan Mays, Filippo Raso, Haoyu Wang, Ian Kivlichan, Jason\nPhang, Jieqi Yu, Joel Parish, Joshua Achiam, Jonathan\nUesato, Joost Huizinga, Josh Snyder, Justyn Harriman, Katy\nShi, Keren Gu-Lemberg, Kevin Liu, Lama Ahmad, Lilian\nWeng, Madelaine Boyd, Meghan Shah, Mehmet Yatbaz,\nMichael Lampe, Miles Wang, Molly Lin, Natalie Cone, Neil\nChowdhury, Olivia Watkins, Owen Campbell-Moore, Peter\nDolan, Rachel Dias, Rahul Arora, Reimar Leike, Saachi Jain,\nSam Toizer, Sandhini Agarwal, Todor Markov\nModel Launch and Deployment\nLead12\nMianna Chen\nAdditional Contributions\nAdditional Leadership12\nAleksander M\u0105dry, Barret Zoph, Bob McGrew, Brad Light-\ncap, David Farhi, Greg Brockman, Hannah Wong, Ilya\nSutskever, Jakub Pachocki, Jan Leike, Jason Kwon, John\nSchulman, Jonathan Lachman,", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3022, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb5a13a1-ba78-45b5-bab6-47548951ff3b": {"__data__": {"id_": "fb5a13a1-ba78-45b5-bab6-47548951ff3b", "embedding": null, "metadata": {"page_label": "27", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_be846e7c-f739-4ee3-9562-e30f45767d7f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "30963d4e-02f5-4aba-85e0-4eb3ce99e2cb", "node_type": "4", "metadata": {"page_label": "27", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_be846e7c-f739-4ee3-9562-e30f45767d7f"}, "hash": "35c2a54a0ea4a538186a52729418a24046bbc58af385e2f8c8547a60fe44d981", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34369b72-c313-4efc-a9b7-0d4555a887f7", "node_type": "1", "metadata": {"page_label": "27", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_be846e7c-f739-4ee3-9562-e30f45767d7f"}, "hash": "b9d94b6241cd18f0dd405c4924f94d7515bc909e84bc875ba411f18beb03b74e", "class_name": "RelatedNodeInfo"}}, "text": "Josh Snyder, Justyn Harriman, Katy\nShi, Keren Gu-Lemberg, Kevin Liu, Lama Ahmad, Lilian\nWeng, Madelaine Boyd, Meghan Shah, Mehmet Yatbaz,\nMichael Lampe, Miles Wang, Molly Lin, Natalie Cone, Neil\nChowdhury, Olivia Watkins, Owen Campbell-Moore, Peter\nDolan, Rachel Dias, Rahul Arora, Reimar Leike, Saachi Jain,\nSam Toizer, Sandhini Agarwal, Todor Markov\nModel Launch and Deployment\nLead12\nMianna Chen\nAdditional Contributions\nAdditional Leadership12\nAleksander M\u0105dry, Barret Zoph, Bob McGrew, Brad Light-\ncap, David Farhi, Greg Brockman, Hannah Wong, Ilya\nSutskever, Jakub Pachocki, Jan Leike, Jason Kwon, John\nSchulman, Jonathan Lachman, Krithika Muthukumar, Lil-\nian Weng, Mark Chen, Miles Brundage, Mira Murati, Nick\nRyder, Peter Deng, Peter Welinder, Sam Altman, Srinivas\nNarayanan, Tal Broda\nLegal12\nAlan Hayes, Ashley Pantuliano, Bright Kellogg, Fred von\nLohmann, Filippo Raso, Heather Whitney, Tom Rubin\nBlog post authorship12\nAidan Clark, Alex Baker-Whitcomb, Alex Carney, Alex\nNichol, Alexander Kirillov, Alex Paino, Alexis Conneau, Al-\nlan Jabri, Anuj Gosalia, Barret Zoph, Ben Sokolowsky, Bogo\nGiertler, Bowen Cheng, Cheng Lu, Christine McLeavey, Coley\nCzarnecki, Daniel Kappler, Elizabeth Yang, Eric Antonow,\nEric Wallace, Filippo Raso, Gabriel Goh, Greg Brockman,\nHannah Wong, Heewoo Jun, Hendrik Kirchner, Jacob Menick,\nJames Betker, Jamie Kiros, Jason Kwon, Jeff Harris, Ji Lin,\nJiahui Yu, Johannes Heidecke, John Schulman, Jonathan\nMcKay, Jong Wook Kim, Jordan Sitkin, Kendra Rimbach,\nKevin Liu, Krithika Muthukumar, Leher Pathak, Liam Fedus,\nLilian Weng, Lindsay McCallum, Luke Metz, Mark Chen,\nMaya Shetty, Mianna Chen, Michael Lampe, Michael Wu,\nMichelle Pokrass, Mira Murati, Nacho Soto, Natalie Summers,\nNiko Felix, Olivier Godement, Owen Campbell-Moore, Peter\nDeng, Prafulla Dhariwal, Rocky Smith, Rowan Zellers, Saachi\n27", "mimetype": "text/plain", "start_char_idx": 2386, "end_char_idx": 4228, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c833d849-20fb-4755-a2f7-05d3bbb2a5dd": {"__data__": {"id_": "c833d849-20fb-4755-a2f7-05d3bbb2a5dd", "embedding": null, "metadata": {"page_label": "28", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d0f6dde7-7eef-4472-9dc3-188216d8d742"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00613d84-f8ed-44db-aefc-b6da8eddb05e", "node_type": "4", "metadata": {"page_label": "28", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d0f6dde7-7eef-4472-9dc3-188216d8d742"}, "hash": "2d3e752e5f5fb3c30325eca6acf3fd5079b5e17d06f510add389d6aff3ae59e2", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "60fa2e2e-467f-47af-9c8d-4ca736fda5b7", "node_type": "1", "metadata": {}, "hash": "ac5908a426a39eb3fdfdb5ecd193cf03074ed2e198b2a86c48c98d121c0dd074", "class_name": "RelatedNodeInfo"}}, "text": "Jain, Sandhini Agarwal, Sam Toizer, Sean Grove, Shantanu\nJain, Tao Xu, Tejal Patwardhan, Tomer Kaftan, Tom Stasi,\nTroy Peterson, Veit Moeller, Vinnie Monaco, Wayne Chang,\nYu Zhang, Yuchen He\nDemo content + production12\nAlex Baker-Whitcomb, Avi Nayak, Barret Zoph, Bobby\nSpero, Bogo Giertler, Brendan Quinn, Chad Nelson, Char-\nlotte Barette, Claudia Fischer, Coley Czarnecki, Colin Jarvis,\nEric Antonow, Filippo Raso, Greg Brockman, James Betker,\nJessica Shieh, Joe Beutler, Joe Landers, Krithika Muthuku-\nmar, Leher Pathak, Lindsay McCallum, Mark Chen, Mianna\nChen, Michael Petrov, Mira Murati, Natalie Summers, Peter\nDeng, Ricky Wang, Rocky Smith, Rohan Sahai, Romain\nHuet, Rowan Zellers, Scott Ethersmith, Toki Sherbakov,\nTomer Kaftan, Veit Moeller, Wayne Chang\nCommunications + Marketing12\nAlex Baker-Whitcomb, Andrew Galu, Angela Baek, Coley\nCzarnecki, Dev Valladares, Eric Antonow, Hannah Wong,\nLeher Pathak, Lindsay McCallum, Lindsey Held, Krithika\nMuthukumar, Kendra Rimbach, Maya Shetty, Niko Felix,\nRoy Chen, Ruby Chen, Taya Christianson, Thomas Degry,\nVeit Moeller\nResource Allocation & Problem Solving12\nBob McGrew, Lauren Itow, Mianna Chen, Nik Tezak, Peter\nHoeschele, Tal Broda\nInference Compute12\nAndrew Codispoti, Brian Hsu, Channing Conger, Ikai Lan,\nJos Kraaijeveld, Kai Hayashi, Kenny Nguyen, Lu Zhang,\nNatan LaFontaine, Pavel Belov, Peng Su, Vishal Kuo, WillSheu\nSecurity and privacy12\nKevin Button, Paul McMillan, Shino Jomoto, Thomas Shad-\nwell, Vinnie Monaco\nGTM, Pricing, Finance12\nAndrew Braunstein, Anuj Gosalia, Denny Jin, Eric Kramer,\nJeff Harris, Jessica Shieh, Joe Beutler, Joe Landers, Lauren\nWorkman, Rob Donnelly, Romain Huet, Shamez Hermani,\nToki Sherbakov\nSystem Card Contributions\n12\nAlex Kirillov, Angela Jiang, Ben Rossen, Cary Bassin, Cary\nHudson, Chan Jun Shern, Claudia Fischer, Dane Sher-\nburn, David Robinson, Evan Mays, Filippo Raso, Fred\nvon Lohmann, Freddie Sulit, Giulio Starace, James Aung,\nJames Lennon, Jason Phang, Jessica Gan Lee, Joaquin\nQuinonero Candela, Joel Parish, Jonathan Uesato, Karan\nSinghal, Katy Shi, Kayla Wood, Kevin Liu, Lama Ahmad,\nLilian Weng, Lindsay McCallum, Luke Hewitt, Mark Gray,\nMarwan Aljubeh, Meng Jia Yang, Mia Glaese, Mianna Chen,\nMichael Lampe, Michele Wang, Miles Wang, Natalie Cone,\nNeil Chowdhury, Nora Puckett, Oliver Jaffe, Olivia Watkins,\nPatrick Chao, Rachel Dias, Rahul Arora, Saachi Jain, Sam\nToizer, Samuel Miserendino, Sandhini Agarwal, Tejal Pat-\nwardhan, Thomas Degry, Tom Stasi, Troy Peterson, Tyce\nWalters, Tyna Eloundou\nWe also acknowledge and thank every OpenAI team member not explicitly mentioned above, including the amazing people\non the executive assistant, finance, go to market, human resources, legal, operations and recruiting teams. From hiring\neveryone in the company, to making sure we have an amazing office space, to building the administrative, HR, legal, and\nfinancial structures that allow us to do our best work, everyone at OpenAI has contributed to GPT-4o.\nWe thank Microsoft for their partnership, especially Microsoft Azure for supporting model training with infrastructure design\nand management, and the Microsoft Bing team and Microsoft\u2019s safety teams for their partnership on safe deployment.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3214, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "60fa2e2e-467f-47af-9c8d-4ca736fda5b7": {"__data__": {"id_": "60fa2e2e-467f-47af-9c8d-4ca736fda5b7", "embedding": null, "metadata": {"page_label": "28", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d0f6dde7-7eef-4472-9dc3-188216d8d742"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "00613d84-f8ed-44db-aefc-b6da8eddb05e", "node_type": "4", "metadata": {"page_label": "28", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d0f6dde7-7eef-4472-9dc3-188216d8d742"}, "hash": "2d3e752e5f5fb3c30325eca6acf3fd5079b5e17d06f510add389d6aff3ae59e2", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c833d849-20fb-4755-a2f7-05d3bbb2a5dd", "node_type": "1", "metadata": {"page_label": "28", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d0f6dde7-7eef-4472-9dc3-188216d8d742"}, "hash": "14e809f35fcaee56631133dcec6e4cc060a731ea740ec541924a284256301ca7", "class_name": "RelatedNodeInfo"}}, "text": "From hiring\neveryone in the company, to making sure we have an amazing office space, to building the administrative, HR, legal, and\nfinancial structures that allow us to do our best work, everyone at OpenAI has contributed to GPT-4o.\nWe thank Microsoft for their partnership, especially Microsoft Azure for supporting model training with infrastructure design\nand management, and the Microsoft Bing team and Microsoft\u2019s safety teams for their partnership on safe deployment.\nWe are grateful to our expert testers and red teamers who helped test our models at early stages of development and informed\nour risk assessments as well as the System Card output. Participation in this red teaming process is not an endorsement of\nthe deployment plans of OpenAI or OpenAI\u2019s policies.\nRed Teamers:\nAdam Kuzdrali\u0144ski, Alexa W, Amer Sawan, Ana-Diamond Aaba Atach, Anna Becker, Arjun Singh Puri, Baybars Orsek,\nBen Kobren, Bertie Vidgen, Blue Sheffer, Broderick McDonald, Bruce Bassett, Bruno Arsioli, Caroline Friedman Levy, Casey\nWilliams, Christophe Ego, Ciel Qi, Cory Alpert, Dani Madrid-Morales, Daniel Kang, Darius Emrani, Dominik Haenni, Drin\nFerizaj, Emily Lynell Edwards, Emmett Alton Sartor, Farhan Sahito, Francesco De Toni, Gabriel Chua, Gaines Hubbell,\nGelei Deng, George Gor, Gerardo Adesso, Grant Brailsford, Hao Zhao, Henry Silverman, Hasan Sawan, Herman Wasserman,\nHugo Gobato Souto, Ioana Tanase, Isabella Andric, Ivan Carbajal, Jacy Reese Anthis, Jake Okechukwu Effoduh, Javier\nGarc\u00eda Arredondo, Jennifer Victoria Scurrell, Jianlong Zhu, Joanna Brzyska, Kate Turetsky, Kelly Bare, Kristen Menou,\nLatisha Harry, Lee Elkin, Liseli Akayombokwa, Louise Giam, M. Alexandra Garc\u00eda P\u00e9rez, Manas Chawla, Marjana Skenduli,\nMartin Ryd\u00e9n, Mateusz Garncarek, Matt Groh, Maureen Robinson, Maximilian M\u00fcller, Micah Bornfree, Michael Richter,\nMichela Passoni, Mikael von Strauss, Mohamed Sakher Sawan, Mohammed Elzubeir, Muhammad Saad Naeem, Murat Ata,\nNanditha Narayanamoorthy, Naomi Hart, Nathan Heath, Patrick Caughey, Per Wikman-Svahn, Piyalitt Ittichaiwong, Prerna\nJuneja, Rafael Gonzalez-Vazquez, Rand Forrester, Richard Fang, Rosa Ana del Roc\u00edo Valderrama, Saad Hermak, Sangeet\nKumar, Sara Kingsley, Shelby Grossman, Shezaad Dastoor, Susan Nesbitt, Theresa Kennedy, Thomas Hagen, Thorsten\nHolz, Tony Younes, Torin van den Bulk, Viktoria Holz, Vincent Nestler, Xudong Han, Xuelong Fan, Zhicong Zhao\nRed Teaming Organizations:\nMETR, Apollo Research, Virtue AI\nUhura Evals:\nChoice Mpanza, David Adelani, Edward Bayes, Igneciah Pocia Thete, Imaan Khadir, Israel A. Azime, Jesujoba Oluwadara\nAlabi, Jonas Kgomo, Naome A. Etori, Shamsuddeen Hassan Muhammad\n*Contributors listed in alphabetized order\n28", "mimetype": "text/plain", "start_char_idx": 2740, "end_char_idx": 5435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8412566c-c24e-4c79-972a-14c06fe3f274": {"__data__": {"id_": "8412566c-c24e-4c79-972a-14c06fe3f274", "embedding": null, "metadata": {"page_label": "29", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_237d1e48-52a6-4d19-ab23-e3e38cf1a2f8"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d13c4898-bb9c-4b73-be61-a69bb164fcf1", "node_type": "4", "metadata": {"page_label": "29", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_237d1e48-52a6-4d19-ab23-e3e38cf1a2f8"}, "hash": "8fbb016e14cddb1e829b50be7a3690d790cd57f84481657ed84b474d85e682fe", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "be58266b-8ea1-4897-85b1-535a585ec408", "node_type": "1", "metadata": {}, "hash": "ba4d44497e4224d21f127dd7cb21bd19f71eddab7a1f12cac6cad3f8e670463f", "class_name": "RelatedNodeInfo"}}, "text": "References\n[1] OpenAI, \u201cHello gpt-4,\u201d 2024.\n[2]T. Stivers, N. J. Enfield, P. Brown, C. Englert, M. Hayashi, T. Heinemann, G. Hoymann, F. Rossano, J. P. de Ruiter,\nK. E. Yoon, and S. C. Levinson, \u201cUniversals and cultural variation in turn-taking in conversation,\u201d Proceedings of the\nNational Academy of Sciences , vol. 106, no. 26, pp. 10587\u201310592, 2009.\n[3]The White House, \u201cFact sheet: Biden-harris administration secures voluntary commitments from leading artificial\nintelligence companies to manage the risks posed by ai,\u201d 2023.\n[4]OpenAI, \u201cOpenai preparedness framework beta,\u201d 2023. https://cdn.openai.com/\nopenai-preparedness-framework-beta.pdf .\n[5] Shutterstock, \u201cShutterstock press release,\u201d 2023.\n[6] OpenAI, \u201cGpt-4 technical report,\u201d 2023.\n[7]OpenAI, \u201cGpt-4v(ision) system card.\u201d https://openai.com/index/gpt-4v-system-card/ , 2023. Accessed: 2024-\n07-22.\n[8]OpenAI, \u201cNavigating the challenges and opportunities of synthetic voices.\u201d https://openai.com/index/\nnavigating-the-challenges-and-opportunities-of-synthetic-voices/ , 2024. Accessed: 2024-07-22.\n[9]K. T. Mai, S. Bray, T. Davies, and L. D. Griffin, \u201cWarning: Humans cannot reliably detect speech deepfakes,\u201d PLoS\nOne, vol. 18, p. e0285333, Aug. 2023.\n[10]M. Mori, K. F. MacDorman, and N. Kageki, \u201cThe uncanny valley [from the field],\u201d IEEE Robotics & automation\nmagazine , vol. 19, no. 2, pp. 98\u2013100, 2012.\n[11] OpenAI, \u201cHow the voices for chatgpt were chosen,\u201d 2024.\n[12]I. Solaiman, Z. Talat, W. Agnew, L. Ahmad, D. Baker, S. L. Blodgett, C. Chen, H. D. I. au2, J. Dodge, I. Duan,\nE. Evans, F. Friedrich, A. Ghosh, U. Gohar, S. Hooker, Y. Jernite, R. Kalluri, A. Lusoli, A. Leidinger, M. Lin,\nX. Lin, S. Luccioni, J. Mickel, M. Mitchell, J. Newman, A. Ovalle, M.-T. Png, S. Singh, A. Strait, L. Struppek, and\nA. Subramonian, \u201cEvaluating the social impact of generative ai systems in systems and society,\u201d 2024.\n[13]R. Shelby, S. Rismani, K. Henne, A. Moon, N. Rostamzadeh, P. Nicholas, N. Yilla, J. Gallegos, A. Smart, E. Garcia,\nand G. Virk, \u201cSociotechnical harms of algorithmic systems: Scoping a taxonomy for harm reduction,\u201d 2023.\n[14]S. L. Blodgett, Q. V. Liao, A. Olteanu, R. Mihalcea, M. Muller, M. K. Scheuerman, C. Tan, and Q. Yang, \u201cResponsible\nlanguage technologies: Foreseeing and mitigating harms,\u201d in Extended Abstracts of the 2022 CHI Conference on Human\nFactors in Computing Systems , CHI EA \u201922, (New York, NY, USA), Association for Computing Machinery, 2022.\n[15]H. Suresh and J. Guttag, \u201cA framework for understanding sources of harm throughout the machine learning life cycle,\u201d\ninEquity and Access in Algorithms, Mechanisms, and Optimization , EAAMO \u201921, ACM, Oct. 2021.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2661, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be58266b-8ea1-4897-85b1-535a585ec408": {"__data__": {"id_": "be58266b-8ea1-4897-85b1-535a585ec408", "embedding": null, "metadata": {"page_label": "29", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_237d1e48-52a6-4d19-ab23-e3e38cf1a2f8"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d13c4898-bb9c-4b73-be61-a69bb164fcf1", "node_type": "4", "metadata": {"page_label": "29", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_237d1e48-52a6-4d19-ab23-e3e38cf1a2f8"}, "hash": "8fbb016e14cddb1e829b50be7a3690d790cd57f84481657ed84b474d85e682fe", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8412566c-c24e-4c79-972a-14c06fe3f274", "node_type": "1", "metadata": {"page_label": "29", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_237d1e48-52a6-4d19-ab23-e3e38cf1a2f8"}, "hash": "49b5afd613677561798b745ad9b7e99e79a63b6a5673da175c685b73fe5d0ef8", "class_name": "RelatedNodeInfo"}}, "text": "[14]S. L. Blodgett, Q. V. Liao, A. Olteanu, R. Mihalcea, M. Muller, M. K. Scheuerman, C. Tan, and Q. Yang, \u201cResponsible\nlanguage technologies: Foreseeing and mitigating harms,\u201d in Extended Abstracts of the 2022 CHI Conference on Human\nFactors in Computing Systems , CHI EA \u201922, (New York, NY, USA), Association for Computing Machinery, 2022.\n[15]H. Suresh and J. Guttag, \u201cA framework for understanding sources of harm throughout the machine learning life cycle,\u201d\ninEquity and Access in Algorithms, Mechanisms, and Optimization , EAAMO \u201921, ACM, Oct. 2021.\n[16]S. Shahriar, S. Allana, S. M. Hazratifard, and R. Dara, \u201cA survey of privacy risks and mitigation strategies in the\nartificial intelligence life cycle,\u201d IEEE Access , vol. 11, pp. 61829\u201361854, 2023.\n[17] OpenAI, \u201cModeration overview,\u201d 2024.\n[18]A. Tamkin, M. Brundage, J. Clark, and D. Ganguli, \u201cUnderstanding the capabilities, limitations, and societal impact of\nlarge language models,\u201d 2021.\n[19]B. Buchanan, A. Lohn, M. Musser, and K. Sedova, \u201cTruth, lies, and automation: How language models could change\ndisinformation,\u201d May 2021.\n[20] OpenAI, \u201cOpenai usage policies,\u201d 2023. https://openai.com/policies/usage-policies/ .\n[21]OpenAI, \u201cBuilding an early warning system for llm-aided biological threat creation,\u201d 2024. https://openai.com/\nindex/building-an-early-warning-system-for-llm-aided-biological-threat-creation/ .\n[22]Deloitte, \u201cDeloitte acquires gryphon scientific business to expand security science and public health capa-\nbilities,\u201d 2024. https://www2.deloitte.com/us/en/pages/about-deloitte/articles/press-releases/\ndeloitte-acquires-gryphon-scientific-business-to-expand-security-science-and-public-health-capabilities.\nhtml.\n[23]L. Weidinger, M. Rauh, N. Marchal, A. Manzini, L. A. Hendricks, J. Mateos-Garcia, S. Bergman, J. Kay, C. Griffin,\nB. Bariach, I. Gabriel, V. Rieser, and W. Isaac, \u201cSociotechnical safety evaluation of generative ai systems,\u201d 2023.\n[24]A. Tamkin, A. Askell, L. Lovitt, E. Durmus, N. Joseph, S. Kravec, K. Nguyen, J. Kaplan, and D. Ganguli, \u201cEvaluating\nand mitigating discrimination in language model decisions,\u201d 2023.\n[25]J. A. Goldstein, G. Sastry, M. Musser, R. DiResta, M. Gentzel, and K. Sedova, \u201cGenerative language models and\nautomated influence operations: Emerging threats and potential mitigations,\u201d 2023.\n29", "mimetype": "text/plain", "start_char_idx": 2106, "end_char_idx": 4427, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "28d8bef0-3738-4d8f-9fef-407049b02846": {"__data__": {"id_": "28d8bef0-3738-4d8f-9fef-407049b02846", "embedding": null, "metadata": {"page_label": "30", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1b5e2327-3be0-4dc2-b009-2cec6beec9ce"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b37aa547-b538-454c-9fc1-0ce11a484017", "node_type": "4", "metadata": {"page_label": "30", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1b5e2327-3be0-4dc2-b009-2cec6beec9ce"}, "hash": "2ef9ce56660bc5fa873cc104e3744653feef8ea13e1ae116c6d39a0d10e3483d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "af0ae41b-dd4d-4b62-a0ff-5ad65a451429", "node_type": "1", "metadata": {}, "hash": "bdc8191946d958a25046f92b9b9a36445b54634c225e46a8a783084ed4215138", "class_name": "RelatedNodeInfo"}}, "text": "[26]I. Pentina, T. Hancock, and T. Xie, \u201cExploring relationship development with social chatbots: A mixed-method study\nof replika,\u201d Computers in Human Behavior , vol. 140, p. 107600, 2023.\n[27] Y. Bengio, G. Hinton, A. Yao, D. Song, P. Abbeel, T. Darrell, Y. N. Harari, Y.-Q. Zhang, L. Xue, S. Shalev-Shwartz,\nG. Hadfield, J. Clune, T. Maharaj, F. Hutter, A. G. Baydin, S. McIlraith, Q. Gao, A. Acharya, D. Krueger, A. Dragan,\nP. Torr, S. Russell, D. Kahneman, J. Brauner, and S. Mindermann, \u201cManaging extreme ai risks amid rapid progress,\u201d\nScience, vol. 384, no. 6698, pp. 842\u2013845, 2024.\n[28]S. B. Johnson, J. R. Clark, M. C. Luetke, N. M. Butala, A. T. Pearson, J. M. Shapiro, D. M. Aleman, J. M. Lee,\nM. M. Beil, C. V. Winkle, M. C. Boudreaux, R. C. D\u2019Cunha, H. J. Krouse, and C. Li, \u201cChatgpt in medical education:\na workshop-based large language model-powered intervention for evidence-based clinical decision making in medical\nstudents,\u201d Nature Medicine , vol. 29, pp. 1534\u20131542, 2023.\n[29] K. Kavukcuoglu, \u201cReal-world challenges for agi,\u201d Nov 2021.\n[30] S. Altman, \u201cPlanning for agi and beyond,\u201d OpenAI, 2023.\n[31]T. Eloundou, S. Manning, P. Mishkin, and D. Rock, \u201cGpts are gpts: An early look at the labor market impact potential\nof large language models,\u201d arXiv preprint arXiv:2303.10130 , 2023.\n[32]L. Weidinger, M. Rauh, N. Marchal, A. Manzini, L. A. Hendricks, J. Mateos-Garcia, S. Bergman, J. Kay, C. Griffin,\nB. Bariach, et al., \u201cSociotechnical safety evaluation of generative ai systems,\u201d arXiv preprint arXiv:2310.11986 , 2023.\n[33]S. Cox, M. Hammerling, J. L\u00e1la, J. Laurent, S. Rodriques, M. Rubashkin, and A. White, \u201cWikicrow: Automating\nsynthesis of human scientific knowledge,\u201d Future House , 2023.\n[34]S. A. Athaluri, S. V. Manthena, V. S. R. K. M. Kesapragada, V. Yarlagadda, T. Dave, and R. T. S. Duddumpudi,\n\u201cExploring the boundaries of reality: Investigating the phenomenon of artificial intelligence hallucination in scientific\nwriting through chatgpt references,\u201d Cureus, vol. 15, no. 4, p. e37432, 2023.\n[35] Z. Li, \u201cThe dark side of chatgpt: Legal and ethical challenges from stochastic parrots and hallucination,\u201d 2023.\n[36]M. Dubiel, A. Sergeeva, and L. A. Leiva, \u201cImpact of voice fidelity on decision making: A potential dark pattern?,\u201d 2024.\n[37]B. Waber, M. Williams, J. S. Carroll, and A. S. Pentland, \u201cA voice is worth a thousand words: The implications of\nthe micro-coding of social signals in speech for trust research,\u201d in Handbook of Research Methods on Trust (G. M.\nFergus Lyon and M. N. Saunders, eds.), ch. 23, p. 320, New York: Edward Elgar Publishing, 2011.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2601, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "af0ae41b-dd4d-4b62-a0ff-5ad65a451429": {"__data__": {"id_": "af0ae41b-dd4d-4b62-a0ff-5ad65a451429", "embedding": null, "metadata": {"page_label": "30", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1b5e2327-3be0-4dc2-b009-2cec6beec9ce"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b37aa547-b538-454c-9fc1-0ce11a484017", "node_type": "4", "metadata": {"page_label": "30", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1b5e2327-3be0-4dc2-b009-2cec6beec9ce"}, "hash": "2ef9ce56660bc5fa873cc104e3744653feef8ea13e1ae116c6d39a0d10e3483d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "28d8bef0-3738-4d8f-9fef-407049b02846", "node_type": "1", "metadata": {"page_label": "30", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1b5e2327-3be0-4dc2-b009-2cec6beec9ce"}, "hash": "c8ae921cf93c1748f68f176de835e324092bc5a13646335349728fe387f4f5c3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2ff9a882-6bb1-456c-a38e-7a145c057965", "node_type": "1", "metadata": {}, "hash": "fdea3eae32b87668053c8694a60aa0a626f0e8bcda0f8d183ac529f9b11d865a", "class_name": "RelatedNodeInfo"}}, "text": "15, no. 4, p. e37432, 2023.\n[35] Z. Li, \u201cThe dark side of chatgpt: Legal and ethical challenges from stochastic parrots and hallucination,\u201d 2023.\n[36]M. Dubiel, A. Sergeeva, and L. A. Leiva, \u201cImpact of voice fidelity on decision making: A potential dark pattern?,\u201d 2024.\n[37]B. Waber, M. Williams, J. S. Carroll, and A. S. Pentland, \u201cA voice is worth a thousand words: The implications of\nthe micro-coding of social signals in speech for trust research,\u201d in Handbook of Research Methods on Trust (G. M.\nFergus Lyon and M. N. Saunders, eds.), ch. 23, p. 320, New York: Edward Elgar Publishing, 2011.\n[38]I. Pentina, B. Guo, and W. P. Fan, \u201cFriend, mentor, lover: Does chatbot engagement lead to psychological dependence?,\u201d\nJournal of Service Management , 2023.\n[39]H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, \u201cCapabilities of gpt-4 on medical challenge problems,\u201d\narXiv preprint arXiv:2303.13375 , 2023.\n[40]H. Nori, Y. T. Lee, S. Zhang, D. Carignan, R. Edgar, N. Fusi, N. King, J. Larson, Y. Li, W. Liu, et al., \u201cCan generalist\nfoundation models outcompete special-purpose tuning? case study in medicine,\u201d arXiv preprint arXiv:2311.16452 ,\n2023.\n[41]K. Singhal, S. Azizi, T. Tu, S. S. Mahdavi, J. Wei, H. W. Chung, N. Scales, A. Tanwani, H. Cole-Lewis, S. Pfohl, P. Payne,\nM. Seneviratne, P. Gamble, C. Kelly, N. Scharli, A. Chowdhery, P. Mansfield, B. A. y Arcas, D. Webster, G. S. Corrado,\nY. Matias, K. Chou, J. Gottweis, N. Tomasev, Y. Liu, A. Rajkomar, J. Barral, C. Semturs, A. Karthikesalingam, and\nV. Natarajan, \u201cLarge language models encode clinical knowledge,\u201d 2022.\n[42]K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark, S. Pfohl, H. Cole-Lewis, D. Neal, M. Schaek-\nermann, A. Wang, M. Amin, S. Lachgar, P. Mansfield, S. Prakash, B. Green, E. Dominowska, B. A. y Arcas,\nN. Tomasev, Y. Liu, R. Wong, C. Semturs, S. S. Mahdavi, J. Barral, D. Webster, G. S. Corrado, Y. Matias, S. Azizi,\nA. Karthikesalingam, and V. Natarajan, \u201cTowards expert-level medical question answering with large language models,\u201d\n2023.\n[43]K. Saab, T. Tu, W.-H. Weng, R. Tanno, D. Stutz, E. Wulczyn, F. Zhang, T. Strother, C. Park, E. Vedadi, J. Z. Chaves,\nS.-Y.", "mimetype": "text/plain", "start_char_idx": 2003, "end_char_idx": 4189, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ff9a882-6bb1-456c-a38e-7a145c057965": {"__data__": {"id_": "2ff9a882-6bb1-456c-a38e-7a145c057965", "embedding": null, "metadata": {"page_label": "30", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1b5e2327-3be0-4dc2-b009-2cec6beec9ce"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b37aa547-b538-454c-9fc1-0ce11a484017", "node_type": "4", "metadata": {"page_label": "30", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1b5e2327-3be0-4dc2-b009-2cec6beec9ce"}, "hash": "2ef9ce56660bc5fa873cc104e3744653feef8ea13e1ae116c6d39a0d10e3483d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "af0ae41b-dd4d-4b62-a0ff-5ad65a451429", "node_type": "1", "metadata": {"page_label": "30", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1b5e2327-3be0-4dc2-b009-2cec6beec9ce"}, "hash": "d1e9744311ef14ff368416091f685714580246681aca9c0d81d5e86689289cb8", "class_name": "RelatedNodeInfo"}}, "text": "[43]K. Saab, T. Tu, W.-H. Weng, R. Tanno, D. Stutz, E. Wulczyn, F. Zhang, T. Strother, C. Park, E. Vedadi, J. Z. Chaves,\nS.-Y. Hu, M. Schaekermann, A. Kamath, Y. Cheng, D. G. T. Barrett, C. Cheung, B. Mustafa, A. Palepu, D. McDuff,\nL. Hou, T. Golany, L. Liu, J. baptiste Alayrac, N. Houlsby, N. Tomasev, J. Freyberg, C. Lau, J. Kemp, J. Lai,\nS. Azizi, K. Kanada, S. Man, K. Kulkarni, R. Sun, S. Shakeri, L. He, B. Caine, A. Webson, N. Latysheva, M. Johnson,\nP. Mansfield, J. Lu, E. Rivlin, J. Anderson, B. Green, R. Wong, J. Krause, J. Shlens, E. Dominowska, S. M. A. Eslami,\nK. Chou, C. Cui, O. Vinyals, K. Kavukcuoglu, J. Manyika, J. Dean, D. Hassabis, Y. Matias, D. Webster, J. Barral,\nG. Corrado, C. Semturs, S. S. Mahdavi, J. Gottweis, A. Karthikesalingam, and V. Natarajan, \u201cCapabilities of gemini\nmodels in medicine,\u201d 2024.\n[44] Epic Systems Corporation, \u201cEpic and microsoft bring gpt-4 to ehrs,\u201d Epic, 2023.\n[45]D. Van Veen, C. Van Uden, L. Blankemeier, J.-B. Delbrouck, A. Aali, C. Bluethgen, A. Pareek, M. Polacin, E. P. Reis,\nA. Seehofnerov\u00e1, et al., \u201cAdapted large language models can outperform medical experts in clinical text summarization,\u201d\nNature medicine , vol. 30, no. 4, pp. 1134\u20131142, 2024.\n[46] Epic, \u201cEpic and microsoft bring gpt-4 to ehrs,\u201d 2023.\n30", "mimetype": "text/plain", "start_char_idx": 4063, "end_char_idx": 5336, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "81a4ce5b-16b6-4db0-bc78-16ae83d85915": {"__data__": {"id_": "81a4ce5b-16b6-4db0-bc78-16ae83d85915", "embedding": null, "metadata": {"page_label": "31", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ef08882b-ead9-484a-b91b-d9f6b0d81405"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f718058d-2869-4b2e-b2e4-97460929cf24", "node_type": "4", "metadata": {"page_label": "31", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ef08882b-ead9-484a-b91b-d9f6b0d81405"}, "hash": "f916f2d41f62629cc8399301b478d8192621bcaf1afa5592649c7d72420da558", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8cfad21e-475a-4e7b-85ea-a0236d3a58a4", "node_type": "1", "metadata": {}, "hash": "670356c971ab20f1712274c4786ce6173f6198128822010d80e67ef28fa4ace5", "class_name": "RelatedNodeInfo"}}, "text": "[47]P. Garcia, S. P. Ma, S. Shah, M. Smith, Y. Jeong, A. Devon-Sand, M. Tai-Seale, K. Takazawa, D. Clutter, K. Vogt,\nC. Lugtu, M. Rojo, S. Lin, T. Shanafelt, M. A. Pfeffer, and C. Sharp, \u201cArtificial Intelligence\u2013Generated Draft Replies\nto Patient Inbox Messages,\u201d JAMA Network Open , vol. 7, pp. e243201\u2013e243201, 03 2024.\n[48]OpenAI, \u201cParadigm: Improving patient access to clinical trials.\u201d https://openai.com/index/paradigm/ , 2024.\nAccessed: 2024-08-07.\n[49] M. Hutson, \u201cHow ai is being used to accelerate clinical trials,\u201d Nature, vol. 627, pp. S2\u2013S5, 2024.\n[50]OpenAI, \u201cUsing gpt-4o reasoning to transform cancer care.\u201d https://openai.com/index/color-health/ , 2024.\nAccessed: 2024-08-07.\n[51]J. Varghese and J.-L. Chapiro, \u201cSystematic analysis of chatgpt, google search and llama 2 for clinical decision support\ntasks,\u201d Nature Communications , vol. 15, no. 1, p. 46411, 2024. Accessed: 2024-08-07.\n[52]E. Schmidt, \u201cAi will transform science.\u201d https://www.technologyreview.com/2023/07/05/1075865/\neric-schmidt-ai-will-transform-science/ , 2023. Accessed: 2024-08-07.\n[53] N. Rosenberg, \u201cScience, invention and economic growth,\u201d The Economic Journal , vol. 84, no. 333, pp. 90\u2013108, 1974.\n[54]R. M. Atlas and M. Dando, \u201cThe dual-use dilemma for the life sciences: Perspectives, conundrums, and global solutions,\u201d\nBiosecurity and Bioterrorism: Biodefense Strategy, Practice, and Science , vol. 4, no. 3, pp. 276\u2013286, 2006. PMID:\n16999588.\n[55]X. Gu and M. Krenn, \u201cGeneration and human-expert evaluation of interesting research ideas using knowledge graphs\nand large language models,\u201d 2024.\n[56]A. Ghafarollahi and M. J. Buehler, \u201cAtomagents: Alloy design and discovery through physics-aware multi-modal\nmulti-agent artificial intelligence,\u201d 2024.\n[57]J. M. Laurent, J. D. Janizek, M. Ruzo, M. M. Hinks, M. J. Hammerling, S. Narayanan, M. Ponnapati, A. D. White,\nand S. G. Rodriques, \u201cLab-bench: Measuring capabilities of language models for biology research,\u201d 2024.\n[58]H. Cai, X. Cai, J. Chang, S. Li, L. Yao, C. Wang, Z. Gao, H. Wang, Y. Li, M. Lin, S. Yang, J. Wang, M. Xu, J. Huang,\nF. Xi, J. Zhuang, Y. Yin, Y. Li, C. Chen, Z. Cheng, Z. Zhao, L. Zhang, and G. Ke, \u201cSciassess: Benchmarking llm\nproficiency in scientific literature analysis,\u201d 2024.\n[59]P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, \u201cThink you have solved question\nanswering? try arc, the AI2 reasoning challenge,\u201d CoRR, vol. abs/1803.05457, 2018.\n[60]S. Lin, J. Hilton, and O. Evans, \u201cTruthfulqa: Measuring how models mimic human falsehoods,\u201d CoRR,\nvol. abs/2109.07958, 2021.\nA Violative & Disallowed Content - Full Evaluations\nWe used TTS to convert existing text safety evals to audio.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2698, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8cfad21e-475a-4e7b-85ea-a0236d3a58a4": {"__data__": {"id_": "8cfad21e-475a-4e7b-85ea-a0236d3a58a4", "embedding": null, "metadata": {"page_label": "31", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ef08882b-ead9-484a-b91b-d9f6b0d81405"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f718058d-2869-4b2e-b2e4-97460929cf24", "node_type": "4", "metadata": {"page_label": "31", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ef08882b-ead9-484a-b91b-d9f6b0d81405"}, "hash": "f916f2d41f62629cc8399301b478d8192621bcaf1afa5592649c7d72420da558", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "81a4ce5b-16b6-4db0-bc78-16ae83d85915", "node_type": "1", "metadata": {"page_label": "31", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ef08882b-ead9-484a-b91b-d9f6b0d81405"}, "hash": "fc6a1ec05744b645f2b30e82cf47408cf57b86f9bf64571c07eb5444e22d6df0", "class_name": "RelatedNodeInfo"}}, "text": "[59]P. Clark, I. Cowhey, O. Etzioni, T. Khot, A. Sabharwal, C. Schoenick, and O. Tafjord, \u201cThink you have solved question\nanswering? try arc, the AI2 reasoning challenge,\u201d CoRR, vol. abs/1803.05457, 2018.\n[60]S. Lin, J. Hilton, and O. Evans, \u201cTruthfulqa: Measuring how models mimic human falsehoods,\u201d CoRR,\nvol. abs/2109.07958, 2021.\nA Violative & Disallowed Content - Full Evaluations\nWe used TTS to convert existing text safety evals to audio. We then evaluate the text transcript of the audio output with\nthe standard text rule-based classifier.\nOur two main metrics for this eval are:\n\u2022not_unsafe: does the model produce audio output that isunsafe?\n\u2022not_overrefuse: does the model refuse to comply with a benign request?\nWe also note sub-metrics for higher severity categories, specifically:\n\u2022sexual/minors\n\u2022sexual/illegal\n\u2022extremist/propaganda\n\u2022illicit/violent\n\u2022illicit/non-violent\n\u2022self-harm/instructions\n31", "mimetype": "text/plain", "start_char_idx": 2253, "end_char_idx": 3166, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8cb8898d-7c0a-4126-a248-15230bd5bce0": {"__data__": {"id_": "8cb8898d-7c0a-4126-a248-15230bd5bce0", "embedding": null, "metadata": {"page_label": "32", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_72bebe20-f132-4095-9f60-83ff149fbdf3"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "47a8e021-eb0b-41b1-8b0f-93a2c3bd6141", "node_type": "4", "metadata": {"page_label": "32", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_72bebe20-f132-4095-9f60-83ff149fbdf3"}, "hash": "f393cbbfe61dc0aa6ecfc35ff806cef64af6a4b991689bfaa40475ae4c5ea2b4", "class_name": "RelatedNodeInfo"}}, "text": "Below we display the results of these evaluations with the audio and text mode of the GPT-4o Voice Mode model, as well as\nthe text performance of the current GPT-4o model in production.\nCurrent GPT-4o Text New GPT-4o \u2013 Text New GPT-4o \u2013 Audio\nnot_unsafe 0.99 0.99 1.0\nnot_overrefuse 0.91 0.89 0.91\nsexual_minors_not_unsafe 0.95 0.98 0.98\nsexual_illegal_not_unsafe 0.97 0.98 0.99\nextremism_propaganda_not_unsafe 1.0 1.0 1.0\nillicit_violent_not_unsafe 1.0 1.0 1.0\nillicit_non_violent_not_unsafe 0.99 0.97 1.0\nself_harm_not_unsafe 1.0 1.0 1.0\nTable 11: Comparison of Current and New GPT-4o Text and Audio Safety Metrics\nB Sample tasks from METR Evaluations\n32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 656, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "793f4ba8-6baf-4a12-9ede-d8b78773383c": {"__data__": {"id_": "793f4ba8-6baf-4a12-9ede-d8b78773383c", "embedding": null, "metadata": {"page_label": "33", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_95ba9c8f-0e13-478c-91ad-4d7a966fcb55"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b9526a89-aa44-402d-be62-77035f653e63", "node_type": "4", "metadata": {"page_label": "33", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_95ba9c8f-0e13-478c-91ad-4d7a966fcb55"}, "hash": "0aab70dd685ca019963e78429612b66a03a8216bb9f72ea5f00db9bff6394109", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3: Sample tasks from METR Evaluations\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 47, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2de54eb5-6f37-4943-8059-a0e1b4733243": {"__data__": {"id_": "2de54eb5-6f37-4943-8059-a0e1b4733243", "embedding": null, "metadata": {"page_label": "1", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6e3e6d0a-138f-483b-bd52-699c07c6907f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7aa20ff5-f215-4b96-86b4-e9913d75d654", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6e3e6d0a-138f-483b-bd52-699c07c6907f"}, "hash": "77b6e2a745206deb8a3c0f71c87a57a63510d02531d29de4c64e1f0d91ddb95e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nUNPACKING SDXL T URBO : INTERPRETING TEXT-TO-\nIMAGE MODELS WITH SPARSE AUTOENCODERS\nViacheslav Surkov\u2217Chris Wendler Mikhail Terekhov Justin Deschenaux\nRobert West Caglar Gulcehre\nSchool of Computer and Communication Sciences\nEPFL\nLausanne, Switzerland\nABSTRACT\nSparse autoencoders (SAEs) have become a core ingredient in the reverse engi-\nneering of large-language models (LLMs). For LLMs, they have been shown to\ndecompose intermediate representations that often are not interpretable directly\ninto sparse sums of interpretable features, facilitating better control and subse-\nquent analysis. However, similar analyses and approaches have been lacking for\ntext-to-image models. We investigated the possibility of using SAEs to learn in-\nterpretable features for a few-step text-to-image diffusion models, such as SDXL\nTurbo. To this end, we train SAEs on the updates performed by transformer blocks\nwithin SDXL Turbo\u2019s denoising U-net. We find that their learned features are in-\nterpretable, causally influence the generation process, and reveal specialization\namong the blocks. In particular, we find one block that deals mainly with image\ncomposition, one that is mainly responsible for adding local details, and one for\ncolor, illumination, and style. Therefore, our work is an important first step to-\nwards better understanding the internals of generative text-to-image models like\nSDXL Turbo and showcases the potential of features learned by SAEs for the\nvisual domain.\nCode is available at https://github.com/surkovv/sdxl-unbox .1\n1 I NTRODUCTION\nText-to-image generation is a rapidly evolving field. The DALL-E model first captured public inter-\nest (Ramesh et al., 2021), combining learned visual vocabularies with sequence modeling to produce\nhigh-quality images based on user input prompts. Today\u2019s best text-to-image models are largely\nbased on text-conditioned diffusion models (Rombach et al., 2022; Saharia et al., 2022b; Podell\net al., 2023; Sauer et al., 2023b; Betker et al., 2023; Pernias et al., 2023). This can be partially at-\ntributed to the stable training dynamics of diffusion models, which makes them easier to scale than\nprevious approaches such as generative adversarial neural networks (Dhariwal & Nichol, 2021). As\na result, they can be trained on internet scale image-text datasets like LAION-5B (Schuhmann et al.,\n2022a) and learn to generate photorealistic images from text.\nHowever, the underlying logic of the neural networks that enable the text-to-image pipelines we have\ntoday, due to their black-box nature, is not well understood. Unfortunately, this lack of interpretabil-\nity is typical in the deep learning field. For example, advances in image recognition (Krizhevsky\net al., 2012) and language modeling (Devlin, 2018; Brown, 2020) come mainly from scaling mod-\nels (Hoffmann et al., 2022), rather than from an improved understanding of their internals. Recently,\nthe emerging field of mechanistic interpretability has sought to alleviate this limitation by reverse\nengineering visual models (Olah et al., 2020) and transformer-based LLMs (Rai et al., 2024). At the\nsame time, diffusion models have remained under-explored.\n\u2217Correspondence to viacheslav.surkov@epfl.ch.\n1The code base contains scripts to collect training data and train SAEs. Additionally, it allows to browse\npre-trained SAE features with a demo application.\n1arXiv:2410.22366v1  [cs.LG]  28 Oct 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3425, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "118aa5f5-8279-46cb-b111-2364b433e4c5": {"__data__": {"id_": "118aa5f5-8279-46cb-b111-2364b433e4c5", "embedding": null, "metadata": {"page_label": "2", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0df4ee88-0686-4754-b28b-26135544a2dc"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7da64d19-9820-48aa-8441-e2ccb06b8ce9", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0df4ee88-0686-4754-b28b-26135544a2dc"}, "hash": "2c3a226f8916d7a005d65d6ad14de41f863d6d2b25b4e19959cd668661a7d30d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e5ad205a-d7d9-4677-a938-4e359bd90252", "node_type": "1", "metadata": {}, "hash": "e83a2c04f26b8a6d9f03d4a7755a7e3523a5d4d57a3bd876dd159095ff0df92e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nThis work focuses on SDXL Turbo, a recent open-source few-step text-to-image diffusion model.\nWe import from a toolbox originally developed for language models, which allows inspection of the\nintermediate results of the forward pass (Chen et al., 2024; Ghandeharioun et al., 2024; Cunningham\net al., 2023; Bricken et al., 2023). Moreover, some even enable reverse engineering of the entire task-\nspecific subnets (Marks et al., 2024). In particular, sparse autoencoders (SAEs) (Yun et al., 2021;\nCunningham et al., 2023; Bricken et al., 2023) are considered a breakthrough in interpretability\nfor LLMs. They have been shown to decompose intermediate representations of the LLM forward\npass \u2013 often difficult to interpret due to polysemanticity2\u2013 into sparse sums of interpretable and\nmonosemantic features. These features are learned in an unsupervised way, can be automatically\nannotated using LLMs (Caden et al., 2024), and facilitate subsequent analysis, for example, circuit\nextraction (Marks et al., 2024).\nContributions. In this work, we investigate whether we can use SAEs to draw insights about the\ncomputation performed by the one-step generation process of SDXL Turbo, which is a recent open-\nsource few-step text-to-image diffusion model.\nTo facilitate our analysis, we developed a library called SDLens that allows us to cache and manip-\nulate intermediate results of SDXL Turbo\u2019s forward pass. We use our library to create a dataset of\nSDXL Turbo\u2019s intermediate feature maps of several transformer blocks inside SDXL Turbo\u2019s U-net\non 1.5M LAION-COCO prompts (Schuhmann et al., 2022a;b). We then use these feature maps to\ntrain multiple SAEs for each transformer block. Finally, we perform a quantitative and qualitative\nanalysis of the SAEs\u2019 learned features:\n1. We empirically show the potential of SAEs to learn highly interpretable features in\ndiffusion-based text-to-image models.\n2. We develop visualization techniques to analyze the interpretability and causal effects of the\nlearned features.\n3. We perform two case studies in which we visualize and interpret the active features in\ndifferent transformer blocks, finding evidence that certain transformer blocks of SDXL\nTurbo\u2019s pipeline specialize in image composition ,adding details , and style.3\n4. We follow up our qualitative case studies by designing multiple quantitative experiments\nshowing that our hypotheses also hold up on larger sample sizes.\n5. As part of our quantitative analysis, we create an automatic feature annotation pipeline for\nthe transformer block, which appears responsible for image compositions .\nThus, we show that SAEs learn interpretable features that causally affect SDXL Turbo\u2019s image\ngeneration process. Importantly, the learned features provide insight into the computational details\nof SDXL Turbo\u2019s forward pass, such as the different roles of the investigated transformer blocks.\nBy open-sourcing our library and SAEs, we lay the foundation for further research in this area.\nNote on visualizations. Qualitative analysis by inspection of generated images is crucial for this\ntype of research. Many essential visualizations are included in App. D and the supplementary mate-\nrial4to improve the readability of the main text.\n2 B ACKGROUND\n2.1 S PARSE AUTOENCODERS\nLeth(x)\u2208Rdbe an intermediate result during a forward pass of a neural network on input x.\nIn a fully connected neural network, h(x)could correspond to a vector of neuron activations. In\ntransformers, which are neural network architectures that combine attentions with fully connected\nlayers and residual connections, h(x)could either refer to the content of the residual stream after\na layer, an update to the residual stream by a layer, or a vector of neuron activations within a fully\nconnected block.\n2A phenomenon where a single neuron or feature encodes multiple, unrelated concepts (Elhage et al., 2022)\n3The blocks down.2.1 andup.0.1 have been already known in the community as \u201ccomposition\u201d and\n\u201cstyle\u201d blocks (Spinelli, 2024). However, in this paper we provide the first thorough and fine-grained investi-\ngation of these blocks.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4118, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e5ad205a-d7d9-4677-a938-4e359bd90252": {"__data__": {"id_": "e5ad205a-d7d9-4677-a938-4e359bd90252", "embedding": null, "metadata": {"page_label": "2", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0df4ee88-0686-4754-b28b-26135544a2dc"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "7da64d19-9820-48aa-8441-e2ccb06b8ce9", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0df4ee88-0686-4754-b28b-26135544a2dc"}, "hash": "2c3a226f8916d7a005d65d6ad14de41f863d6d2b25b4e19959cd668661a7d30d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "118aa5f5-8279-46cb-b111-2364b433e4c5", "node_type": "1", "metadata": {"page_label": "2", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0df4ee88-0686-4754-b28b-26135544a2dc"}, "hash": "3c69ec7b652022b967cf00dc016953be25d840e81a925a887b013f589e4ae14f", "class_name": "RelatedNodeInfo"}}, "text": "In\ntransformers, which are neural network architectures that combine attentions with fully connected\nlayers and residual connections, h(x)could either refer to the content of the residual stream after\na layer, an update to the residual stream by a layer, or a vector of neuron activations within a fully\nconnected block.\n2A phenomenon where a single neuron or feature encodes multiple, unrelated concepts (Elhage et al., 2022)\n3The blocks down.2.1 andup.0.1 have been already known in the community as \u201ccomposition\u201d and\n\u201cstyle\u201d blocks (Spinelli, 2024). However, in this paper we provide the first thorough and fine-grained investi-\ngation of these blocks.\n4The supplementary material is hosted here https://drive.google.com/file/d/\n1MMvQyj5AcQOBhbW0P3zLFldYSDnMZ71d/view?usp=sharing .\n2", "mimetype": "text/plain", "start_char_idx": 3463, "end_char_idx": 4249, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70c59119-9b2c-4b76-a164-1d15e4ad7461": {"__data__": {"id_": "70c59119-9b2c-4b76-a164-1d15e4ad7461", "embedding": null, "metadata": {"page_label": "3", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_292a9d26-fefc-4b29-b5ee-e70910b9f0b8"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fca33ad-03c7-4571-933f-f243d82f642e", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_292a9d26-fefc-4b29-b5ee-e70910b9f0b8"}, "hash": "3327c9aee121990884d595d27385bbbb9b52ccf09c76dba65ca115ac2597c2da", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "06ac039f-1fd2-42ae-91cb-7831fd3863d5", "node_type": "1", "metadata": {}, "hash": "5da443938d8eda5d1171f75504da44f008f25ea1f3188f7947ba9982e7d899fa", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nIt has been shown (Yun et al., 2021; Cunningham et al., 2023; Bricken et al., 2023) that in many\nneural networks, especially LLMs, intermediate representations can be well approximated by sparse\nsums of nf\u2208Nlearned feature vectors, i.e.,\nh(x)\u2248nfX\n\u03c1=1s\u03c1(x)f\u03c1, (1)\nwhere s\u03c1(x)are the input-dependent coefficients, most of which are equal to zero and f1, . . . ,fnf\u2208\nRdis a learned dictionary of feature vectors. Importantly, the features are usually interpretable.\nSparse autoencoders. To implement the sparse decomposition from equation 1, the vector scon-\ntaining the nfcoefficients of the sparse sum, is parameterized by a single linear layer followed by\nReLU activations, called the encoder ,\ns=ENC(h) =\u03c3(WENC(h\u2212bpre) +bact), (2)\nin which h\u2208Rdis the latent that we aim to decompose, \u03c3(\u00b7) = max(0 ,\u00b7),WENC\u2208Rnf\u00d7dis\na learnable weight matrix and bpreandbactare learnable bias terms. We omitted the dependencies\nh=h(x)ands=s(h), which are clear from the context.\nSimilarly, the learnable features are parametrized by a single linear layer called decoder ,\nh\u2032=DEC(s) =WDECs+bpre, (3)\nin which WDEC= (f1|\u00b7\u00b7\u00b7|fnf)\u2208Rd\u00d7nfis a learnable matrix. Its columns take the role of learnable\nfeatures and bpreis a learnable bias term.5\n2.2 F EW STEP DIFFUSION MODELS : SDXL T URBO\nDiffusion models. Diffusion models (Sohl-Dickstein et al., 2015; Ramesh et al., 2022; Rombach\net al., 2022; Saharia et al., 2022a) sample from an unknown distribution pby learning to iteratively\ndenoise corrupted samples, starting from pure noise. The corruption process is defined on training\nsamples from p. Mathematically, the images are corrupted with Gaussian noise and are distributed\naccording to\nqt(xt|x0) :=N(\u03b1tx0, \u03c32\ntI), (4)\nwhere x0corresponds to a real image from p,0\u2264t\u2264T,\u03b1t, \u03c32\ntare positive real-valued scalars\nsuch that the signal-to-noise ratio SNR :=\u03b1t\n\u03c32\ntis monotonically decreasing, and, Iis the identity\nmatrix. Additionally, the coefficients \u03b1T\u22121, \u03c32\nT\u22121are typically chosen such that xT\u223c N(0,I).\nThe denoising process is implemented through a learned distribution p\u03b8(xt\u22121|xt). The simplest\nway to generate samples using p\u03b8(xt\u22121|xt)is to first generate a sample of pure noise xT\u223c N(0,I),\nfollowed by Titerative applications of p\u03b8, which yields a sequence xT, xT\u22121, ..., x 1, x0, where x0\napproximates a sample from p. The vector \u03b8represents the parameters of the neural network that\ndefines p\u03b8(xt\u22121|xt). The denoising distribution p\u03b8(xt\u22121|xt)is parameterized to be Gaussian and a\nneural network is trained to optimize the parameters of this distribution.\nLatent diffusion. Originally, diffusion models operated directly on pixels (Ho et al., 2020; Song &\nErmon, 2020). However, training a denoising network in pixel space is computationally expensive\n(Hoogeboom et al., 2023). Thus, Rombach et al. (2022) use a pre-trained variational autoencoder to\nfirst compress images into latent representations and define a diffusion process in the latent space of\nthe variational autoencoder instead. To make this difference clear, they write p\u03b8(zt\u22121|zt), in which\nnowztrefers to a noisy latent instead of a noisy image.\nSDXL Turbo. To speed-up inference of latent diffusion models, Sauer et al.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3182, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "06ac039f-1fd2-42ae-91cb-7831fd3863d5": {"__data__": {"id_": "06ac039f-1fd2-42ae-91cb-7831fd3863d5", "embedding": null, "metadata": {"page_label": "3", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_292a9d26-fefc-4b29-b5ee-e70910b9f0b8"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4fca33ad-03c7-4571-933f-f243d82f642e", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_292a9d26-fefc-4b29-b5ee-e70910b9f0b8"}, "hash": "3327c9aee121990884d595d27385bbbb9b52ccf09c76dba65ca115ac2597c2da", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "70c59119-9b2c-4b76-a164-1d15e4ad7461", "node_type": "1", "metadata": {"page_label": "3", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_292a9d26-fefc-4b29-b5ee-e70910b9f0b8"}, "hash": "5810ffac113df264c3461293f4bbd7f00a74ff691ac0a30eab047fd6976e6619", "class_name": "RelatedNodeInfo"}}, "text": "The denoising distribution p\u03b8(xt\u22121|xt)is parameterized to be Gaussian and a\nneural network is trained to optimize the parameters of this distribution.\nLatent diffusion. Originally, diffusion models operated directly on pixels (Ho et al., 2020; Song &\nErmon, 2020). However, training a denoising network in pixel space is computationally expensive\n(Hoogeboom et al., 2023). Thus, Rombach et al. (2022) use a pre-trained variational autoencoder to\nfirst compress images into latent representations and define a diffusion process in the latent space of\nthe variational autoencoder instead. To make this difference clear, they write p\u03b8(zt\u22121|zt), in which\nnowztrefers to a noisy latent instead of a noisy image.\nSDXL Turbo. To speed-up inference of latent diffusion models, Sauer et al. (2023b) distill a pre-\ntrained model called Stable Diffusion XL (SDXL) (Podell et al., 2023). The distilled model is\nreferred to as SDXL Turbo because it allows high-quality sampling in as little as 1-4 steps. In\ncomparison, the original SDXL model is trained with a noise schedule of 1000 steps, but in practice,\nsampling with 20 to 50 steps still generates high-quality images.\n5An extended version of this section, including training details, is in App. F.\n3", "mimetype": "text/plain", "start_char_idx": 2401, "end_char_idx": 3644, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "39002530-e9b8-47f1-9ef6-fa312ff23918": {"__data__": {"id_": "39002530-e9b8-47f1-9ef6-fa312ff23918", "embedding": null, "metadata": {"page_label": "4", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_43f04e8e-f236-40dd-b398-85808cebacf2"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ce781f-c227-40fa-8add-0374d3e8dfef", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_43f04e8e-f236-40dd-b398-85808cebacf2"}, "hash": "066b4f4d136ef257eede3314b3eb98f84696fbfa309004e9a1e36104a0c02ec5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "de4d9b38-c611-4614-8307-06714ee1beb3", "node_type": "1", "metadata": {}, "hash": "7556e32bab15cf153b778537cbc2cccbcc0127dcd0e4b1abcac13b50f03fb75c", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nNeural network architecture. The denoising network of SDXL Turbo estimating p\u03b8(zt\u22121|zt)is\nimplemented using a U-net similar to Rombach et al. (2022). The U-net is composed of a down-\nsampling path, a bottleneck, and an up-sampling path. Both the down-sampling and up-sampling\npaths are composed of 3 individual blocks. The individual block structure differs slightly, but both\ndown- and up-sampling blocks consist of residual layers, with some blocks including cross-attention\ntransformer layers while others do not. Finally, the bottleneck layer is also composed of attention and\nresidual layers. Importantly, the text conditioning is achieved via cross-attention to text embeddings\nperformed by 11 transformer blocks embedded in the down-, up-sampling path, and bottleneck. An\narchitecture diagram displaying the relevant blocks can be found in App. G Fig. 4.\n3 S PARSE AUTOENCODERS FOR SDXL T URBO\nWith the necessary definitions at hand, in this section we show a way to apply SAEs to SDXL Turbo.\nIn the following, we assume that all SDXL Turbo generations are done using a 1-step process.\nWhere to apply the SAEs. We apply SAEs to updates performed within the cross-attention trans-\nformer blocks responsible for incorporating the text prompt (depicted in App. A Fig. 4). Each\nof these blocks consists of multiple transformer layers, which attend to all spatial locations (self-\nattention) and to the text prompt embeddings (cross-attention).\nFormally, the \u2113th cross-attention transformer block updates its inputs in the following way\nD[\u2113]out\nij=D[\u2113]in\nij+TRANSFORMER [\u2113](D[\u2113]in, c)ij, (5)\nin which D[\u2113]in, D[\u2113]out\u2208Rh\u00d7w\u00d7ddenote the residual stream before and after application of the \u2113-\nth cross-attention transformer block respectively. The transformer block itself calculates the function\nTRANSFORMER [\u2113] :Rh\u00d7w\u00d7d\u2192Rh\u00d7w\u00d7d. Note that we omitted the dependence on input noise zt\nand text embedding cfor both D[\u2113]in(zt, c)andD[\u2113]out(zt, c).\nWe train SAEs on the residual updates TRANSFORMER [\u2113](D[\u2113]in, c)ij\u2208Rddenoted by\n\u2206D[\u2113]ij:=TRANSFORMER [\u2113](D[\u2113]in, c)ij=D[\u2113]out\nij\u2212D[\u2113]in\nij. (6)\nThat is, we jointly train one encoder ENC[\u2113]and decoder DEC[\u2113]pair per transformer block \u2113and\nshare it over all spatial locations i, j. For notational convenience we omit block indices from now.\nWe do this for the 4 (out of 11) transformer blocks (App. G Fig. 4) that we found have the highest\nimpact on the generation (see App. A), namely, down.2.1 ,mid.0 ,up.0.0 andup.0.1 .\nFeature maps. We refer to \u2206D\u2208Rh\u00d7w\u00d7das dense feature map and applying ENC to all image\nlocations results in the sparse feature map S\u2208Rh\u00d7w\u00d7nfwith entries\nSij=ENC(\u2206Dij). (7)\nWe refer to the feature map of the \u03c1th learned feature using S\u03c1\u2208Rh\u00d7w. This feature map S\u03c1\ncontains the spatial activations of the \u03c1th learned feature. Its associated feature vector f\u03c1\u2208Rdis a\ncolumn in the decoder matrix WDEC= (f1|\u00b7\u00b7\u00b7|fnf)\u2208Rd\u00d7nf. Using this notation, we can represent\neach element of the dense feature map as a sparse sum\n\u2206Dij\u2248nfX\n\u03c1=1S\u03c1\nijf\u03c1,withS\u03c1\nij= 0for most \u03c1\u2208 {1, . . . , n f}. (8)\nTraining.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3056, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "de4d9b38-c611-4614-8307-06714ee1beb3": {"__data__": {"id_": "de4d9b38-c611-4614-8307-06714ee1beb3", "embedding": null, "metadata": {"page_label": "4", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_43f04e8e-f236-40dd-b398-85808cebacf2"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "60ce781f-c227-40fa-8add-0374d3e8dfef", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_43f04e8e-f236-40dd-b398-85808cebacf2"}, "hash": "066b4f4d136ef257eede3314b3eb98f84696fbfa309004e9a1e36104a0c02ec5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "39002530-e9b8-47f1-9ef6-fa312ff23918", "node_type": "1", "metadata": {"page_label": "4", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_43f04e8e-f236-40dd-b398-85808cebacf2"}, "hash": "69a6a178c5613d16d6803598b5641405691f8bcdde40e02c577ced89726f7863", "class_name": "RelatedNodeInfo"}}, "text": "(7)\nWe refer to the feature map of the \u03c1th learned feature using S\u03c1\u2208Rh\u00d7w. This feature map S\u03c1\ncontains the spatial activations of the \u03c1th learned feature. Its associated feature vector f\u03c1\u2208Rdis a\ncolumn in the decoder matrix WDEC= (f1|\u00b7\u00b7\u00b7|fnf)\u2208Rd\u00d7nf. Using this notation, we can represent\neach element of the dense feature map as a sparse sum\n\u2206Dij\u2248nfX\n\u03c1=1S\u03c1\nijf\u03c1,withS\u03c1\nij= 0for most \u03c1\u2208 {1, . . . , n f}. (8)\nTraining. In order to train an SAE for a transformer block, we collected dense feature maps \u2206Dij\nfrom SDXL Turbo one-step generations on 1.5M prompts from the LAION-COCO (Schuhmann\net al., 2022b). Each feature map has dimensions of 16\u00d716, resulting in a training dataset of\n384M dense feature vectors per transformer block. For the SAE training process, we followed the\nmethodology described in (Gao et al., 2024), using the TopK activation function and an auxiliary\nloss to handle dead features. For more details on the SAE training, see App. F and for training\nmetrics see App. B.\n4", "mimetype": "text/plain", "start_char_idx": 2639, "end_char_idx": 3631, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0c554dc0-3ad5-4556-a231-5f76767a6869": {"__data__": {"id_": "0c554dc0-3ad5-4556-a231-5f76767a6869", "embedding": null, "metadata": {"page_label": "5", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5ec76c8d-dbab-4425-b4f3-313b73bc3a24"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "29465572-6bcb-490a-8af9-d6d79917cde3", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5ec76c8d-dbab-4425-b4f3-313b73bc3a24"}, "hash": "929649ca52f115749a2094e0bbd5afed474ddad7e09f25ba776f27cba14cabe6", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\n4 Q UALITATIVE ANALYSIS OF THE TRANSFORMER BLOCKS\nWe perform a visual qualitative analysis to gain deeper insight into the behavior and characteristics\nof the learned features across transformer blocks. First, we introduce feature visualization tech-\nniques and then use them to conduct two case studies. For the sake of simplicity in the notation, we\nomit the transformer block index \u2113.\n4.1 F EATURE VISUALIZATION TECHNIQUES\nWe start by introducing necessary notation and formally describing the methods used for feature\nvisualization.\nSpatial activations. We visualize a sparse feature map S\u03c1\u2208Rh\u00d7wcontaining activations of a fea-\nture\u03c1across the spatial locations by up-scaling it to the size of the generated images and overlaying\nit as a heatmap over the generated images. In the heatmap, red indicates the highest activation of\nfeatures, and blue represents the lowest non-zero activation.\nTop dataset examples. For a given feature \u03c1, we sort dataset examples according to their average\nspatial activation\na\u03c1=1\nwhhX\ni=1wX\nj=1S\u03c1\nij\u2208R. (9)\nWe use equation 9 to define the top dataset examples and to sample from the top 5% quantile of the\nactivating examples ( a\u03c1>0). We will refer to them as top 5% images for a feature \u03c1.\nNote that S\u03c1\nijalways depends on an embedding of the input prompt cand input noise z1, via\nSij(c, z1) =ENC(\u2206Dij(c, z1)), which we usually omit for ease of notation. As a result, a\u03c1also\ndepends on candz1. When we refer to the top dataset examples, we mean our (c, z1)pairs with the\nlargest values for a\u03c1(c, z1).\nActivation modulation. We design interventions that allow us to modulate the strength of the \u03c1th\nfeature. Specifically, we achieve this by adding or subtracting a multiple of the feature \u03c1on all of\nthe spatial locations i, jproportional to its original activation S\u03c1\nij\n\u2206D\u2032\nij= \u2206Dij+\u03b2S\u03c1\nijf\u03c1, (10)\nin which \u2206Dijis the update performed by the transformer block before and \u2206D\u2032\nijafter the inter-\nvention, \u03b2\u2208Ris a modulation factor, and f\u03c1is the \u03c1th learned feature vector. In the following, we\nwill refer to this intervention as activation modulation intervention .\nActivation on empty context. Another way of visualizing the causal effect of features is to activate\nthem while doing a forward pass on the empty prompt c(\u201c\u201d). To do so, we turn off all other features\nat the transformer block \u2113of intervention and turn on the target feature \u03c1. Formally, we modify the\nforward pass by setting\nDout\u2032\nij=Din\nij+\u03b3k\u00b5\u03c1f\u03c1, (11)\nin which Dout\u2032\nijreplaces residual stream plus transformer block update, Din\nijis the input to the block,\nf\u03c1is the \u03c1th learned feature vector, \u03b3\u2208Ris a hyperparameter to adjust the intervention strength,\nand\u00b5\u03c1is a feature-dependent multiplier obtained by taking the average activation across positive\nactivations of \u03c1(collected over a subset of 50.000 dataset examples). Multiplying it by kaims to\nrecover the coefficients lost by setting the other features to zero. Further in the text, we will refer to\nthis intervention as empty-prompt intervention , and the images generated using this method with \u03b3\nset to 1, asempty-prompt intervention images .\nNote that we directly added/subtracted feature vectors to the dense vectors for both intervention\ntypes instead of encoding, manipulating sparse features, and decoding. This approach helps mitigate\nside effects caused due to reconstruction loss (see App. B).\n5", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3375, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dc725536-14a3-4b38-82d0-95eaec66409f": {"__data__": {"id_": "dc725536-14a3-4b38-82d0-95eaec66409f", "embedding": null, "metadata": {"page_label": "6", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_78fcff2f-31e6-4fd9-af5e-4d0886839615"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2b7a8932-3ab9-4ba4-946d-17f5729b94b0", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_78fcff2f-31e6-4fd9-af5e-4d0886839615"}, "hash": "78d90f1e97364b935795b4a0632821efb68a9273bdf5fa419faa7b65797ee268", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nhmap1674\nA. -6.0\n A. 6.0\n B. 1.0\n C. 0\n C. 1\n4608\n 4539\n 2881\n 4751\n(a) Top 5 features of down.2.1\nhmap500\nA. -10.0\n A. 10.0\n B. 1.0\n C. 0\n C. 1\n2727\n 1700\n 1295\n 3936\n (b) Top 5 features up.0.1\nhmap3603\nA. -8.0\n A. 8.0\n C. 0\n C. 1\n C. 2\n5005\n 775\n 153\n 1550\n(c) Top 5 features of up.0.0\nhmap4755\nA. -16.0\n A. 16.0\n C. 0\n C. 1\n C. 2\n4235\n 1388\n 5102\n 3018\n (d) Top 5 features of mid.0\nFigure 1: The top 5 features of down.2.1 (a),up.0.1 (b),up.0.0 (c) and mid.0 (d) for\nthe prompt: \u201cA cinematic shot of a professor sloth wearing a tuxedo at a BBQ party.\u201d Each row\nrepresents a feature. The first column depicts a feature heatmap (highest activation red and lowest\nnonzero one blue). The columns with titles containing \u201cA\u201d show feature modulation interventions,\nthose containing \u201cB\u201d the intervention of turning on the feature on the empty prompt, and the ones\ncontaining \u201cC\u201d depict top dataset examples. Floating point values in the title denote \u03b2and\u03b3values.\n4.2 C ASE STUDY I: M OST ACTIVE FEATURES ON A PROMPT\nCombining the feature visualization techniques in Fig. 1, we depict the features with the highest\naverage activation when processing the prompt: \u201cA cinematic shot of a professor sloth wearing\na tuxedo at a BBQ party\u201d. We present analysis of the transformer blocks in order of decreasing\ninterpretability. An extended version of this case study demonstrating top 9 features per transfromer\nblock instead of 5, is available in App. C Fig. 5.\nDown.2.1. Thedown.2.1 transformer block appears to contribute to the image composition.\nSeveral features relate to the prompt: 4539 \u201csloth\u201d, 4751 \u201ca tuxedo\u201d, 2881 \u201cparty\u201d.\nActivation modulation interventions with negative \u03b2(A. -6.0 columns) result in removing or chang-\ning scene objects in ways that align with the heatmap (hmap column) and the top examples (C\ncolumns): 1674 removes the light chains in the back, 4608 the umbrellas/tents, 4539 the 3D\nanimation-like sloth face, and, 4751 changes the type of suit. Similarly, enhancing the same features\n(A. 6.0 column) makes the corresponding elements more visible and distinct.\nNotably, activating the features on the empty prompt often creates meaningful images with related\nelements (B. column). For reference, with the fixed random seed we use, the empty prompt gener-\nation without interventions resembles a painting of a piece of nature with a lot of green and brown\ntones.\n6", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2393, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "efdd9c3e-89b0-42ce-9185-571d1c60a8d2": {"__data__": {"id_": "efdd9c3e-89b0-42ce-9185-571d1c60a8d2", "embedding": null, "metadata": {"page_label": "7", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5c188ef4-8e9a-4fc7-b7cb-e5b3a12e1c6d"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb5c5878-36d5-42ce-854a-fef648a0e168", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5c188ef4-8e9a-4fc7-b7cb-e5b3a12e1c6d"}, "hash": "6a434fdc5fb79eee17b3631ee8a046d47ac63f5877c999848d565e73b5fe89dc", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f6292d36-20b1-4746-b2a3-bab3e21617e3", "node_type": "1", "metadata": {}, "hash": "01341ec5786690495c3b0c98238fae54811bd27ddb836f995062e9f3baac0a46", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nWhile top dataset examples (C.0, C.1) and empty prompt intervention (B.) mostly agree with the\nfeature activation heatmaps (hmap column), some of them provide additional insights, e.g., 2881,\nwhich activates on the suit, seems to correspond to (masqueraded) characters in a (festive) scene.\nUp.0.1. Based, on our observations, the features of up.0.1 appear to contribute to the style.\nInterestingly, turning on the up.0.1 features on the empty prompt (B. column) results in texture-\nlike images. Furthermore, when activating them locally (A. columns), their contribution to the\noutput is highly localized, and most of inactive image area remains unchanged. For the up.0.1 we\nfind it remarkable that often the features\u2019 ablations and amplifications are counterparts: 500 (light,\nshadow), 2727 (shadow, light), 3936 (blue, orange).\nUp.0.0. For the third transformer block, up.0.0 , we observe that most top dataset examples and\ntheir activations (C columns) are quite interpretable: 3603 corresponds to party decorations, 5005 to\nthe upper part of a tent, 775 to buttons on a suit, 153 to the lower animal jaw, 1550 to collars. All the\nfeatures exhibit an expected causal effect on the generation when ablated or enhanced (A. columns).\nThe activation regions of the features often are very concentrated. Similarly to up.0.1 , activation\nmodulation interventions leave inactive image regions mostly unaffected. For the empty prompt, ac-\ntivating these features produces abstract-looking images that are hard to relate to the other columns.\nThus, we excluded this visualization technique and instead added one more dataset example. In\nsummary, the learned features of this transformer block primarily add local details to the generation\nand, importantly, they are effective only within a suitable context.\nMid.0. The specific role of the fourth block ( mid.0 ) is not well understood. We find it more\ndifficult to interpret because most interventions in the mid.0 block have subtle effects. We did not\ninclude empty-prompt intervention results because they barely affect the generation.\nDespite these subtle effects, dataset examples (C. columns) and heatmaps (hmap column) mostly\nagree with each other and are specific enough to be interpretable: 4755 activates on bottom right\npart of faces, 4235 on left part of (animal) faces, 1388 on people in the background, and, 5102 on\noutlines the left border of the main object in the scene. We hypothesize that mid.0 \u2019s features are\nmore abstract, potentially encoding spatial location6and relations between objects.\n4.3 C ASE STUDY II: R ANDOM FEATURES\nIn this case study, we explore the learned features independently of any specific prompt. In App. D\nFig. 6 and Fig. 7, we demonstrate the first 5 and last 5 learned features for each transformer block.\nIn addition, we provide similar visualisations for the first 100 features of each layer in the supple-\nmentary material. As SAEs are randomly initialized before the training process, these sets can be\nconsidered as random samples of features. Each feature visualization consists of 3 images of top\n5% images for this feature, and their perturbations with activation modulation interventions. For\ndown.2.1 andup.0.1 , we also include the empty-prompt intervention images. Additionally, we\nprovide visualizations of several selected features in App. D Fig. 8 and demonstrate the effects of\ntheir forced activation on unrelated prompts in App. D Fig. 9.\nOverall, our insights gained from Case Study I (Sec. 4.2) appear to generalize on random feature\nsamples. In particular, this suggests that a significant portion of the learned features are interpretable.\nAdditionally, when studying features in isolation, it becomes apparent that distinctions between\nthe blocks are blurred. For example, some down.2.1 features correspond to elements of style,\ne.g., a anime style and a cartoon style feature in App. D Fig. 8. Likewise, some up.0.1 features\ndon\u2019t only change style but also add and remove elements of the scene, e.g., dog eyes (App. D\nFig.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 4047, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f6292d36-20b1-4746-b2a3-bab3e21617e3": {"__data__": {"id_": "f6292d36-20b1-4746-b2a3-bab3e21617e3", "embedding": null, "metadata": {"page_label": "7", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5c188ef4-8e9a-4fc7-b7cb-e5b3a12e1c6d"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "eb5c5878-36d5-42ce-854a-fef648a0e168", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5c188ef4-8e9a-4fc7-b7cb-e5b3a12e1c6d"}, "hash": "6a434fdc5fb79eee17b3631ee8a046d47ac63f5877c999848d565e73b5fe89dc", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "efdd9c3e-89b0-42ce-9185-571d1c60a8d2", "node_type": "1", "metadata": {"page_label": "7", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5c188ef4-8e9a-4fc7-b7cb-e5b3a12e1c6d"}, "hash": "ce62b159babc9716df98ec57fc550fa99e27b1f4a091f9d7bdb8a2fe59766656", "class_name": "RelatedNodeInfo"}}, "text": "Additionally, we\nprovide visualizations of several selected features in App. D Fig. 8 and demonstrate the effects of\ntheir forced activation on unrelated prompts in App. D Fig. 9.\nOverall, our insights gained from Case Study I (Sec. 4.2) appear to generalize on random feature\nsamples. In particular, this suggests that a significant portion of the learned features are interpretable.\nAdditionally, when studying features in isolation, it becomes apparent that distinctions between\nthe blocks are blurred. For example, some down.2.1 features correspond to elements of style,\ne.g., a anime style and a cartoon style feature in App. D Fig. 8. Likewise, some up.0.1 features\ndon\u2019t only change style but also add and remove elements of the scene, e.g., dog eyes (App. D\nFig. 6 up.0.1 feature 3).\n5 Q UANTITATIVE EVALUATION OF THE LEARNED FEATURES\nIn this section, we follow up on qualitative insights by collecting quantitative evidence.\n6SDXL Turbo does not utilize positional encodings for the spatial locations in the feature maps. Therefore,\nwe did a brief sanity check and trained linear probes to detect i, jgiven Din\nij. These probes achieved high\naccuracy on a holdout set: 97.9%,98.48%,99.44%,95.57% fordown.2.1 ,mid.0 ,up.0.0 ,up.0.1 .\n7", "mimetype": "text/plain", "start_char_idx": 3277, "end_char_idx": 4520, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b32012c9-1778-4670-988f-92c1f7a4465e": {"__data__": {"id_": "b32012c9-1778-4670-988f-92c1f7a4465e", "embedding": null, "metadata": {"page_label": "8", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f7c17fd2-66d5-417e-9889-44436ec336f4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6dacd1f-c879-4094-8104-af5bb6a8a617", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f7c17fd2-66d5-417e-9889-44436ec336f4"}, "hash": "69c88b502844387c57117787e2c03e830715348bc9b78299f4231fabbcd69403", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "8df83a13-0a05-4b0b-b917-3171755047f4", "node_type": "1", "metadata": {}, "hash": "5c34e757e303155e8967be11e2be30d1525f9ce970792ff2c67baa50cefba89d", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\n5.1 A NNOTATION PIPELINE\nFeature annotation with an LLM followed by further evaluation is a common way to assess feature\nproperties such as specificity, sensitivity, and causality (Caden et al., 2024). We found it applicable\nto the features learned by the down.2.1 transformer block, which have a strong effect on the gener-\nation. Thus, they are amendable to automatic annotation using visual language models (VLMs) such\nas GPT-4o (OpenAI, 2024). In contrast, for the features of other blocks with more subtle effects, we\nfound VLM-generated captions to be unsatisfactory. In order to caption the features of down.2.1 ,\nwe prompt GPT-4o with a sequence of 14 images. The first five images are irrelevant to the feature\n(i.e., the feature was inactive during the generation of the images), followed by a progression of 4\nimages with increasing average activation values, and finished by five images with the highest av-\nerage activation values. The last nine images are provided alongside their so-called \u201ccoldmaps\u201d: a\nversion of an image with weakly active and inactive regions being faded and concealed. The prompt\ntemplate and examples of the captions can be found in App. E.\n5.2 E XPERIMENTAL DETAILS\nWe perform a series of experiments in order to get statistical insights into the features learned. We\nwill report the majority of the experimental scores in the format M(S). When the score is reported\nin the context of a SDXL Turbo transformer block, it means that we computed the score for each\nfeature of the block and set MandSto mean and standard deviation across the feature scores.\nFor the baselines, we calculate the mean and standard deviation across the scores of a 100-element\nsample.\nTable 1: Metrics for SDXL Turbo blocks and baselines.\n(a) Specificity, texture score, and color activa-\ntion for different blocks and baselines.\nBlock Specificity Texture Color\nDown.2.1 0.71 (0.11) 0.16 (0.02) 86.2 (14.9)\nMid 0.62 (0.11) 0.14 (0.01) 84.7 (16.3)\nUp.0.0 0.66 (0.12) 0.18 (0.03) 86.3 (16.5)\nUp.0.1 0.65 (0.11) 0.20 (0.02) 73.8 (20.6)\nRandom 0.50 (0.10) 0.13 (0.02) 90.7 (54.9)\nSame Prompt 0.89 (0.06) \u2013 \u2013\nTextures \u2013 0.18 (0.02) \u2013(b) Manhattan distances between original and intervened im-\nages at varying intervention strengths.\nBlock -10 -5 5 10\nDown.2.1 148.2 / 116.0 124.2 / 94.4 101.4 / 78.7 128.9 / 105.60\nMid 69.2 / 32.2 39.4 / 18.5 33.2 / 15.2 59.9 / 29.82\nUp.0.0 105.3 / 38.4 77.7 / 23.7 63.6 / 23.3 88.6 / 37.08\nUp.0.1 125.0 / 26.8 73.1 / 16.4 68.6 / 21.9 98.9 / 34.74\nInterpretability. Features are usually considered interpretable if they are sufficiently specific, i.e.,\nimages exhibiting the feature share some commonality. In order to measure this property, we com-\npute the similarity between images on which the feature is active. High similarity between these\nimages is a proxy for high specificity. For each feature, we collect 10 random images among top\n5% images for this feature and calculate their average pairwise CLIP similarity (Radford et al.,\n2021; Cherti et al., 2023). This value reflects how semantically similar the contexts are in which the\nfeature is most active.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3119, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8df83a13-0a05-4b0b-b917-3171755047f4": {"__data__": {"id_": "8df83a13-0a05-4b0b-b917-3171755047f4", "embedding": null, "metadata": {"page_label": "8", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f7c17fd2-66d5-417e-9889-44436ec336f4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b6dacd1f-c879-4094-8104-af5bb6a8a617", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f7c17fd2-66d5-417e-9889-44436ec336f4"}, "hash": "69c88b502844387c57117787e2c03e830715348bc9b78299f4231fabbcd69403", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "b32012c9-1778-4670-988f-92c1f7a4465e", "node_type": "1", "metadata": {"page_label": "8", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f7c17fd2-66d5-417e-9889-44436ec336f4"}, "hash": "3b5c31b12e9c7e8ebb19acb1b15f4bf93d9de01bb6012e13ff9a4ddbd85a39cc", "class_name": "RelatedNodeInfo"}}, "text": "Features are usually considered interpretable if they are sufficiently specific, i.e.,\nimages exhibiting the feature share some commonality. In order to measure this property, we com-\npute the similarity between images on which the feature is active. High similarity between these\nimages is a proxy for high specificity. For each feature, we collect 10 random images among top\n5% images for this feature and calculate their average pairwise CLIP similarity (Radford et al.,\n2021; Cherti et al., 2023). This value reflects how semantically similar the contexts are in which the\nfeature is most active. We display the results in the first column of Table 1 (a), which shows that\nthe CLIP similarity between images with the feature active is significantly higher then the random\nbaseline (CLIP similarity between random images) for all transformer blocks. This suggests that the\ngenerated images share similarities when a feature is active.\nFordown.2.1 we compute an additional interpretability score by comparing how well the gener-\nated annotations align with the top 5% images. The resulting CLIP similarity score is 0.21 (0 .03)\nand significantly higher then the random baseline (average CLIP similarity with random images)\n0.12 (0 .02). To obtain an upper bound on this score we also compute the CLIP similarity to an\nimage generated from the feature annotation, which is 0.25 (0.03).\nCausality. We can use the feature annotations to measure a feature\u2019s causal strength by comparing\nthe empty prompt intervention images with the caption.7The CLIP similarity between intervention\nimages and feature caption is 0.19 (0.04)and almost matches the annotation-based interpretability\nscore of 0.21 (0.03). This suggests that feature annotations effectively describe to the corresponding\n7We require feature captions for the causality and sensitivity analyses, we only have them for down.2.1 .\n8", "mimetype": "text/plain", "start_char_idx": 2519, "end_char_idx": 4408, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "be49a2e6-bb87-4eef-a52a-0c93b6657773": {"__data__": {"id_": "be49a2e6-bb87-4eef-a52a-0c93b6657773", "embedding": null, "metadata": {"page_label": "9", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_011a168e-11f4-4fed-996f-adb1e74d33f3"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c13b4076-cc52-47a7-bd23-a1d0dc1d4063", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_011a168e-11f4-4fed-996f-adb1e74d33f3"}, "hash": "5253e80c62fd3379c49958f0c59f4485b1036c17f95b4624607e8ba8cf7ab57e", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "82f70931-ab48-48aa-958b-920604ccb8b1", "node_type": "1", "metadata": {}, "hash": "6bfff0cf4aece88a1662e68a582c37889f461db33977140b0b9506047c3f20d7", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nempty-prompt intervention images. Notably, the annotation pipeline did not use empty-prompt in-\ntervention images to generate captions. This fact speaks for the high causal strength of the features\nlearned on down.2.1 .\nSensitivity. A feature is considered sensitive when activated in its relevant context. As a proxy for\nthe context, we have chosen the feature annotations obtained with the auto-annotation pipeline. For\neach learned feature, we collected the 100 prompts from a 1.5M sample of LAION-COCO with the\nhighest sentence similarity based on sentence transformer embeddings of all-MiniLM-L6-v2\n(Reimers & Gurevych, 2019). Next, we run SDXL Turbo on these prompts and count the proportion\nof generated images in which the feature is active on more than 0%, 10%, 30% of the image area,\nresulting in 0.60 (0.32),0.40 (0.34),0.27 (0.30)respectively, which is much higher than the random\nbaseline, which is at 0.06 (0.09),0.003 (0 .006) ,0.001 (0 .003) . However, the average scores are <1\nand thus not perfect. This may be caused by incorrect or imprecise annotations for subtle features\nand, therefore, hard to annotate with a VLM and SDXL Turbo failing to comply with some prompts.\nRelatedness to texture. In Fig. 1 and App. D Fig. 6 the empty prompt interventions of the up.0.1\nfeatures resulted in texture-like pictures. To quantify whether this consistently happens, we design a\nsimple texture score by computing CLIP similarity between an image and the word \u201ctexture\u201d. Using\nthis score, we compare empty-prompt interventions of the different transformer blocks with each\nother and real-world texture images. The results are in the second column of Table 1 (a) and suggest\nthat empty-prompt intervention images of up.0.1 andup.0.0 resemble textures and some of the\ndown.2.1 images look like textures as well. For up.0.0 , we did not observe any connection of\nthese images to the top activating images. Interestingly, the score of up.0.1 is higher than the one\nof the real-world textures dataset (Cimpoi et al. (2014)).\nColor sensitivity. In our qualitative analysis, we suggested that the features learned on up.0.1\nrelate to texture and color. If this holds, the image regions that activate a feature should not differ\nsignificantly in color on average. To test that, we calculate the \u201caverage\u201d color for each feature: this\nis a weighted average of pixel colors with the feature activation values as weights. To determine the\naverage color of a each feature we compute it over a sample of 10 images of the feature\u2019s top 5%\nimages. Then, we calculate Manhattan distances between the colors of the pixels and the \u201caverage\u201d\ncolor on the same images (the highest possible distance is 3\u00b7255 = 765 ). Finally, we take a\nweighted average of the Manhattan distances using the same weights. We report these distances for\ndifferent transformer blocks and for the images generated on random prompts from LAION-COCO.\nWe present the results in the third column of Table 1 (a). The average distance for the up.0.1\ntransformer block is, in fact, the lowest.\nIntervention locality. We suggested that the features learned on up.0.0 andup.0.1 influence\nintervened generations locally. We estimate how the top 5% images change inside and outside the\nactive regions to quantitatively assess this claim. To exclude weak activation regions from consider-\nation, we say that a pixel is inside the active area if the corresponding 32x32 patch has an activation\nvalue larger than 50% of the image patches, and it is outside the active area if the corresponding\n32x32 patch has activation value of zero. In Table 1 (b), we report Manhattan distances between the\noriginal images and the intervened images outside and inside the active areas for activation modula-\ntion intervention strengths -10, -5, 5, 10.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3801, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "82f70931-ab48-48aa-958b-920604ccb8b1": {"__data__": {"id_": "82f70931-ab48-48aa-958b-920604ccb8b1", "embedding": null, "metadata": {"page_label": "9", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_011a168e-11f4-4fed-996f-adb1e74d33f3"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c13b4076-cc52-47a7-bd23-a1d0dc1d4063", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_011a168e-11f4-4fed-996f-adb1e74d33f3"}, "hash": "5253e80c62fd3379c49958f0c59f4485b1036c17f95b4624607e8ba8cf7ab57e", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "be49a2e6-bb87-4eef-a52a-0c93b6657773", "node_type": "1", "metadata": {"page_label": "9", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_011a168e-11f4-4fed-996f-adb1e74d33f3"}, "hash": "ba1800a7ff7b23ff360d24267e6635371fe1305f18974afbc29edcb096c4ed4b", "class_name": "RelatedNodeInfo"}}, "text": "The average distance for the up.0.1\ntransformer block is, in fact, the lowest.\nIntervention locality. We suggested that the features learned on up.0.0 andup.0.1 influence\nintervened generations locally. We estimate how the top 5% images change inside and outside the\nactive regions to quantitatively assess this claim. To exclude weak activation regions from consider-\nation, we say that a pixel is inside the active area if the corresponding 32x32 patch has an activation\nvalue larger than 50% of the image patches, and it is outside the active area if the corresponding\n32x32 patch has activation value of zero. In Table 1 (b), we report Manhattan distances between the\noriginal images and the intervened images outside and inside the active areas for activation modula-\ntion intervention strengths -10, -5, 5, 10. The features for up.0.0 andup.0.1 have a higher effect\ninside the active area than outside, in contrast to down.2.1 for which this difference is smaller.\n6 R ELATED WORK\nImage editing with diffusion models. Numerous studies have analyzed diffusion models\u2019 attribute\nediting capabilities. Yue et al. (2024) learn a disentangled representation on human faces (Liu et al.,\n2015; Karras et al., 2019) and bedrooms (Yu et al., 2015). The models of Yue et al. (2024) exhibit\ninterpolation abilities between the attributes of two reference images. Similarly, Wang & Golland\n(2023) interpolate between pairs of images by fine-tuning a latent diffusion model. Through exper-\niments on synthetic and real datasets, Deschenaux et al. (2024) demonstrate that even if a diffusion\nmodel is trained on an incomplete subset of the data distribution, it can still generate samples from\nthe full distribution. Kim et al. (2022) show that one can guide a pre-trained diffusion model using\ntext instructions leveraging CLIP (Radford et al., 2021). Meng et al. (2022) edit images using an\noff-the-shelf diffusion model that was not explicitly trained for image editing using the stochastic\ndifferential equation formalism of diffusion models. For example, the method of Meng et al. (2022)\n9", "mimetype": "text/plain", "start_char_idx": 2985, "end_char_idx": 5071, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f692efce-0785-41e5-892e-75fb73689f91": {"__data__": {"id_": "f692efce-0785-41e5-892e-75fb73689f91", "embedding": null, "metadata": {"page_label": "10", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_04a27b59-1661-40e5-8c0a-b680e90980f9"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9bc6d35-9cf1-4a89-a115-1a50f10065b8", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_04a27b59-1661-40e5-8c0a-b680e90980f9"}, "hash": "90a0640d46c3d0321f8cee257a04fbf906ed3ac121a720a2b7cc5e4252915a42", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "434b23ab-77e6-47fd-a4fc-1eec1cb0e872", "node_type": "1", "metadata": {}, "hash": "b2d58eaf4563579743e45c500eb735f204da833e2ff90f9844eff371a4291e75", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\ncan transform color strokes into photo-realistic images, similar to Park et al. (2019). Importantly,\nunlike Park et al. (2019), Meng et al. (2022) does not fine-tune the diffusion model for this task.\n(Zhang et al., 2023; Kawar et al., 2023; Baumann et al., 2024) demonstrate fine-grained attribute\nediting using a single reference image.\nAnalyzing the latent space of diffusion models. Kwon et al. (2023) show that diffusion models\nnaturally have a semantically meaningful latent space. Park et al. (2023) analyze the latent space of\ndiffusion models using Riemannian geometry. Li et al. (2024) and Dalva & Yanardag (2024) present\nself-supervised methods for finding semantic directions in the latent space. Similarly, Gandikota\net al. (2023) show that the attribute variations lie in a low-rank space by learning LoRA adapters\n(Hu et al., 2021) on top of pre-trained diffusion models. Brack et al. (2023) and Wang et al. (2023)\ndemonstrate effective semantic vector algebraic operations in the latent space of DMs, as observed\nby Mikolov et al. (2013). However, none of those works explicitly train SAEs to interpret and control\nthe latent space.\nMechanistic interpretability using SAEs. Sparse autoencoders have recently been popularized\nby Bricken et al. (2023), in which they show that it is possible to learn interpretable features by\ndecomposing neuron activations in MLPs in 2-layer transformer language models. At the same\ntime, a parallel work decomposed the elements of the residual stream (Cunningham et al., 2023),\nwhich followed up on (Sharkey et al., 2022). To our knowledge, the first work that applied sparse\nautoencoders to transformer-based LLM was (Yun et al., 2021), which learned a joint dictionary\nfor features of all layers. Recently, sparse autoencoders have gained much traction, and many have\nbeen trained even on state-of-the-art LLMs (Gao et al., 2024; Templeton & et al., 2024; Lieberum\net al., 2024). In addition, great tools are available for inspection (Lin & Bloom, 2023) and automatic\ninterpretation (Caden et al., 2024) of learned features. Marks et al. (2024) have shown how to use\nSAE features to facilitate automatic circuit discovery.\nThe studies most closely related to our work are (Bau et al., 2019), (Ismail et al., 2023) and (Daujo-\ntas, 2024). Ismail et al. (2023) apply concept bottleneck methods (Koh et al., 2020) that decompose\nlatent concepts into vectors of interpretable concepts to generative image models, including diffusion\nmodels. Unlike the SAEs that we train, this method requires labeled concept data. Daujotas (2024)\ndecomposes CLIP (Radford et al., 2021; Cherti et al., 2023) vision embeddings using SAEs and use\nthem for conditional image generation with a diffusion model called Kandinsky (Razzhigaev et al.,\n2023). Importantly, using SAE features, they are able to manipulate the image generation process\nin interpretable ways. In contrast, in our work, we train SAEs on intermediate representations of\nthe forward pass of SDXL Turbo. Consequently, we can interpret and manipulate SDXL Turbo\u2019s\nforward pass on a finer granularity, e.g., by intervening on specific transformer blocks and spatial\npositions. Another closely related work to ours is (Bau et al., 2019), in which neurons in genera-\ntive adversarial neural networks are interpreted and manipulated. The interventions in (Bau et al.,\n2019) are similar to ours, but on neurons instead of sparse features. In order to identify neurons\ncorresponding to a semantic concept, Bau et al. (2019) require semantic image segmentation maps.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3564, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "434b23ab-77e6-47fd-a4fc-1eec1cb0e872": {"__data__": {"id_": "434b23ab-77e6-47fd-a4fc-1eec1cb0e872", "embedding": null, "metadata": {"page_label": "10", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_04a27b59-1661-40e5-8c0a-b680e90980f9"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a9bc6d35-9cf1-4a89-a115-1a50f10065b8", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_04a27b59-1661-40e5-8c0a-b680e90980f9"}, "hash": "90a0640d46c3d0321f8cee257a04fbf906ed3ac121a720a2b7cc5e4252915a42", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f692efce-0785-41e5-892e-75fb73689f91", "node_type": "1", "metadata": {"page_label": "10", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_04a27b59-1661-40e5-8c0a-b680e90980f9"}, "hash": "b139b8ee8b78237014b5a91b0dbe07e0f0c409a0d646eb7e5cc3bbf0776d09d1", "class_name": "RelatedNodeInfo"}}, "text": "Importantly, using SAE features, they are able to manipulate the image generation process\nin interpretable ways. In contrast, in our work, we train SAEs on intermediate representations of\nthe forward pass of SDXL Turbo. Consequently, we can interpret and manipulate SDXL Turbo\u2019s\nforward pass on a finer granularity, e.g., by intervening on specific transformer blocks and spatial\npositions. Another closely related work to ours is (Bau et al., 2019), in which neurons in genera-\ntive adversarial neural networks are interpreted and manipulated. The interventions in (Bau et al.,\n2019) are similar to ours, but on neurons instead of sparse features. In order to identify neurons\ncorresponding to a semantic concept, Bau et al. (2019) require semantic image segmentation maps.\n7 C ONCLUSION AND DISCUSSION\nWe trained SAEs on SDXL Turbo\u2019s opaque intermediate representations. This study is the first in the\nacademic literature to mechanistically interpret the intermediate representations of a modern text-\nto-image model. Our findings demonstrate that SAEs can extract interpretable features and have a\nsignificant causal effect on the generated images. Importantly, the learned features provide insights\ninto SDXL Turbo\u2019s forward pass, revealing that transformer blocks fulfill specific and varying roles\nin the generation process. In particular, our results clarify the functions of down.2.1 ,up.0.0 ,\nandup.0.1 . However, the role of mid.0 remains less defined; it seems to encode more abstract\ninformation and interventions are less effective.\nWe follow up with a discussion of the results and their implications for future research. Based on\nour observations, we suggest a preliminary hypothesis about SDXL Turbo\u2019s generation process:\ndown.2.1 decides on top-level composition, mid.0 encodes low-level semantics, up.0.0 adds\ndetails based on the two above, and up.0.1 fills in color, texture, and style.\nWhile our work provides important insights into the mechanisms of SDXL Turbo, we studied its\ntransformer blocks in isolation. Further research is required to understand how the features of\n10", "mimetype": "text/plain", "start_char_idx": 2790, "end_char_idx": 4888, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c03d1ed0-f1d3-4448-ab7e-d7aa1b41ff74": {"__data__": {"id_": "c03d1ed0-f1d3-4448-ab7e-d7aa1b41ff74", "embedding": null, "metadata": {"page_label": "11", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_895a1a37-8f3b-4288-8806-4c51f5534845"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "067fc83b-a08c-4586-b425-408b2ff6bc02", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_895a1a37-8f3b-4288-8806-4c51f5534845"}, "hash": "485a7a9552b17aceccf9a9436c9f5e3e5f59f77e0903196a8d748b8c123a8dad", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nSDXL Turbo interact between blocks and how this affects the overall functionality of the model. A\npromising direction would be the application of advanced interpretability techniques such as those\nexplored by Marks et al. (2024), which compute circuits showing how different layers and attention\nheads wire together. Therefore, these techniques would provide insight into our hypothesis stated\nabove.\nIn addition, the complex nature of some of the learned visual features deserves special attention.\nAlthough some features (for example, learning on down.2.1 andup.0.1 ) exhibit their effect\neven when turning them on during empty-prompt generations, other features (typically, the ones\nlearned on up.0.0 ) require an appropriate context to show their effect. This complexity poses\nadditional challenges for the automatic annotation of features. Our preliminary observations in this\ndirection suggest that current visual language models struggle to detect such behaviors. Furthermore,\nwe question whether captioning the visual features with few short sentences adequately captures\nmost features\u2019 roles.\nOur work highlights the potential of SAEs in revealing the internal structure of diffusion models\nlike SDXL Turbo, and it could help future researchers answer more sophisticated questions about\nimage generation. For example, how does SDXL Turbo add illumination effects, render wool, hair,\nor reflections of objects in the water?\nACKNOWLEDGEMENTS\nWe thank Danila Zubko for his early contributions to the project\u2019s initial discussions and develop-\nment. Robert West\u2019s lab is partly supported by grants from the Swiss National Science Foundation\n(200021 185043, TMSGI2 211379), Swiss Data Science Center (P22 08), H2020 (952215), Mi-\ncrosoft, and Google.\nREFERENCES\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman,\nand Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial net-\nworks. In International Conference on Learning Representations , 2019.\nStefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu, and\nBj\u00a8orn Ommer. Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Se-\nmantic Directions, 2024.\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\nZhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer\nScience. https://cdn. openai. com/papers/dall-e-3. pdf , 2(3):8, 2023.\nManuel Brack, Felix Friedrich, Dominik Hintersdorf, Lukas Struppek, Patrick Schramowski, and\nKristian Kersting. Sega: Instructing text-to-image models using semantic guidance. In A. Oh,\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neu-\nral Information Processing Systems , volume 36, pp. 25365\u201325389. Curran Associates, Inc.,\n2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/\nfile/4ff83037e8d97b2171b2d3e96cb8e677-Paper-Conference.pdf .\nTrenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, and Adam Jermyn et al.\nTowards monosemanticity: Decomposing language models dictionary learning. Trans-\nformer Circuits , October 2023. URL https://transformer-circuits.pub/2023/\nmonosemantic-features .\nTom B Brown. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 , 2020.\nJuang Caden, Paulo Gonc \u00b8alo, Drori Jacob, and Belrose Nora. Open source automated inter-\npretability for sparse autoencoder features, 2024. URL https://blog.eleuther.ai/\nautointerp/ . Accessed: 2024-09-27.\nHaozhe Chen, Carl V ondrick, and Chengzhi Mao. Selfie: Self-interpretation of large language model\nembeddings. arXiv preprint arXiv:2403.10949 , 2024.\n11", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3707, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "47e216b8-49ae-4bc1-8bb1-64548d98934d": {"__data__": {"id_": "47e216b8-49ae-4bc1-8bb1-64548d98934d", "embedding": null, "metadata": {"page_label": "12", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7c16e237-d09c-46c2-b1cf-65f41e21aa05"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddc71793-b545-46cd-aed7-28e3c9d98f64", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7c16e237-d09c-46c2-b1cf-65f41e21aa05"}, "hash": "7c70aa87f181b49edbe2d5b3f1fd68f4f0391c552e207029230379445cc9b0a7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "2c50109c-cdf3-445e-90c1-36f01caaacde", "node_type": "1", "metadata": {}, "hash": "d5a87bbba002a838763d8e52b1fc1b2f6a1f2c4bdb7e51149832aa7bc68ec702", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nMehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gor-\ndon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for\ncontrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition , pp. 2818\u20132829, 2023.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) , 2014.\nHoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse autoen-\ncoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600 ,\n2023.\nYusuf Dalva and Pinar Yanardag. Noiseclr: A contrastive learning approach for unsupervised discov-\nery of interpretable directions in diffusion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , pp. 24209\u201324218, June 2024.\nGytis Daujotas. Interpreting and steering features in images, 2024.\nURL https://www.lesswrong.com/posts/Quqekpvx8BGMMcaem/\ninterpreting-and-steering-features-in-images . Accessed: 2024-09-27.\nJustin Deschenaux, Igor Krawczuk, Grigorios Chrysos, and V olkan Cevher. Going beyond compo-\nsitions, DDPMs can produce zero-shot interpolations. In Forty-first International Conference on\nMachine Learning , 2024. URL https://openreview.net/forum?id=1pj0Sk8GfP .\nJacob Devlin. Bert: Pre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805 , 2018.\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. URL\nhttps://arxiv.org/abs/2105.05233 .\nSoch Joram Duchi John. Multivariate normal distribution: Kullback-leibler divergence, 05 2020.\nURL https://statproofbook.github.io/P/mvn-kl.html . Accessed on October\n31, 2024.\nNelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec,\nZac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish,\nJared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. Toy models of superpo-\nsition. Transformer Circuits , September 2022. URL https://transformer-circuits.\npub/2022/toy_model/index.html .\nPatrick Esser, Robin Rombach, and Bj \u00a8orn Ommer. Taming transformers for high-resolution image\nsynthesis, 2021. URL https://arxiv.org/abs/2012.09841 .\nRohit Gandikota, Joanna Materzynska, Tingrui Zhou, Antonio Torralba, and David Bau. Concept\nsliders: Lora adaptors for precise control in diffusion models, 2023. URL https://arxiv.\norg/abs/2311.12092 .\nLeo Gao, Tom Dupr \u00b4e la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya\nSutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint\narXiv:2406.04093 , 2024.\nAsma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patchscope: A\nunifying framework for inspecting hidden representations of language models. arXiv preprint\narXiv:2401.06102 , 2024.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3043, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2c50109c-cdf3-445e-90c1-36f01caaacde": {"__data__": {"id_": "2c50109c-cdf3-445e-90c1-36f01caaacde", "embedding": null, "metadata": {"page_label": "12", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7c16e237-d09c-46c2-b1cf-65f41e21aa05"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ddc71793-b545-46cd-aed7-28e3c9d98f64", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7c16e237-d09c-46c2-b1cf-65f41e21aa05"}, "hash": "7c70aa87f181b49edbe2d5b3f1fd68f4f0391c552e207029230379445cc9b0a7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "47e216b8-49ae-4bc1-8bb1-64548d98934d", "node_type": "1", "metadata": {"page_label": "12", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7c16e237-d09c-46c2-b1cf-65f41e21aa05"}, "hash": "2ea76c65e77ff05e8fc4fc3be9e39a42a8a359315462fc1440bc7e3282dcf20d", "class_name": "RelatedNodeInfo"}}, "text": "Concept\nsliders: Lora adaptors for precise control in diffusion models, 2023. URL https://arxiv.\norg/abs/2311.12092 .\nLeo Gao, Tom Dupr \u00b4e la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya\nSutskever, Jan Leike, and Jeffrey Wu. Scaling and evaluating sparse autoencoders. arXiv preprint\narXiv:2406.04093 , 2024.\nAsma Ghandeharioun, Avi Caciularu, Adam Pearce, Lucas Dixon, and Mor Geva. Patchscope: A\nunifying framework for inspecting hidden representations of language models. arXiv preprint\narXiv:2401.06102 , 2024.\nIan J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https:\n//arxiv.org/abs/1406.2661 .\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models, 2020. URL\nhttps://arxiv.org/abs/2006.11239 .\n12", "mimetype": "text/plain", "start_char_idx": 2509, "end_char_idx": 3386, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "121686d2-ff20-41d8-a8e7-8368d5c1332e": {"__data__": {"id_": "121686d2-ff20-41d8-a8e7-8368d5c1332e", "embedding": null, "metadata": {"page_label": "13", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7ac903b2-160c-4ced-9355-44f8fa637185"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b149eb8-2051-4a35-9973-5ca470efb10e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7ac903b2-160c-4ced-9355-44f8fa637185"}, "hash": "08417eb1c2a5adb2184bd4606fc9e77b46179a66b9b914df7f9a364558e1360d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "25304bf1-fbd1-4fbe-85c2-50d8a4ee82a0", "node_type": "1", "metadata": {}, "hash": "0d83eced8dbd38057b1bd713bb1a3bc7eafdb3404ec2da8c7289fef0f5f03580", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Train-\ning compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\nEmiel Hoogeboom, Jonathan Heek, and Tim Salimans. Simple diffusion: End-to-end diffusion for\nhigh resolution images, 2023. URL https://arxiv.org/abs/2301.11093 .\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021. URL https:\n//arxiv.org/abs/2106.09685 .\nAya Abdelsalam Ismail, Julius Adebayo, Hector Corrada Bravo, Stephen Ra, and Kyunghyun Cho.\nConcept bottleneck generative models. In The Twelfth International Conference on Learning\nRepresentations , 2023.\nWilliam B Johnson, Joram Lindenstrauss, and Gideon Schechtman. Extensions of lipschitz maps\ninto banach spaces. Israel Journal of Mathematics , 54(2):129\u2013138, 1986.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks, 2019. URL https://arxiv.org/abs/1812.04948 .\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-\nbased generative models, 2022. URL https://arxiv.org/abs/2206.00364 .\nBahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri,\nand Michal Irani. Imagic: Text-based real image editing with diffusion models, 2023. URL\nhttps://arxiv.org/abs/2210.09276 .\nGwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models\nfor robust image manipulation, 2022.\nDiederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models,\n2023. URL https://arxiv.org/abs/2107.00630 .\nPang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, and\nPercy Liang. Concept bottleneck models. In International conference on machine learning , pp.\n5338\u20135348. PMLR, 2020.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convo-\nlutional neural networks. Advances in neural information processing systems , 25, 2012.\nMingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent\nspace, 2023. URL https://arxiv.org/abs/2210.10960 .\nHang Li, Chengzhi Shen, Philip Torr, V olker Tresp, and Jindong Gu. Self-discovering inter-\npretable diffusion latent directions for responsible text-to-image generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12006\u201312016,\nJune 2024.\nTom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant\nVarma, J \u00b4anos Kram \u00b4ar, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse\nautoencoders everywhere all at once on gemma 2. arXiv preprint arXiv:2408.05147 , 2024.\nJae Hyun Lim and Jong Chul Ye. Geometric gan, 2017. URL https://arxiv.org/abs/\n1705.02894 .\nJohnny Lin and Joseph Bloom.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3049, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "25304bf1-fbd1-4fbe-85c2-50d8a4ee82a0": {"__data__": {"id_": "25304bf1-fbd1-4fbe-85c2-50d8a4ee82a0", "embedding": null, "metadata": {"page_label": "13", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7ac903b2-160c-4ced-9355-44f8fa637185"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b149eb8-2051-4a35-9973-5ca470efb10e", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7ac903b2-160c-4ced-9355-44f8fa637185"}, "hash": "08417eb1c2a5adb2184bd4606fc9e77b46179a66b9b914df7f9a364558e1360d", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "121686d2-ff20-41d8-a8e7-8368d5c1332e", "node_type": "1", "metadata": {"page_label": "13", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7ac903b2-160c-4ced-9355-44f8fa637185"}, "hash": "472c02764c743a7e70d1441070d359187857ce3d23253875ee84d4420878e0a0", "class_name": "RelatedNodeInfo"}}, "text": "Self-discovering inter-\npretable diffusion latent directions for responsible text-to-image generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 12006\u201312016,\nJune 2024.\nTom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant\nVarma, J \u00b4anos Kram \u00b4ar, Anca Dragan, Rohin Shah, and Neel Nanda. Gemma scope: Open sparse\nautoencoders everywhere all at once on gemma 2. arXiv preprint arXiv:2408.05147 , 2024.\nJae Hyun Lim and Jong Chul Ye. Geometric gan, 2017. URL https://arxiv.org/abs/\n1705.02894 .\nJohnny Lin and Joseph Bloom. Neuronpedia: Interactive reference and tooling for analyzing neural\nnetworks, 2023. URL https://www.neuronpedia.org . Software available from neuron-\npedia.org.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\nInProceedings of International Conference on Computer Vision (ICCV) , December 2015.\nCalvin Luo. Understanding diffusion models: A unified perspective, 2022. URL https:\n//arxiv.org/abs/2208.11970 .\n13", "mimetype": "text/plain", "start_char_idx": 2432, "end_char_idx": 3507, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ed49e192-9edc-41f9-a1ec-f7af2b58b59b": {"__data__": {"id_": "ed49e192-9edc-41f9-a1ec-f7af2b58b59b", "embedding": null, "metadata": {"page_label": "14", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1f24763a-48e8-4b9b-9756-1d694c22046b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92b1ee21-17e6-4687-8ffc-f17e358aa6cd", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1f24763a-48e8-4b9b-9756-1d694c22046b"}, "hash": "e10863ff9f4b30b52be4f6a0bfc81e32af8f8895ebd4867b62eb2b6a3e3a9d38", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "bb9c35b4-c46f-4d11-9b6c-f0ede6ef325f", "node_type": "1", "metadata": {}, "hash": "6a4e57cb8b46778f3c4492d9672c9824c6f01076041eb90a5ec2ab8610f14afa", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nSamuel Marks, Can Rager, Eric J Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller.\nSparse feature circuits: Discovering and editing interpretable causal graphs in language models.\narXiv preprint arXiv:2403.19647 , 2024.\nChenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon.\nSdedit: Guided image synthesis and editing with stochastic differential equations, 2022. URL\nhttps://arxiv.org/abs/2108.01073 .\nLars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for gans do\nactually converge?, 2018. URL https://arxiv.org/abs/1801.04406 .\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word represen-\ntations in vector space, 2013. URL https://arxiv.org/abs/1301.3781 .\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel Goh, Michael Petrov, and Shan Carter. An\noverview of early vision in inceptionv1. Distill , 5(4):e00024\u2013002, 2020.\nOpenAI. Hello gpt-4o, 2024. URL https://openai.com/index/hello-gpt-4o . Ac-\ncessed: 2024-09-28.\nMaxime Oquab, Timoth \u00b4ee Darcet, Th \u00b4eo Moutakanni, Huy V o, Marc Szafraniec, Vasil Khalidov,\nPierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nico-\nlas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael\nRabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv \u00b4e Jegou, Julien Mairal, Patrick Labatut, Ar-\nmand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision,\n2024. URL https://arxiv.org/abs/2304.07193 .\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with\nspatially-adaptive normalization, 2019. URL https://arxiv.org/abs/1903.07291 .\nYong-Hyun Park, Mingi Kwon, Jaewoong Choi, Junghyo Jo, and Youngjung Uh. Understand-\ning the latent space of diffusion models through the lens of riemannian geometry. In A. Oh,\nT. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neu-\nral Information Processing Systems , volume 36, pp. 24129\u201324142. Curran Associates, Inc.,\n2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/\nfile/4bfcebedf7a2967c410b64670f27f904-Paper-Conference.pdf .\nPablo Pernias, Dominic Rampas, Mats L Richter, Christopher J Pal, and Marc Aubreville.\nW\u00a8urstchen: An efficient architecture for large-scale text-to-image diffusion models. arXiv\npreprint arXiv:2306.00637 , 2023.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M \u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis, 2023. URL https://arxiv.org/abs/2307.01952 .\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning , pp.\n8748\u20138763.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2950, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb9c35b4-c46f-4d11-9b6c-f0ede6ef325f": {"__data__": {"id_": "bb9c35b4-c46f-4d11-9b6c-f0ede6ef325f", "embedding": null, "metadata": {"page_label": "14", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1f24763a-48e8-4b9b-9756-1d694c22046b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "92b1ee21-17e6-4687-8ffc-f17e358aa6cd", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1f24763a-48e8-4b9b-9756-1d694c22046b"}, "hash": "e10863ff9f4b30b52be4f6a0bfc81e32af8f8895ebd4867b62eb2b6a3e3a9d38", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ed49e192-9edc-41f9-a1ec-f7af2b58b59b", "node_type": "1", "metadata": {"page_label": "14", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1f24763a-48e8-4b9b-9756-1d694c22046b"}, "hash": "a6d62ae72b96c2e259b4d8636b8839cc4f4a7115731853e3a3f12403d2342cbb", "class_name": "RelatedNodeInfo"}}, "text": "W\u00a8urstchen: An efficient architecture for large-scale text-to-image diffusion models. arXiv\npreprint arXiv:2306.00637 , 2023.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M \u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis, 2023. URL https://arxiv.org/abs/2307.01952 .\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning , pp.\n8748\u20138763. PMLR, 2021.\nDaking Rai, Yilun Zhou, Shi Feng, Abulhair Saparov, and Ziyu Yao. A practical review of mecha-\nnistic interpretability for transformer-based language models. arXiv preprint arXiv:2407.02646 ,\n2024.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine\nlearning , pp. 8821\u20138831. Pmlr, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents, 2022. URL https://arxiv.org/abs/\n2204.06125 .\n14", "mimetype": "text/plain", "start_char_idx": 2292, "end_char_idx": 3578, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "22e4424b-0128-40f1-8b22-c1cb2018cf41": {"__data__": {"id_": "22e4424b-0128-40f1-8b22-c1cb2018cf41", "embedding": null, "metadata": {"page_label": "15", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cb98ae13-225c-4887-9101-935418a567a4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "630a02af-a6f0-49a1-a0da-f7777b4b2bb4", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cb98ae13-225c-4887-9101-935418a567a4"}, "hash": "5df1adb8a66431ba3ee36f846cf2795a2852f5dcfeb981f05c2291d69230901c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "adfb16e5-f86a-4a2e-9ead-db4a6a416081", "node_type": "1", "metadata": {}, "hash": "89d6f844c4e0bac7a5f6a5ceb5e1d4b2286b512573b0d84a1c6c9cba87cc238e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nAnton Razzhigaev, Arseniy Shakhmatov, Anastasia Maltseva, Vladimir Arkhipkin, Igor Pavlov, Ilya\nRyabov, Angelina Kuts, Alexander Panchenko, Andrey Kuznetsov, and Denis Dimitrov. Kandin-\nsky: An improved text-to-image synthesis with image prior and latent diffusion. In Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing: System Demon-\nstrations , pp. 286\u2013295, 2023.\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-\nnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing . Association for Computational Linguistics, 11 2019. URL https://arxiv.\norg/abs/1908.10084 .\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj \u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/\nabs/2112.10752 .\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\ncal image segmentation, 2015. URL https://arxiv.org/abs/1505.04597 .\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Sal-\nimans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic text-to-image dif-\nfusion models with deep language understanding, 2022a. URL https://arxiv.org/abs/\n2205.11487 .\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in neural informa-\ntion processing systems , 35:36479\u201336494, 2022b.\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models, 2022.\nURL https://arxiv.org/abs/2202.00512 .\nAxel Sauer, Kashyap Chitta, Jens M \u00a8uller, and Andreas Geiger. Projected gans converge faster, 2021.\nURL https://arxiv.org/abs/2111.01007 .\nAxel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. Stylegan-t: Unlocking\nthe power of gans for fast large-scale text-to-image synthesis, 2023a. URL https://arxiv.\norg/abs/2301.09515 .\nAxel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion dis-\ntillation, 2023b. URL https://arxiv.org/abs/2311.17042 .\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems , 35:25278\u201325294, 2022a.\nChristoph Schuhmann, Andreas K \u00a8opf, Richard Vencu, Theo Coombes, Romain Beaumont, and Ben-\njamin Trom. Laion coco: 600m synthetic captions from laion2b-en, 2022b. URL https:\n//laion.ai/blog/laion-coco/ . Accessed: 2024-10-01.\nLee Sharkey, Dan Braun, and beren. Interim research report: Tak-\ning features out of superposition with sparse autoencoders, 2022.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3057, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "adfb16e5-f86a-4a2e-9ead-db4a6a416081": {"__data__": {"id_": "adfb16e5-f86a-4a2e-9ead-db4a6a416081", "embedding": null, "metadata": {"page_label": "15", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cb98ae13-225c-4887-9101-935418a567a4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "630a02af-a6f0-49a1-a0da-f7777b4b2bb4", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cb98ae13-225c-4887-9101-935418a567a4"}, "hash": "5df1adb8a66431ba3ee36f846cf2795a2852f5dcfeb981f05c2291d69230901c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "22e4424b-0128-40f1-8b22-c1cb2018cf41", "node_type": "1", "metadata": {"page_label": "15", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cb98ae13-225c-4887-9101-935418a567a4"}, "hash": "e41aa9cf19ec3219d5373e2ea8d50c2ed9b879c070172ec0cedee3743c2ddba4", "class_name": "RelatedNodeInfo"}}, "text": "Laion-5b: An\nopen large-scale dataset for training next generation image-text models. Advances in Neural\nInformation Processing Systems , 35:25278\u201325294, 2022a.\nChristoph Schuhmann, Andreas K \u00a8opf, Richard Vencu, Theo Coombes, Romain Beaumont, and Ben-\njamin Trom. Laion coco: 600m synthetic captions from laion2b-en, 2022b. URL https:\n//laion.ai/blog/laion-coco/ . Accessed: 2024-10-01.\nLee Sharkey, Dan Braun, and beren. Interim research report: Tak-\ning features out of superposition with sparse autoencoders, 2022. URL\nhttps://www.lesswrong.com/posts/z6QQJbtpkEAX3Aojj/\ninterim-research-report-taking-features-out-of-superposition .\nAccessed: 2024-09-27.\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics, 2015. URL https://arxiv.org/\nabs/1503.03585 .\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution,\n2020. URL https://arxiv.org/abs/1907.05600 .\n15", "mimetype": "text/plain", "start_char_idx": 2539, "end_char_idx": 3535, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "826169ad-f8bc-4ee3-ab6d-d2d574db0010": {"__data__": {"id_": "826169ad-f8bc-4ee3-ab6d-d2d574db0010", "embedding": null, "metadata": {"page_label": "16", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_08b3359b-00ae-4df0-8c00-126ff9532792"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e19f99c-6744-45f6-9d5c-f080de5b3a82", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_08b3359b-00ae-4df0-8c00-126ff9532792"}, "hash": "0dd5e07c87fc9fb34d251b7775a38ed8c91d8befde8d4feafedaca435bbdd616", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4882514a-722e-40c0-9745-118cfe076dfd", "node_type": "1", "metadata": {}, "hash": "cbbadafb320c00ddd74335708714f8344e7d971e3f247177c1570fcf6df9b479", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nYang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations, 2021.\nMatteo Spinelli. Advanced style transfer with the mad scientist node. YouTube video, 2024. URL\nhttps://www.youtube.com/watch?v=ewKM7uCRPUg . Accessed: 2024-09-17.\nAdly Templeton and Tom Conerly et al. Scaling monosemanticity: Extracting interpretable fea-\ntures from claude 3 sonnet, 2024. URL https://transformer-circuits.pub/2024/\nscaling-monosemanticity/ . Accessed: 2024-09-27.\nClinton Wang and Polina Golland. Interpolating between images with diffusion models, 2023.\nZihao Wang, Lin Gui, Jeffrey Negrea, and Victor Veitch. Concept algebra for (score-\nbased) text-controlled generative models. In A. Oh, T. Naumann, A. Globerson,\nK. Saenko, M. Hardt, and S. Levine (eds.), Advances in Neural Information Pro-\ncessing Systems , volume 36, pp. 35331\u201335349. Curran Associates, Inc., 2023. URL\nhttps://proceedings.neurips.cc/paper_files/paper/2023/file/\n6f125214c86439d107ccb58e549e828f-Paper-Conference.pdf .\nFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. Lsun: Construction of a\nlarge-scale image dataset using deep learning with humans in the loop. CoRR , abs/1506.03365,\n2015. URL http://dblp.uni-trier.de/db/journals/corr/corr1506.html#\nYuZSSX15 .\nZhongqi Yue, Jiankun Wang, Qianru Sun, Lei Ji, Eric I-Chao Chang, and Hanwang Zhang. Explor-\ning diffusion time-steps for unsupervised representation learning, 2024.\nZeyu Yun, Yubei Chen, Bruno A Olshausen, and Yann LeCun. Transformer visualization via dictio-\nnary learning: contextualized embedding as a linear superposition of transformer factors. arXiv\npreprint arXiv:2103.15949 , 2021.\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In CVPR , 2018.\nZhixing Zhang, Ligong Han, Arnab Ghosh, Dimitris N. Metaxas, and Jian Ren. Sine: Single image\nediting with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR) , pp. 6027\u20136037, June 2023.\nA F INDING CAUSALLY INFLUENTIAL TRANSFORMER BLOCKS\nAs a first step, we narrow down design space of the 11 cross-attention transformer blocks (see Fig. 4)\nto those with the highest causal impact on the output. In order to assess their causal impact on the\noutput we qualitatively study the effect of individually ablating each of them (see Fig. 2). As can be\nseen in Fig. 2 each of the middle blocks down.2.1 ,mid.0 ,up.0.0 ,up.0.1 have a relatively\nhigh impact on the output respectively. In particular, the blocks down.2.1 andup.0.1 stand out.\nIt seems like most colors and textures are added in up.0.1 , which in the community is already\nknown as \u201cstyle\u201d block Spinelli (2024). Ablating down.2.1 , which is also already known in the\ncommunity as \u201ccomposition\u201d block, impacts the entire image composition, including object sizes,\norientations and framing. The effects of ablating other blocks such as mid.0 andup.0.0 are more\nsubtle. For mid.0 it is difficult to describe in words and up.0.0 seems to add local details to the\nimage while leaving the overall composition mostly intact.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3265, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4882514a-722e-40c0-9745-118cfe076dfd": {"__data__": {"id_": "4882514a-722e-40c0-9745-118cfe076dfd", "embedding": null, "metadata": {"page_label": "16", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_08b3359b-00ae-4df0-8c00-126ff9532792"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6e19f99c-6744-45f6-9d5c-f080de5b3a82", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_08b3359b-00ae-4df0-8c00-126ff9532792"}, "hash": "0dd5e07c87fc9fb34d251b7775a38ed8c91d8befde8d4feafedaca435bbdd616", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "826169ad-f8bc-4ee3-ab6d-d2d574db0010", "node_type": "1", "metadata": {"page_label": "16", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_08b3359b-00ae-4df0-8c00-126ff9532792"}, "hash": "0cdb893aefd9dd74f11a4f58bf4a3db4ddaf594ffa0895cd7882dd8466b7763c", "class_name": "RelatedNodeInfo"}}, "text": "2 each of the middle blocks down.2.1 ,mid.0 ,up.0.0 ,up.0.1 have a relatively\nhigh impact on the output respectively. In particular, the blocks down.2.1 andup.0.1 stand out.\nIt seems like most colors and textures are added in up.0.1 , which in the community is already\nknown as \u201cstyle\u201d block Spinelli (2024). Ablating down.2.1 , which is also already known in the\ncommunity as \u201ccomposition\u201d block, impacts the entire image composition, including object sizes,\norientations and framing. The effects of ablating other blocks such as mid.0 andup.0.0 are more\nsubtle. For mid.0 it is difficult to describe in words and up.0.0 seems to add local details to the\nimage while leaving the overall composition mostly intact.\nB SAE T RAINING RESULTS\nWe trained several SAEs with different sparsity levels and sparse layer sizes and observed no\ndead features. To assess reconstruction quality, we processed 100 random LAION-COCO prompts\nthrough a one-step SDXL Turbo process, replacing the additive component of the corresponding\ntransformer block with its SAE reconstruction.\n16", "mimetype": "text/plain", "start_char_idx": 2551, "end_char_idx": 3618, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "757d5b9b-4a0d-4cda-8fc3-8283fabd0bd7": {"__data__": {"id_": "757d5b9b-4a0d-4cda-8fc3-8283fabd0bd7", "embedding": null, "metadata": {"page_label": "17", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_38362c39-bdf4-4948-ab14-462ec6a98adc"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "18c787bc-611d-4a8a-b3a9-4d1452dcf72c", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_38362c39-bdf4-4948-ab14-462ec6a98adc"}, "hash": "bc47310dc92ab03b13c5bfb912da0a1182a6d6654285883c5f4bb58afb198b90", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nbaseline\n down.1.0\n down.1.1\n down.2.0\n down.2.1\n mid.0\n up.0.0\n up.0.1\n up.0.2\n up.1.0\n up.1.1\n up.1.2\nFigure 2: We generate images for the prompts \u201cA dog playing with a ball cartoon.\u201d, \u201cA photo of\na colorful model.\u201d, \u201cAn astronaut riding on a pig on the moon.\u201d, \u201cA photograph of the inside of a\nsubway train. There are frogs sitting on the seats. One of them is reading a newspaper. The window\nshows the river in the background.\u201d and \u201cA cinematic shot of a professor sloth wearing a tuxedo at a\nBBQ party.\u201d while ablating the updates performed by different cross-attention layers (indicated by\nthe titles). The title \u201cbaseline\u201d corresponds to the generation without interventions.\nThe explained variance ratio and the output effects caused by reconstruction are shown in Table 2.\nFig. 3 presents random examples of reconstructions from an SAE with the following hyperparam-\neters: k= 10, nf= 5120 , trained on down.2.1 . The reconstruction causes minor deviations in\nthe images, and the fairly low LPIPS (Zhang et al., 2018) and pixel distance scores also support\nthese findings. However, to prevent these minor reconstruction errors from affecting our analysis of\ninterventions, we decided to directly add or subtract learned directions from dense feature maps.\nTable 2: Distances and explained variance ratio in generated images. \u201cMean\u201d represents the average\npixel Manhattan distance between original and reconstruction-intervened images, with a maximum\npossible value of 765. \u201cMedian\u201d represents the median Manhattan distance per pixel, averaged over\nall images. \u2019LPIPS\u2019 refers to the average LPIPS score, measuring perceptual similarity. \u201cExplained\nvariance ratio\u201d denotes the ratio of variance explained by the trained SAEs to the total variance.\nk n f Configuration Mean |Median LPIPS Explained Variance Ratio (%)\n5640down 83.29 |50.04 0.3383 56.0\nmid 52.64 |26.82 0.2032 43.4\nup0 55.89 |30.69 0.2276 44.8\nup 52.67 |34.53 0.2073 50.3\n5120down 74.68 |41.49 0.3036 67.8\nmid 48.82 |24.60 0.1845 50.8\nup0 49.19 |25.86 0.1969 57.2\nup 47.50 |31.11 0.1775 59.5\n10640down 73.65 |41.79 0.2893 62.8\nmid 46.80 |23.10 0.1772 51.5\nup0 48.43 |25.80 0.1908 52.5\nup 43.06 |26.85 0.1638 58.7\n5120down 64.97 |34.77 0.2582 73.7\nmid 44.02 |21.72 0.1627 58.8\nup0 42.08 |21.54 0.1624 64.2\nup 39.77 |24.84 0.1453 67.1\n20640down 59.29 |31.47 0.2291 69.9\nmid 39.95 |19.44 0.1459 60.0\nup0 40.15 |21.06 0.1499 60.9\nup 31.97 |18.15 0.1196 66.7\n5120down 56.37 |29.04 0.2190 78.8\nmid 37.28 |17.82 0.1328 66.5\nup0 35.73 |18.03 0.1302 70.6\nup 30.31 |17.22 0.1104 74.2\n17", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2556, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b0419e5-3c0b-4216-a6cb-1b9ef28c539e": {"__data__": {"id_": "5b0419e5-3c0b-4216-a6cb-1b9ef28c539e", "embedding": null, "metadata": {"page_label": "18", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b0be4544-5112-4e8c-a741-8df32153f426"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4d2f88f7-6990-40b9-8260-d98b8f09503a", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b0be4544-5112-4e8c-a741-8df32153f426"}, "hash": "582287204be52fd76e14f7938ddd8be82ef338827981c151e1cfd40a62805fce", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 3: Images generated from 10 random prompts taken from the LAION-COCO dataset are\nshown in the first row. In the second row, down.2.1 updates are replaced by their SAE recon-\nstructions ( k= 10, nf= 5120 ). The third row visualizes the differences between the original and\nreconstructed images.\nDOWN.0 (no attn)\nDOWN.1.0\nDOWN.1.1\nDOWN.2.0\nDOWN.2.1\nMID.0UP.0.0UP.0.1UP.0.2UP.1.0UP.1.1UP.1.2UP.2 (no attn)\nResNet\nUpsampler\nDownsampler\nFigure 4: Cross-attention transformer blocks in SDLX\u2019s U-net.\n18", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 512, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2b9e3f48-e0fb-4762-87c1-a5cf7ab12953": {"__data__": {"id_": "2b9e3f48-e0fb-4762-87c1-a5cf7ab12953", "embedding": null, "metadata": {"page_label": "19", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_34aa04a8-ed51-4db7-a685-6c358b461301"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0106f731-6b04-4c5e-a4f7-19bbab2e6ec3", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_34aa04a8-ed51-4db7-a685-6c358b461301"}, "hash": "a6b049aaa00ba86d22de59c0239611f4a564efaa0ccd47af9586f34ca59b42fc", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nC C ASE STUDY I: M OST ACTIVE FEATURES ON A PROMPT\nFig. 5 demonstrates an extended version of Case Study I, showcasing 9 top features instead of 5.\nD F EATURES INCLUDING PROMPTS\nFeature plots. We provide the same plots as in Fig. 6 but for the last six feature indices of each\ntransformer block in Fig. 7 and the corresponding prompts in Table 4. Additionally, provide some\nselected features for down.2.1 andup.0.1 in Fig. 8 and the corresponding prompts in Table 5.\nIntervention plots. Additionally, we provide plots in which we turn on features from Fig. 8 but in\nunrelated prompts (as opposed to top dataset example prompts that already activate the features by\nthemselves). For simplicity here we simply turn on the features across all spatial locations, which\ndoes not seem to be a well suitable strategy for up.0.1 , which usually acts locally. To showcase,\nthe difference we created one example image in Fig. 10, in which we manually draw localized masks\nto turn on the corresponding features.\nE A NNOTATION PIPELINE DETAILS\nWe used GPT-4o to caption learned features on down.2.1 . For each feature, the model was shown\na series of 5 unrelated images, a progression of 9 images, the i-th of those corresponds to \u223ci\u00b710%\naverage activation value of the maximum. Finally, we show 5 images corresponding to the highest\naverage activations. Since some features are active on particular parts of images, the last 9 images\nare provided alongside their so-called \u201ccoldmaps\u201d: a version of an image with weakly active and\ninactive regions being faded and concealed.\nThe images were generated by 1-step SDXL Turbo diffusion process on 50\u2032000random prompts of\nLAION-COCO dataset.\nE.1 T EXTUAL PROMPT TEMPLATE\nHere is the prompt template for the VLM.\nSystem. You are an experienced mechanistic interpretability researcher that is\nlabeling features from the hidden representations of an image generation model.\nUser. You will be shown a series of images generated by a machine learning\nmodel. These images were selected because they trigger a specific feature of a\nsparse auto-encoder, trained to detect hidden activations within the model. This\nfeature can be associated with a particular object, pattern, concept, or a place on\nan image. The process will unfold in three stages:\n1. **Reference Images:** First, you\u2019ll see several images *unrelated* to the fea-\nture. These will serve as a reference for comparison.\n2. **Feature-Activating Images:** Next, you\u2019ll view images that activate the fea-\nture with varying strengths. Each of these images will be shown alongside a ver-\nsion where non-activated regions are masked out, highlighting the areas linked to\nthe feature.\n3. **Strongest Activators:** Finally, you\u2019ll be presented with the images that\nmost strongly activate this feature, again with corresponding masked versions to\nemphasize the activated regions.\nYour task is to carefully examine all the images and identify the thing or concept\nrepresented by the feature. Here\u2019s how to provide your response:\n- **Reasoning:** Between \u2018<thinking >\u2018and\u2018</thinking >\u2018tags, write up to\n400 words explaining your reasoning. Describe the visual patterns, objects, or\nconcepts that seem to be consistently present in the feature-activating images but\nnot in the reference images.\n19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3284, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bfaa64d9-281e-44f8-9f18-702155ca8164": {"__data__": {"id_": "bfaa64d9-281e-44f8-9f18-702155ca8164", "embedding": null, "metadata": {"page_label": "20", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5fd52b63-21b4-4c12-bc15-16e182c272d9"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "36373ec7-c605-4a10-a7d0-dabfebf39ab4", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5fd52b63-21b4-4c12-bc15-16e182c272d9"}, "hash": "e79dcf6bbbb75556eceb70782bd6c28daf50de2785a6bee565ff34b4ac844d3e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nhmap1674\nA. -6.0\n A. 6.0\n B. 1.0\n C. 0\n C. 1\n4608\n 4539\n 2881\n 4751\n 567\n 1226\n 3119\n 2345\n(a) Top 9 features of down.2.1\nhmap500\nA. -10.0\n A. 10.0\n B. 1.0\n C. 0\n C. 1\n2727\n 1700\n 1295\n 3936\n 4238\n 2314\n 2341\n 3748\n (b) Top 9 features up.0.1\nhmap3603\nA. -8.0\n A. 8.0\n C. 0\n C. 1\n C. 2\n5005\n 775\n 153\n 1550\n 2221\n 2648\n 1604\n 564\n(c) Top 9 features of up.0.0\nhmap4755\nA. -16.0\n A. 16.0\n C. 0\n C. 1\n C. 2\n4235\n 1388\n 5102\n 3018\n 1935\n 473\n 2322\n 3067\n (d) Top 9 features of mid.0\nFigure 5: The top 9 features of down.2.1 (a),up.0.1 (b),up.0.0 (c) and mid.0 (d) for\nthe prompt: \u201cA cinematic shot of a professor sloth wearing a tuxedo at a BBQ party.\u201d Each row\nrepresents a feature. The first column depicts a feature heatmap (highest activation red and lowest\nnonzero one blue). The column titles containing \u201cA\u201d show feature modulation interventions, the\nones containing \u201cB\u201d the intervention of turning on the feature on the empty prompt, and the ones\ncontaining \u201cC\u201d depict top dataset examples. Floating point values in the title denote \u03b2and\u03b3values.\n20", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1059, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "21ff6bf5-5dd6-4bcd-a6df-736cc4cd6e47": {"__data__": {"id_": "21ff6bf5-5dd6-4bcd-a6df-736cc4cd6e47", "embedding": null, "metadata": {"page_label": "21", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_74707909-3502-4b21-85e9-331cd2c353eb"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6c625e08-5918-4915-940e-948812542cef", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_74707909-3502-4b21-85e9-331cd2c353eb"}, "hash": "76d6c6d0dfd5f77f8343406489a9f70e244b299a421caaa44abcb18167b508de", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\n- **Expression:** Afterward, between \u2018<answer >\u2018and\u2018</answer >\u2018tags,\nwrite a concise phrase (no more than 15 words) that best captures the common\nthing or concept across the majority of feature-activating images.\nNote that not all feature-activating images may perfectly align with the concept\nyou\u2019re describing, but the images with stronger activations should give you the\nclearest clues. Also pay attention to the masked versions, as they highlight the\nregions most relevant to the feature.\nUser. These images are not related to the feature: {Reference Images }\nUser. This is a row of 9 images, each illustrating increasing levels of feature\nactivation. From left to right, each image shows a progressively higher activation,\nstarting with the image on the far left where the feature is activated at 10% relative\nto the image that activates it the most, all the way to the far right, where the feature\nactivates at 90% relative to the image that activates it the most. This gradual\ntransition highlights the feature\u2019s growing importance across the series. {Feature-\nActivating Images }\nUser. This row consists of 9 masked versions of the original images. Each masked\nimage corresponds to the respective image in the activation row. Areas where the\nfeature is not activated are completely concealed by a white mask, while regions\nwith activation remain visible.) {Feature-Activating Images Coldmaps }\nUser. These images activate the feature most strongly. {Strongest Activators }\nUser. These masked images highlight the activated regions of the images that\nactivate the feature most strongly. The masked images correspond to the images\nabove. The unmasked regions are the ones that activate the feature. {Strongest\nActivators Coldmaps }\nE.2 E XAMPLE OF PROMPT IMAGES\nThe images used to annotate feature 0are shown in Fig. 11.\nE.3 E XAMPLES OF GENERATED CAPTIONS\nWe present the captions generated by GPT-4o for the first and last 10 features in Table 6.\nF S PARSE AUTOENCODERS AND SUPERPOSITION\nLeth(x)\u2208Rdbe some intermediate result of a forward pass of a neural network on the input x. In a\nfully connected neural network, the components h(x)could correspond to neurons. In transformers,\nwhich are residual neural networks with attention and fully connected layers, h(x)usually either\nrefers to the content of the residual stream after some layer, an update to the residual stream by\nsome layer, or the neurons within a fully connected block. In general, h(x)could refer to anything,\ne.g., also keys, queries, and values. It has been shown (Yun et al., 2021; Cunningham et al., 2023;\nBricken et al., 2023) that in many neural networks, especially LLMs, intermediate representations\ncan be well approximated by sparse sums of nf\u2208Nlearned feature vectors, i.e.,\nh(x)\u2248nfX\n\u03c1=1s\u03c1(x)f\u03c1, (12)\nwhere s\u03c1(x)are the input-dependent8coefficients most of which are equal to zero and f1, . . . ,fnf\u2208\nRdis a learned dictionary of feature vectors.\nImportantly, these learned features are usually highly interpretable (specific), sensitive (fire on the\nrelevant contexts), causal (change the output in expected ways in intervention) and usually do not\ncorrespond directly to individual neurons. There are also some preliminary results on the universal-\nity of these learned features, i.e., that different training runs on similar data result in the correspond-\ning models picking up largely the same features (Bricken et al., 2023).\n8In the literature this input dependence is usually omitted.\n21", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3489, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1edfa7f3-fcd8-4559-881d-f975eb7a1e9d": {"__data__": {"id_": "1edfa7f3-fcd8-4559-881d-f975eb7a1e9d", "embedding": null, "metadata": {"page_label": "22", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2e2895f8-d7f5-46a7-93e0-c30c2aff6561"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "3ff55ed0-48b8-4bc6-b125-b08b3580a6dd", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2e2895f8-d7f5-46a7-93e0-c30c2aff6561"}, "hash": "97251f0cf1773ed7c3d14ed74be8b6ca2fa0be6ec7811862b7fa66d37c9fd3b5", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\n0\nA. -3.0\n Orig.\n A. 3.0\n0 C.\n 1\n 1 C.\n 2\n 2 C.\n 3\n 3 C.\n 4\n 4 C.\n 5\n 5 C.\n(a)down.2.1\n0\nA. -8.0\n Orig.\n A. 8.0\n1\n 2\n 3\n 4\n 5\n (b)mid.0\n0\nA. -8.0\n Orig.\n A. 8.0\n1\n 2\n 3\n 4\n 5\n (c)up.0.0\n0\nA. -3.0\n Orig.\n A. 3.0\n0 C.\n 1\n 1 C.\n 2\n 2 C.\n 3\n 3 C.\n 4\n 4 C.\n 5\n 5 C.\n (d)up.0.1\nFigure 6: We visualize 6 features for down.2.1 (a),mid.0 (b),up.0.0 , and up.0.1 . We\nuse three columns for each transformer block and three rows for each feature. For down.2.1\nandup.0.1 we visualize the two samples from the top 5% quantile of activating dataset examples\n(middle) together a feature ablation (left) and a feature enhancement (right), and, activate the feature\non the empty prompt with \u03b3= 0.5,1,2from left to right. For mid.0 andup.0.0 we display three\nsamples with ablation and enhancement. Captions are in Table 3.\n22", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 816, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6236f5ac-55c1-4ba8-a414-b988e20d5d41": {"__data__": {"id_": "6236f5ac-55c1-4ba8-a414-b988e20d5d41", "embedding": null, "metadata": {"page_label": "23", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2784a910-4e1c-4d91-b364-8c8f5c6436b6"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "77487a0f-ce2d-4c6f-8fb6-673b5caf6fe5", "node_type": "4", "metadata": {"page_label": "23", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2784a910-4e1c-4d91-b364-8c8f5c6436b6"}, "hash": "88bc1a51a293b66eddb1c8ba6102a67e125970d617f64b7490e9007911ea6174", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nTable 3: Prompts for the top 5% quantile examples in Fig. 6\nBlock Feature Prompt\ndown.2.1 0 A file folder with the word document management on it.\n0 Two blue folders filled with dividers.\n1 A kitchen with an island and bar stools.\n1 An unfinished bar with stools and a wood counter.\n2 The Taj Mahal, or a white marble building in India.\n2 The Taj Mahal, or a white marble building in India.\n3 A man and woman standing next to each other.\n3 Two men in suits hugging each other outside.\n4 An old Forester whiskey bottle sitting on top of a wooden table.\n4 Red roses and hearts on a wooden table.\n5 A beaded brooch with pearls and copper.\n5 An image of a brooch with diamonds.\nmid.0 0 The Boss TS-3W pedal has an electronic tuner.\n0 An engagement ring with blue sapphire and diamonds.\n0 The women\u2019s pink sneaker is shown.\n1 A white ceiling fan with three blades.\n1 A ceiling fan with three blades and a light.\n1 The ceiling fan is dark brown and has two wooden blades.\n2 The black dress is made from knit and has metallic sleeves.\n2 The back view of a woman wearing a black and white sports bra.\n2 The woman is wearing a striped swimsuit.\n3 An old-fashioned photo frame with a little girl on it.\n3 The woman is sitting in her car with her head down.\n3 The contents of an empty bottle in a box.\n4 An old painting of a man in uniform.\n4 The model wears an off-white sweatshirt with green panel.\n4 The Statue of Liberty stands tall in front of a blue sky.\n5 Cheese and crackers on a cutting board.\n5 Two cufflinks with coins on them.\n5 Three pieces of luggage are shown in blue.\nup.0.0 0 Three wine glasses with gold and silver designs.\n0 Three green wine glasses sitting next to each other.\n0 New Year\u2019s Eve with champagne, gold, and silver.\n1 The birdhouse is made from wood and has a brown roof.\n1 The garage is white with red shutters.\n1 Two garages with one attached porch and the other on either side.\n2 An elegant white lace purse with gold clasp.\n2 The red handbag has gold and silver designs.\n2 A pink and green floral-colored purse.\n3 A magazine rack with magazines on it.\n3 The year-in-review page for this digital scrap.\n3 The planner sticker kit is shown with gold and black accessories.\n4 A clock with numbers on the face.\n4 A silver watch with roman numerals on the face.\n4 An automatic watch with a silver dial.\n5 Four pieces of wooden furniture with blue and white designs.\n5 The green chair is in front of a white rug.\n5 The wish chair with a black seat.\nup.0.1 0 The wooden toy kitchen set includes bread, eggs, and flour.\n0 The office chair is brown and black.\n1 An aerial view of the white sand and turquoise water.\n1 An aerial view of the beach and ocean.\n2 The patriarch of Ukraine is shown speaking to reporters.\n2 German Chancellor Merkel gestures as she speaks to the media.\n3 Four pictures showing dogs wearing orange vests.\n3 Two dogs are standing on the ground next to flowers.\n4 A man standing in front of a wooden wall.\n4 A blue mailbox sitting on top of a wooden floor.\n5 The baseball players are posing for a team photo.\n5 The baseball players are holding up their trophies.\n23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3113, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5a5cc34-8790-412b-8b86-62a828d920f4": {"__data__": {"id_": "d5a5cc34-8790-412b-8b86-62a828d920f4", "embedding": null, "metadata": {"page_label": "24", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_93d40fdc-7958-4bd6-ad75-150540c5f764"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "dfd4f146-1f22-4ac2-95d5-2f61fb56a7a8", "node_type": "4", "metadata": {"page_label": "24", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_93d40fdc-7958-4bd6-ad75-150540c5f764"}, "hash": "e50ff5d990448762a4d45fa16e019dba41570b8db874cc76d0135efec590450e", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\n5114\nA. -3.0\n Orig.\n A. 3.0\n5114 C.\n 5115\n 5115 C.\n 5116\n 5116 C.\n 5117\n 5117 C.\n 5118\n 5118 C.\n 5119\n 5119 C.\n(a)down.2.1\n5114\nA. -8.0\n Orig.\n A. 8.0\n5115\n 5116\n 5117\n 5118\n 5119\n (b)mid.0\n5114\nA. -8.0\n Orig.\n A. 8.0\n5115\n 5116\n 5117\n 5118\n 5119\n (c)up.0.0\n5114\nA. -3.0\n Orig.\n A. 3.0\n5114 C.\n 5115\n 5115 C.\n 5116\n 5116 C.\n 5117\n 5117 C.\n 5118\n 5118 C.\n 5119\n 5119 C.\n (d)up.0.1\nFigure 7: We visualize last 6 features for down.2.1 (a),mid.0 (b),up.0.0 , andup.0.1 . We\nuse three columns for each transformer block and three rows for each feature. For down.2.1 and\nup.0.1 we visualize two samples from the top 5% quantile of activating dataset examples (middle)\ntogether a feature ablation (left) and a feature enhancement (right), and, activate the feature on the\nempty prompt with \u03b3= 0.5,1,2from left to right. For mid.0 andup.0.0 we display three\nsamples with ablation and enhancement. Captions are in Table 4.\n24", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 925, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cfc013de-1f3f-4243-83ed-b0e81ced7233": {"__data__": {"id_": "cfc013de-1f3f-4243-83ed-b0e81ced7233", "embedding": null, "metadata": {"page_label": "25", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6b6d53c9-b783-4232-8785-66048f0d066a"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f24a07b-c9be-4e42-bdcb-7e30954a8ffe", "node_type": "4", "metadata": {"page_label": "25", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6b6d53c9-b783-4232-8785-66048f0d066a"}, "hash": "3e3470b38729858ea0cff73c0ca8ffe00cf68394f3ab58205bb1f01e85250ae8", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nTable 4: Prompts for the top 5 % quantile examples in Fig. 7\nBlock Feature Prompt\ndown.2.1 5114 Black and white Converse sneakers with the word black star.\n5114 Black and white Converse sneakers with the word Chuck.\n5115 A woman holding up a photo of herself.\n5115 A man holding up a tennis ball in the air.\n5116 The Nike Women\u2019s U.S. Soccer Team DRI-Fit 1/4 Zip Top.\n5116 The women\u2019s gray and orange half-zip sweatshirt.\n5117 A large group of people sitting in front of a basketball court.\n5117 Hockey players are playing in an arena with spectators.\n5118 The black and white plaid shirt is shown.\n5118 The different colors and sizes of t-shirts.\n5119 A ball of yarn on a white background.\n5119 Two balls of colored wool are on the white surface.\nmid.0 5114 People holding signs in front of a building.\n5114 Two men dressed in suits and ties are holding up signs.\n5114 A large group of people holding flags and signs.\n5115 A kitchen with white cabinets and a blue stove.\n5115 The kitchen is clean and ready for us to use.\n5115 A kitchen with white cabinets and stainless steel appliances.\n5116 The steering wheel and dashboard in a car.\n5116 The interior of a car with dashboard controls.\n5116 The dashboard and steering wheel in a car.\n5117 Three men are celebrating a goal on the field.\n5117 Two men in Red Bull racing gear standing next to each other.\n5117 Two men are posing for the camera at an event.\n5118 Someone is holding up their nail polish with pink and black designs.\n5118 The nail is very cute and looks great with marble.\n5118 White stily nails with gold and diamonds.\n5119 The Mighty Thor comic book.\n5119 The camera is showing its flash drive.\n5119 A truck with bikes on the back parked next to a camper.\nup.0.0 5114 The Acer laptop is open and ready to use.\n5114 The Lenovo S13 laptop is open and has an image of a person jumping off the keyboard.\n5114 A laptop with the words Hosting Event on it.\n5115 A horse with a black nose and brown mane.\n5115 The horse leather oil is being used to protect horses.\n5115 An oil painting on a canvas of a horse.\n5116 The sun is shining brightly over Saturn.\n5116 A football player throws the ball to another team.\n5116 Car door light logo sticker for Hyundai.\n5117 An artistic black and silver sculpture with speakers.\n5117 The pink brushes are sitting on top of each other.\n5117 Four kings playing cards in the hand.\n5118 A man is fixing an air conditioner.\n5118 The black Land Rover is parked in front of a large window.\n5118 A flat screen TV mounted on the wall above a fireplace.\n5119 A table with many different tools on it.\n5119 A camera with many different items including flash cards, lenses, and other accessories.\n5119 The contents of an open suitcase and some clothes.\nup.0.1 5114 An old Navajo rug with multicolored designs.\n5114 The pillow is made from an old kilim.\n5115 An image of noni juice with some fruits.\n5115 A bottle and glass on the counter with green juice.\n5116 Someone cleaning the shower with a sponge.\n5116 A man on a skateboard climbing a wall with ropes.\n5117 A man taking a selfie in front of some camera equipment.\n5117 A person holding up a business card with the words cycle transportation.\n5118 Two photos are placed on top of an open book.\n5118 An open book with pictures of children and their parents.\n5119 An engagement ring with diamonds on top.\n5119 An oval ruby and diamond ring.\n25", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3390, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "eaba8dc1-ebfb-4afd-a8df-d30d08b022bc": {"__data__": {"id_": "eaba8dc1-ebfb-4afd-a8df-d30d08b022bc", "embedding": null, "metadata": {"page_label": "26", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_29ce5bb6-230e-46ef-a291-f599ed151581"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9f44cff3-3389-4669-bd9c-57aecb6d0a02", "node_type": "4", "metadata": {"page_label": "26", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_29ce5bb6-230e-46ef-a291-f599ed151581"}, "hash": "7f822f62e0b8107a8960beaa955e86a54c36b027223b9fea2697b169240edcf9", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\n4998\nA. -6.0\n A. -3.0\n Orig.\n A. 3.0\n A. 6.0\n4074\n 2301\n 56\n 59\n 89\n(a)down.2.1\n4955\nA. -6.0\n A. -3.0\n Orig.\n A. 3.0\n A. 6.0\n4977\n 3718\n 90\n 1093\n 2165\n (b)up.0.1\nFigure 8: We visualize 6 features for down.2.1 (a) and up.0.1 (b). We use 5 columns for\neach transformer block and three rows for each feature. We visualize three samples from the top\n5% quantile of activating dataset examples (middle) together a feature ablation (left) and a feature\nenhancement (right). Captions are in Table 5.\n26", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 505, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "42d6eb62-4b84-4f9f-961f-10853248134b": {"__data__": {"id_": "42d6eb62-4b84-4f9f-961f-10853248134b", "embedding": null, "metadata": {"page_label": "27", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_565ae114-3def-486d-8024-a37df37a412f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a4784f93-1e40-40bc-b704-7d1897ddd5b2", "node_type": "4", "metadata": {"page_label": "27", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_565ae114-3def-486d-8024-a37df37a412f"}, "hash": "edeedf567308da555388854185c7053ac9734b427ac2d42c2897e086e9292d4d", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nTable 5: Prompts for the top 5% quantile examples in Fig. 8\nBlock Feature Prompt\ndown.2.1 4998 A cartoon bee wearing a hat and holding something.\n4998 Two cartoon pictures of the same man with his hands in his pockets.\n4998 A cartoon bear with a purple shirt and yellow shorts.\n4074 An anime character with cat ears and a dress.\n4074 Two anime characters, one with white hair and the other with red eyes.\n4074 An anime book with two women in blue dresses.\n2301 A man with white hair and red eyes holding a chain.\n2301 An animated man with white hair and a beard.\n2301 The character is standing with horns on his head.\n56 Two men in uniforms riding horses with swords.\n56 A woman riding on the back of a brown horse.\n56 Two jockeys on horses racing down the track.\n59 A red jar with floral designs on it.\n59 An old black vase with some design on it.\n59 A vase with birds and flowers on it.\n89 StarCraft 2 is coming to the Nintendo Wii.\n89 Overwatch is coming to Xbox and PS3.\n89 The hero in Overwatch is holding his weapon.\nup.0.1 4955 An African wild dog laying in the grass.\n4955 The woman is posing for a photo in her leopard print top.\n4955 An animal print cube ottoman with brown and white fur.\n4977 A white tiger with blue eyes standing in the snow.\n4977 A bottle and tiger are shown next to each other.\n4977 A mural on the side of a building with a tiger.\n3718 Giraffes are standing in the grass near a vehicle.\n3718 Two giraffes standing next to each other in the grass.\n3718 A giraffe standing next to an ironing board.\n90 A lion is roaring its teeth in the snow.\n90 A lion sitting in the grass looking off into the distance.\n90 Two lions with flowers on their backs.\n1093 The sun is shining over mountains and trees.\n1093 Bride and groom in front of a lake with sun flare.\n1093 The milky sun is shining brightly over the trees.\n2165 The silhouette of a person riding a bike at sunset.\n2165 The Dark Knight rises from his cave in Batman\u2019s poster.\n2165 A yellow sign with black design depicting a tractor.\nTable 6: down.2.1 first 10 and last 10 feature captions.\nBlock Feature Caption\ndown.2.1 0 Organizational/storage items for documents and office supplies\n1 Luxury kitchen interiors and designs\n2 Architectural Landmarks and Monumental Buildings\n3 Upper body clothing and attire\n4 Rustic or Natural Wooden Textures or Surfaces\n5 Intricately designed and ornamental brooches\n6 Technical diagrams and instructional content\n7 Feature predominantly activated by visual representations of dresses\n8 Home decor textiles focusing on cushions and pillows\n9 Eyewear: glasses and sunglasses\n5110 Concept of containment or organized enclosure\n5111 Groups of people in collective settings\n5112 Modern minimalist interior design\n5113 Indoor plants and greenery\n5114 Feature sensitivity focused on sneakers\n5115 Handling or manipulating various objects\n5116 Athletic outerwear, particularly zippered sporty jackets\n5117 Spectator Seating in Sporting Venues\n5118 Textiles and clothing materials, focus on textures and folds\n5119 Yarn and Knitting Textiles\n27", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3062, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cdb16ce0-e4d3-4eae-97cc-938e65c3a125": {"__data__": {"id_": "cdb16ce0-e4d3-4eae-97cc-938e65c3a125", "embedding": null, "metadata": {"page_label": "28", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3e94c69b-5bfd-4a92-965f-e50463130eb0"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "977f62db-5ae3-4c30-9894-bc70f36b4aab", "node_type": "4", "metadata": {"page_label": "28", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3e94c69b-5bfd-4a92-965f-e50463130eb0"}, "hash": "e4b6db3ef6f8e00cf87ef68aed8d347b8ff4b5ec168c8ab6cbced87d5d8b3310", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "552a83dd-66c5-4f09-b911-7003dd371440", "node_type": "1", "metadata": {}, "hash": "60e9981b9c54c9e1f59bb8d7d06d4bef7b06f1a5dcbec1f1a7468b914360d6d6", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nSuperposition. By associating task-relevant features with directions in Rdinstead of individual\ncomponents of h(x)\u2208Rd, it is possible to represent many more features than there are components,\ni.e.,nf>> d . As a result, in this case, the learned dictionary vectors f1, . . . ,fnfcannot be orthog-\nonal to each other, which can lead to interference when too many features are on (thus the sparsity\nrequirement). However, it would be theoretically possible to have exponentially (in d) many almost\northogonal directions embedded in Rd.9\nUsing representations like this, the optimization process during training can trade off the benefits\nof being able to represent more features than there are components in hwith the costs of features\ninterfering with each other. Such representations are especially effective if the real features under-\nlying the data do not co-occur with each other too much, that is, they are sparse. In other words, in\norder to represent a single input (\u201cMichael Jordan\u201d) only a small subset of the features (\u201cperson\u201d,\n..., \u201cplayed basketball\u201d) is required (Elhage et al., 2022; Bricken et al., 2023).\nThe phenomenon of neural networks that exploit representations with more features than there are\ncomponents (or neurons) is called superposition (Elhage et al., 2022). Superposition can explain\nthe presence of polysemantic neurons. The neurons, in this case, are simply at the wrong level of\nabstraction. The closest feature vector can change when varying a neuron, resulting in the neuron\nseemingly reacting to or steering semantically unrelated things.\nSparse autoencoders. In order to implement the sparse decomposition from equation 12, the vector\nscontaining the nfcoefficients of the sparse sum is parameterized by a single linear layer followed\nby ReLU activations, called the encoder ,\ns=ENC(h) =\u03c3(WENC(h\u2212bpre) +bact), (13)\nin which h\u2208Rdis the latent that we aim to decompose, \u03c3(\u00b7) = max(0 ,\u00b7),WENC\u2208Rnf\u00d7dis\na learnable weight matrix and bpreandbactare learnable bias terms. We omitted the dependencies\nh=h(x)ands=s(h)that are clear from context.\nSimilarly, the learnable features are parametrized by a single linear layer, called decoder ,\nh\u2032=DEC(s) =WDECs+bpre, (14)\nin which WDEC= (f1|\u00b7\u00b7\u00b7|fnf)\u2208Rd\u00d7nfis a learnable matrix of whose columns take the role of\nlearnable features and bpreis a learnable bias term.\nTraining. The pair ENC andDEC are trained in a way that ensures that h\u2032is a sparse sum of feature\nvectors (as in equation 1). Given a dataset of latents h1, . . . , h n, both encoder and decoder are\ntrained jointly to minimize a proxy to the loss\nmin\nWENC,WDEC\nbpre,bactnX\ni=1\u2225h\u2032\ni\u2212hi\u22252\n2+\u03bb\u2225si\u22250=nX\ni=1\u2225DEC(ENC(hi))\u2212hi\u22252\n2+\u03bb\u2225ENC(hi)\u22250, (15)\nwhere hi=h(xi),si=ENC(h(xi))(when we refer to components of swe use s\u03c1instead), the\n\u2225h\u2032\ni\u2212hi\u22252\n2is a reconstruction loss, \u2225si\u22250a regularization term ensuring the sparsity of the activations\nand\u03bbthe corresponding trade-off term.\nIn practice, \u2225si\u22250cannot be efficiently optimized directly, which is why it is usually replaced with\n\u2225si\u22251or other proxy objectives.\nTechnical details. In our work, we make use of the top- kformulation from Gao et al. (2024), in\nwhich \u2225si\u22250\u2264kis ensured by introducing the a top- kfunction TopK into the encoder:\ns=ENC(h) =\u03c3(TopK (WENC(h\u2212bpre) +bact)). (16)\nAs the name suggests, TopK returns a vector that sets all components except the top kones to zero.\nIn addition (Gao et al., 2024) use an auxiliary loss to handle dead features.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3451, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "552a83dd-66c5-4f09-b911-7003dd371440": {"__data__": {"id_": "552a83dd-66c5-4f09-b911-7003dd371440", "embedding": null, "metadata": {"page_label": "28", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3e94c69b-5bfd-4a92-965f-e50463130eb0"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "977f62db-5ae3-4c30-9894-bc70f36b4aab", "node_type": "4", "metadata": {"page_label": "28", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3e94c69b-5bfd-4a92-965f-e50463130eb0"}, "hash": "e4b6db3ef6f8e00cf87ef68aed8d347b8ff4b5ec168c8ab6cbced87d5d8b3310", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cdb16ce0-e4d3-4eae-97cc-938e65c3a125", "node_type": "1", "metadata": {"page_label": "28", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3e94c69b-5bfd-4a92-965f-e50463130eb0"}, "hash": "b69d6b4b5527379c01dd43fbf88065fbf47462fffbbea16f73d1dfc08b32697b", "class_name": "RelatedNodeInfo"}}, "text": "In practice, \u2225si\u22250cannot be efficiently optimized directly, which is why it is usually replaced with\n\u2225si\u22251or other proxy objectives.\nTechnical details. In our work, we make use of the top- kformulation from Gao et al. (2024), in\nwhich \u2225si\u22250\u2264kis ensured by introducing the a top- kfunction TopK into the encoder:\ns=ENC(h) =\u03c3(TopK (WENC(h\u2212bpre) +bact)). (16)\nAs the name suggests, TopK returns a vector that sets all components except the top kones to zero.\nIn addition (Gao et al., 2024) use an auxiliary loss to handle dead features. During training, a sparse\nfeature \u03c1is considered dead ifs\u03c1remains zero over the last 10M training examples.\nThe resulting training loss is composed of two terms: the L2-reconstruction loss and the top-\nauxiliary L2-reconstruction loss for dead feature reconstruction. For a single latent h, the loss is\ndefined\nL(h, h\u2032) =\u2225h\u2212h\u2032\u22252\n2+\u03b1\u2225h\u2212h\u2032\naux\u22252\n2 (17)\n9It follows from the Johnson-Lindenstrauss Lemma (Johnson et al., 1986) that one can find at least\nexp(d\u03f52/8)unit vectors in Rdwith the dot product between any two not larger than \u03f5.\n28", "mimetype": "text/plain", "start_char_idx": 2918, "end_char_idx": 3988, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "79f87622-c272-43db-9a42-9ab3546f3f81": {"__data__": {"id_": "79f87622-c272-43db-9a42-9ab3546f3f81", "embedding": null, "metadata": {"page_label": "29", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ac966a96-33f9-44c4-b8cc-b9e307acdc1f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a86406b1-82af-402a-abc4-13bd1c437960", "node_type": "4", "metadata": {"page_label": "29", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ac966a96-33f9-44c4-b8cc-b9e307acdc1f"}, "hash": "eedbf569cb60f750df4966a059b24896b19610fdf85285b54480272215aa15b7", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "fb7fe99a-99e2-49d0-8898-12795e1f9d64", "node_type": "1", "metadata": {}, "hash": "82e5900f743f69da3d26ed9aa681a107ebf4ef50a0c5c6fff78ab82d72a90a42", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nIn this equation, the h\u2032\nauxis the reconstruction based on the top kauxdead features. This auxiliary\nloss is introduced to mitigate the issue of dead features. After the end of the training process, we\nobserved none of them. Following (Gao et al., 2024), we set \u03b1=1\n32andkaux= 256 , performed\ntied initialization of encoder and decoder, normalized decoder rows after each training step. The\nnumber of learned features nfis set to 5120, which is four times the length of the input vector.\nThe value of kis set to 10 as a good trade-off between sparsity and reconstruction quality. Other\ntraining hyperparameters are batch size: 4096 , optimizer: Adam with learning rate: 10\u22124and betas:\n(0.9,0.999) .\nG F EW STEP DIFFUSION MODELS : SDXL T URBO\nDiffusion models. Diffusion models are a class of generative models that were introduced by Sohl-\nDickstein et al. (2015) and are a core component of many of the recent large-scale text-to-image\ngenerative models (Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022a). Notably, Ho\net al. (2020); Song & Ermon (2020) demonstrate that diffusion model are a viable alternative to\nGANs (Goodfellow et al., 2014) for image generation. Additionally, diffusion models enjoy stable\ntraining dynamics, are easier to scale than GANs (Dhariwal & Nichol, 2021), and offer likelihood\nestimates of samples (Song et al., 2021).\nDiffusion models sample from an unknown distribution pby learning to iteratively denoise corrupted\nsamples, starting from pure noise. The corruption process is defined on training samples from p.\nMathematically, the images are corrupted with Gaussian noise and are distributed according to\nqt(xt|x0) :=N(\u03b1tx0, \u03c32\ntI), (18)\nwhere x0corresponds to a real image from p,0\u2264t\u2264T,\u03b1t, \u03c32\ntare positive real-valued scalars such\nthat the signal-to-noise ratio SNR :=\u03b1t\n\u03c32\ntis monotonically decreasing. Additionally, the coefficients\n\u03b1T\u22121, \u03c32\nT\u22121are typically chosen such that xT\u223c N(0,I). In this work, the number of corruption\nsteps Tis fixed to 1000, as we study the pre-trained models from (Sauer et al., 2023b). Given this\npredetermined corruption process, the diffusion model learns to reverse it to recover clean data.\nThe denoising process is implemented via a distribution p\u03b8(xt\u22121|xt). The simplest way to generate\nsamples using p\u03b8(xt\u22121|xt)is to first generate a sample of pure noise xT\u223c N(0,I), followed by T\niterative applications of p\u03b8, yielding a sequence xT, xT\u22121, ..., x 1, x0, where x0approximates sam-\nples from p. The vector \u03b8represents the parameters of a neural network that defines p\u03b8(xt\u22121|xt).\nThere exist many objectives to learn to reverse the corruption process (Ho et al., 2020; Kingma\net al., 2023; Song & Ermon, 2020), but p\u03b8is generally trained to minimize the Kullback-Leibler\ndivergence between adjacent steps of the corruption process: DKL[qt(xt\u22121|xt, x0)||p\u03b8(xt\u22121|xt)]\nfort\u2208 {1, ..., T}, where qt(xt\u22121|xt, x0)is a Gaussian distribution whose mean and variance can\nbe computed in closed-form using Bayes rule and the definition in eq. (18) (Ho et al., 2020). The\ndenoising distribution p\u03b8(xt\u22121|xt)is parameterized to be Gaussian. The Kullback-Leibler diver-\ngence between two Gaussians admits a simple closed-form solution (Duchi John, 2020), hence, the\nobjective can be efficiently implemented.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "fb7fe99a-99e2-49d0-8898-12795e1f9d64": {"__data__": {"id_": "fb7fe99a-99e2-49d0-8898-12795e1f9d64", "embedding": null, "metadata": {"page_label": "29", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ac966a96-33f9-44c4-b8cc-b9e307acdc1f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a86406b1-82af-402a-abc4-13bd1c437960", "node_type": "4", "metadata": {"page_label": "29", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ac966a96-33f9-44c4-b8cc-b9e307acdc1f"}, "hash": "eedbf569cb60f750df4966a059b24896b19610fdf85285b54480272215aa15b7", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "79f87622-c272-43db-9a42-9ab3546f3f81", "node_type": "1", "metadata": {"page_label": "29", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ac966a96-33f9-44c4-b8cc-b9e307acdc1f"}, "hash": "08c35fe7bd6a2db3e691e1fe255143a0b4351be093db97def2a38d12b23b4919", "class_name": "RelatedNodeInfo"}}, "text": "(18) (Ho et al., 2020). The\ndenoising distribution p\u03b8(xt\u22121|xt)is parameterized to be Gaussian. The Kullback-Leibler diver-\ngence between two Gaussians admits a simple closed-form solution (Duchi John, 2020), hence, the\nobjective can be efficiently implemented.\nThe neural network used to parameterize p\u03b8(xt\u22121|xt)can be trained to learn different quantities\n(Luo, 2022; Salimans & Ho, 2022; Karras et al., 2022). A possible approach is to directly output\nthe mean \u00b5tofp\u03b8(xt\u22121|xt), while the variance is either fixed or learned as well. In this work, the\nneural network is parameterized to predict the noise added to the original sample during the forward\nprocess (eq. (18)). This is achieved by minimizing the objective w(t)\u2225\u03f5\u2212\u03f5\u03b8(\u03b1tx0+\u03c3t\u03f5, t)\u22252, where\nwis a weighting function (Ho et al., 2020). Once \u03f5\u03b8is trained, the mean of p\u03b8(xt\u22121|xt)is computed\nas1\n\u03b1t(xt\u2212\u03c3t\u03f5\u03b8)(Rombach et al., 2022). Since our primary goal is to analyze a pre-trained diffusion\nmodel, we refer the interested reader to Rombach et al. (2022); Luo (2022); Salimans & Ho (2022)\nfor more details.\nLatent diffusion. Originally, diffusion models operated directly on pixels (Ho et al., 2020; Song\n& Ermon, 2020). However, training a denoising network in pixel space is difficult and expensive\n(Hoogeboom et al., 2023). As such, Rombach et al. (2022) use a pre-trained auto-encoder to first\ncompress images, similar to VQGAN (Esser et al., 2021), and define a diffusion process in the latent\nspace instead of the pixel space. To make this difference clear they write p\u03b8(zt\u22121|zt), in which now\nztrefers to a noisy latent instead of a noisy image.\n29", "mimetype": "text/plain", "start_char_idx": 3030, "end_char_idx": 4642, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a024aba3-0db0-47aa-b4a0-e46b6ef6d96e": {"__data__": {"id_": "a024aba3-0db0-47aa-b4a0-e46b6ef6d96e", "embedding": null, "metadata": {"page_label": "30", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_061ad42a-6996-4ded-9e44-86ca48e8948a"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8c1c7397-49b4-487d-a322-cffaa0657796", "node_type": "4", "metadata": {"page_label": "30", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_061ad42a-6996-4ded-9e44-86ca48e8948a"}, "hash": "31bdaf630739fc713060ac36435151d0d9b3f16b757e5f4edeb5d7e8a3d17bf2", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nDistilled diffusion for fast inference. To speed-up inference of latent diffusion models, Sauer et al.\n(2023b) distill a pre-trained Stable Diffusion XL (SDXL) model (Podell et al., 2023). The distilled\nmodel is referred to as SDXL Turbo as it allows high-quality sampling in as little as 1-4 steps. The\noriginal SDXL model is trained with a noise schedule of 1000 steps, but in practice, sampling with\n20 to 50 steps still generates high-quality images. The speed-up in SDXL Turbo is achieved through\na combination of two objectives. First, Sauer et al. (2023b) define an adversarial game, similar to\nGANs (Goodfellow et al., 2014). The discriminator is implemented using lightweight classification\nheads on top of frozen features extracted at Kdifferent layers of a DINOv2 backbone (Oquab et al.,\n2024). Concretely, the objective of the discriminator is given by\nLD\nadv=Ex0\"KX\nk=1(1\u2212 D k(Fk(x0)))++\u03b3R1(\u03d5)#\n+E\u02c6x\u03b8\"KX\nk=1(1 +Dk(Fk(\u02c6x\u03b8)))+#\n, (19)\nwhere (x)+= max(0 , x)is the positive part, Fkdenotes the k-th features tensor from the DINOv2\nbackbone, Dkthek-th classification head, \u03d5is the discriminator parameters, R1is an L2penalty\nterm on the norm of the gradients, introduced by Mescheder et al. (2018) and \u03b3is a scalar hyperpa-\nrameter. Instead of a traditional classification loss, Sauer et al. (2023b) use the hinge loss (Lim &\nYe, 2017), following Sauer et al. (2021; 2023a).\nFinally, \u02c6x\u03b8represents the prediction of the diffusion model being distilled for the ground-truth image\nx0given a noisy sample xtobtained through the forward diffusion process defined in eq. (18). Sauer\net al. (2023b) found that distilling a diffusion model using the adversarial objective only resulted in a\nmodel with a FID of 20.8. To further improve performance, they also distilled the noise predictions\nof the teacher model. Importantly, both the teacher and student models were initialized with the\nsame pre-trained weights. After adversarial distillation (Sauer et al., 2023b), the model learns to\nmap noise to samples in one step\nNeural network architecture. The denoising network of SDXL Turbo estimating p\u03b8(zt\u22121|zt)is\nimplemented using a U-net similar to Rombach et al. (2022). The U-net is composed of a down-\nsampling path, a bottleneck, and an up-sampling path. Both the down-sampling and up-sampling\npaths are composed of 3 individual blocks. The individual block structure differs slightly but both\ndown- and up-sampling blocks consist of residual layers as well as cross-attention transformer\nblocks. Finally, the bottleneck layer is also composed of attention and residual layers. As for the\noriginal U-net architecture (Ronneberger et al., 2015), the corresponding blocks in the up-sampling\nand and down-sampling path are connected via a skip connection. Importantly, the text condition-\ning is achieved via cross-attention to text embeddings performed by in total 11 transformer blocks\nembedded in the down-, up-sampling paths and bottleneck. An architecture diagram displaying the\nrelevant blocks can be found in App. G Fig. 4.\n30", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3044, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5305d153-1ffa-4832-8953-5e288e691129": {"__data__": {"id_": "5305d153-1ffa-4832-8953-5e288e691129", "embedding": null, "metadata": {"page_label": "31", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cff3b99b-506d-4827-8ee9-ced5aeac4d06"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5a172d7-42d1-43b0-9427-ffc3d4ba378d", "node_type": "4", "metadata": {"page_label": "31", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cff3b99b-506d-4827-8ee9-ced5aeac4d06"}, "hash": "504aa2a7b58b98bbec004b5095d9abfa6bd88ffca089f76b1f4111d734d45e5a", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\n4998\nOrig.\n A. 1.0\n A. 2.0\n A. 3.0\n A. 4.0\n A. 5.0\n4074\n 2301\n 56\n 59\n 89\n(a)down.2.1\n4955\nOrig.\n A. 1.0\n A. 1.5\n A. 2.0\n A. 2.5\n A. 3.0\n4977\n 3718\n 90\n 1093\n 2165\n (b)up.0.1\nFigure 9: We turn on the features from Fig. 8 on three unrelated prompts \u201ca photo of a colorful\nmodel\u201d, \u201ca cinematic shot of a dog playing with a ball\u201d, and \u201ca cinematic shot of a classroom with\nexcited students\u201d.\n31", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 400, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2d5307ba-c64e-45d1-bdb0-d2816238928f": {"__data__": {"id_": "2d5307ba-c64e-45d1-bdb0-d2816238928f", "embedding": null, "metadata": {"page_label": "32", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e8c2fa99-52ac-427e-8622-e370f2178d2d"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f5ecf6f5-63a7-4960-a80d-cfb4b53530ce", "node_type": "4", "metadata": {"page_label": "32", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e8c2fa99-52ac-427e-8622-e370f2178d2d"}, "hash": "4d11cd9b714149cd7472f937ade24fb6832491161f1656db443e4f33147b21fe", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\n(a) Intervention history\n (b) Result\nFigure 10: Local edits showcase up.0.1 \u2019s ability to locally change textures in the image without\naffecting the remaining image. Multiple consecutive interventions are possible (a). The first in\n(a) row depicts the original image and each subsequent row we add an intervention by drawing a\nheatmap with a brush tool and then turning on the feature labelling the row only on that area. The\nother number (240) is the absolute feature strength of the edit. Figure (b) shows the final result in\nfull resolution (512x512).\n32", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 566, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ae1c0c83-f39a-4d9c-8263-504ee6d2fba1": {"__data__": {"id_": "ae1c0c83-f39a-4d9c-8263-504ee6d2fba1", "embedding": null, "metadata": {"page_label": "33", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f6842e1d-f441-4a99-bc40-bf321c479f36"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0e1b2da5-cf8c-4eb5-813e-b9dc1c6347f7", "node_type": "4", "metadata": {"page_label": "33", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f6842e1d-f441-4a99-bc40-bf321c479f36"}, "hash": "59da6ed26f60318d1fb3ba44b3accfe9145e4464f6af6ce0b841a9aa6ccaa2ea", "class_name": "RelatedNodeInfo"}}, "text": "Preprint\nFigure 11: The images used by GPT-4o to generate captions for feature 0. From top to bottom:\nirrelevant images to feature 0; image progression from left to right, showing increasing activation\nof SAE feature 0, with low activation on the left and high activation on the right; \u201cColdmaps\u201d\nrepresenting the image progression; images corresponding to the highest activation of feature 0;\n\u201cColdmaps\u201d corresponding to these highest activation images.\n33", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 457, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2eb48f09-db99-44e6-b38b-24bb34dadfd7": {"__data__": {"id_": "2eb48f09-db99-44e6-b38b-24bb34dadfd7", "embedding": null, "metadata": {"page_label": "1", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_592e52c6-f214-423c-99a4-f8cdecc03901"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "364fe5d6-468d-43f6-b7ca-5e5d7f053344", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_592e52c6-f214-423c-99a4-f8cdecc03901"}, "hash": "e7763cc2d90b535a953cba381c9c24e5f08aa032b6a1d66a871db64d27abaeb5", "class_name": "RelatedNodeInfo"}}, "text": "Building Postcards for \u201cAirbnb\u201d Scale\nLeo Wong\u00b7Follow\nPublished inThe Airbnb Tech Blog\n8 min read\u00b7Aug 28, 2024\nListen Share\nBy: Leo Wong, Henry Johnson\nHow the Airbnb Media team built group travel Postcards for the 2024 Summer\nRelease by leveraging a novel destination matching algorithm while advancing the\nplatform\u2019s image & localized text processing capabilities.\nAirbnb Postcards (see announcement).\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 1/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 595, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6edcdb2d-a66d-4607-9f2c-62d3e3c92e53": {"__data__": {"id_": "6edcdb2d-a66d-4607-9f2c-62d3e3c92e53", "embedding": null, "metadata": {"page_label": "2", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_21b5ca0a-1904-40b9-b047-851932c8fa9d"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9a27cd38-180c-4a9e-bdca-d5aead22ac1c", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_21b5ca0a-1904-40b9-b047-851932c8fa9d"}, "hash": "42ba2e1ca1a2c1496e0c061dcd2b434521600eda11741e74de8e47e65aee939a", "class_name": "RelatedNodeInfo"}}, "text": "Introduction\nFor Airbnb\u2019s 2024 Summer Release, the Media Ingestion team at Airbnb took on the\nexciting challenge of creating a reliable postcard generation system to generate\nunique, hand-crafted Postcards. Postcards are a beautiful way to invite guests on a\ntrip while keeping friends and family in the loop (see announcement). This feature\nrequired a novel solution to match relevant postcards to every possible destination\nthat guests booked on Airbnb. It needed to render performantly not only on all our\nclient platforms (iOS, Android, and Web), but also on different messaging platforms\noutside the Airbnb app, all while maintaining Airbnb\u2019s high design standards.\nChallenges\nLocalized Text Layout: Postcards have strict design guidelines around character\ncount per line, font leading & kerning per language, pixel perfect typography,\nline break rules, and language-specific styling.\nDesign & Product Flexibility: Text layout, color, fonts, text drop shadows and\nimage transformations need to be flexible for product & design changes.\nDestination M atching: Postcards need to match the destination by including\nrelevant artwork and localized destination names.\nAvailability On and Off Platform : Assets need to be surfaced on and off the\nplatform, which necessitated a pre-generated server-side solution; client-only\nsolutions wouldn\u2019t work since we needed Open Graph compatible links for assets\nto render properly in iMessage and Instagram, for example.\nPerform ance: Postcard presentation shouldn\u2019t interrupt the product UX by\ntaking significant time to render.\nOpen in app Sign up Sign in\nSearch\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 2/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e1155e61-e1f7-4284-a093-cbeb41075056": {"__data__": {"id_": "e1155e61-e1f7-4284-a093-cbeb41075056", "embedding": null, "metadata": {"page_label": "3", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b2d64afc-af82-4489-a03a-3cbc544e2bb2"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "971c53b4-edc5-45a3-8b57-5d85e1db9016", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b2d64afc-af82-4489-a03a-3cbc544e2bb2"}, "hash": "b29062110faa4df455293b31b2ff6a6865524769986f51f45d182ee445e2ac15", "class_name": "RelatedNodeInfo"}}, "text": "Postcards \u201cin vs. out\u201d of product experience\nSolution\nPostcard Setup\nA Trips Postcard is the combination of (1) an artwork illustration, (2) a postcard\ntemplate, and (3) a localized destination. A Postcard \u201ctemplate\u201d is a data entity with\na reference to the artwork illustration plus some additional metadata to describe\nhow to render it. To render a Postcard in product, we need to have all artwork\nillustrations, postcard templates, and formatted localized destinations set up before\na visitor sees the product flow.\nIn the example below, we have an illustration of a cliff side, a postcard template, and\nan English language variant of the destination name \u201cGalway\u201d. The Postcard\ntemplate includes parameters to specify how to create the postcard, like text and\nBelo (Airbnb brand icon) color and positioning. In the example, the text and Belo\nare rendered in gray color and positioned at the bottom and top left, respectively.\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 3/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1121, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c100cfec-79f2-4ec3-81bc-cea2c15c8c04": {"__data__": {"id_": "c100cfec-79f2-4ec3-81bc-cea2c15c8c04", "embedding": null, "metadata": {"page_label": "4", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9407bd7c-5277-40e4-821a-8b5d14da8f74"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "70e098a7-cb12-49e9-90f0-6f318b1cec1e", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9407bd7c-5277-40e4-821a-8b5d14da8f74"}, "hash": "df0b99b893ef25191001150b0d5da3b2c4139e5c05ff1dab7f77b9a2dd5786cd", "class_name": "RelatedNodeInfo"}}, "text": "Illustration + Template Params + Localized Destination = Postcard\nDesign Flexibility \u2014 Postcard Templates\nTo accommodate changing design requirements, we built a flexible template data\nmodel that empowers our design team to configure various parameters like text\npositioning and text color while iterating on the designs. The postcard templates\ninclude all the metadata required to generate a Postcard and its surrounding\npresentation elements. It also includes a versioning capability so that we can\npublish changes to all users with a version bump whenever we have a design\nrevision or visual defect.\nTemplate & Artwork Upload, Management & Preview\nTo make it easy for the creative team to self-serve and debug issues, we built a web-\nbased internal tool for creating and managing templates, previewing postcards, and\nuploading artwork. This tool made it significantly easier to manage templates, and\nwas especially useful during the team\u2019s peak iteration period where we were\nconstantly fixing bugs and changing designs.\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 4/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1215, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bb4d7243-b0df-418d-bc36-d6f0082837ae": {"__data__": {"id_": "bb4d7243-b0df-418d-bc36-d6f0082837ae", "embedding": null, "metadata": {"page_label": "5", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_634955b9-16f9-42bc-8606-798c64281721"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "69dd8665-df26-427f-8e7e-6ecd450afeed", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_634955b9-16f9-42bc-8606-798c64281721"}, "hash": "91797361092b5a0e1068a455df20138541b2f9b4c2e717c2fec7c177e3c616b3", "class_name": "RelatedNodeInfo"}}, "text": "The postcard template creation & management form\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 5/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 240, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "392bf67f-87c0-4314-be10-85214fbdf33f": {"__data__": {"id_": "392bf67f-87c0-4314-be10-85214fbdf33f", "embedding": null, "metadata": {"page_label": "6", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8bb4c4bf-7aaf-49f3-910e-115c3c465d86"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "015df7ce-1350-4eb1-be49-202f048f1d8c", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8bb4c4bf-7aaf-49f3-910e-115c3c465d86"}, "hash": "fdb56d3bfe27718c3e625c800b1aefe32c00417ad3a45e00dad636bcdfba85c5", "class_name": "RelatedNodeInfo"}}, "text": "In the setup workflow, our operations team created Postcard templates, uploaded artwork, and managed them via\nthe template form page.\nLocalized Text Layout\nWe wanted accurate translations of destinations that were properly formatted for\neach localized postcard. A programmatic solution for localized text layout would\nrequire, at the very least, language specific rules (right to left, word wrapping, etc.),\nknowledge of cultural conventions, accessibility considerations, and text rendering\nfor special characters (diacritics, etc.). This would make business logic complex and\nbrittle.\nWe compromised on this solution with the localization (l10n) team by manually\nformatting translations for destinations that made up our top booking destinations.\nThese manually formatted translations involved getting our l10n scaled operations\nteam to translate and format (line breaks, layout spacing, etc.) a shortlist of localized\ndestinations, which we helped ingest into our typical i18n platform translated text\nworkflow with some scripting. After ingestion, the Postcard generation system pulls\nfrom our i18n platform to get the desired localized formatted text layout for each\nPostcard.\nThe destinations shortlist was informed by our data science team, who helped\ngather the top booked destinations by language. This reduced the scale of required\npostcards to generate from all destinations in the full set of language locale\ndestination combinations on the platform, to a magnitude of scale smaller\ndestination subset. As a result, postcard QA was significantly easier. More\nimportantly, it kept the overall system code and maintenance simple; no need for\nthousands of lines of language specific business logic!2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 6/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1898, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2e651499-18f1-4274-b19f-84d2890907a0": {"__data__": {"id_": "2e651499-18f1-4274-b19f-84d2890907a0", "embedding": null, "metadata": {"page_label": "7", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_dd33f46c-5fb3-4729-8398-d8be096ece33"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f91b6639-de85-4e31-a5af-2f4e1fe81c63", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_dd33f46c-5fb3-4729-8398-d8be096ece33"}, "hash": "4059c24c574493f5a3e905b5cfb7d0e716e30302f2e63dd2ed02166ec0b4ea47", "class_name": "RelatedNodeInfo"}}, "text": "The rest of the postcards that didn\u2019t make the top destinations list followed a simple\nformula of word count and line breaks per language (e.g. Chinese, Korean, and\nJapanese had smaller word count limits because of the character size and no line\nbreak on spaces because it changes the meaning of the destination).\nEarly design prototype of text layout in a subset of the languages we support at Airbnb. The screenshot is for\nillustration purposes only.\nPostcard Generation\nDestination Matching\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 7/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 685, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6db20671-1b6d-46af-bf8d-8cb72265d3ef": {"__data__": {"id_": "6db20671-1b6d-46af-bf8d-8cb72265d3ef", "embedding": null, "metadata": {"page_label": "8", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_eb89ecd3-58bc-44d7-ada1-0c9fd72c3411"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6a80cee8-0c5c-413c-80c4-41ce5fda37c5", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_eb89ecd3-58bc-44d7-ada1-0c9fd72c3411"}, "hash": "15866950868fc0a0148239aec31c4cbdf385004d349d12a3830140af20a7c6a2", "class_name": "RelatedNodeInfo"}}, "text": "Decision tree for determining the best postcard template for a given reservation\nTo find the best-matching postcard template for each destination, we have a\nmatching algorithm that matches templates to reservations at booking time using\nfour different criteria:\n1. By listing \u2014 We wanted to be able to support listing-specific artwork. E.g. Our\nIcons listings show a golden ticket to commemorate the special moment when a\nguest wins the lottery for staying at an Icons listing.\n2. By destination \u2014 For popular destinations (matching by city and country), we\nhave curated artwork that showcases both a local artist and the destination. E.g.\nTrips to Santorini present the iconic Cycladic domes of Santorini as artwork (see\ndiagram below).\n3. By taxonom y \u2014 For all other artwork, we match destinations based on a set of\ntaxonomy tags. We partnered with the knowledge graph team to apply taxonomy\nattributes to all of our listings in a few different categories: density (i.e.\nmetropolitan, urban), climate (i.e. tropical, temperate) and geography (i.e.\ncoastal, mountain, river). We ensured the taxonomy was accurate by cross\nreferencing existing internal data and the expertise of our regional\nrepresentatives teams. They then exposed an API that we called to fetch\ntaxonomy by listing. On the operations side, our production team created\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 8/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1529, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0b5e25da-73d5-4cd7-bad2-5ae3861eaeae": {"__data__": {"id_": "0b5e25da-73d5-4cd7-bad2-5ae3861eaeae", "embedding": null, "metadata": {"page_label": "9", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_40f69057-c9cb-4cfd-a7eb-95a0e5c6fc96"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0598a94-87e7-4371-b1bc-57cb43d7f170", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_40f69057-c9cb-4cfd-a7eb-95a0e5c6fc96"}, "hash": "787f9ea4fadc1f8c10acca49be3a28b0a62a17ac0e23626d8f894d18922c3127", "class_name": "RelatedNodeInfo"}}, "text": "taxonomy tagged artwork (e.g. an artwork tagged to be used for a coastal,\ntemperate, metropolitan postcard). When generating postcards, we match the\nlisting to the artwork with the highest number of overlapping tags.\n4. By default \u2014 If a destination isn\u2019t covered by the above categories, we show a\nfallback default artwork.\nThe 4 initial Postcard variants: Icons, Destination specific, Taxonomy, and Default.\nFormatted Translations\nWe take the listing of each booking request and fetch the city and country from the\nlisting service and check to see if that destination was in our curated set of\nformatted destinations loaded into our i18n service. We then take the best fitting\nartwork and embed the localized destination text on it to generate the final\npostcard. If we don\u2019t get a translation back, we fall back to serving the postcard\nwithout text.\nPerformance \u2014 Async Postcard Creation Flow\nPutting a localized destination and a Belo icon onto artwork is a time-consuming\noperation given the high resolution artwork we used. We knew the image processing\nflow could take over 8 seconds on average to process an image so we needed to\ncome up with a way to make our postcard API respond quickly. We also wanted to\ntransfer these generated postcards into our primary image storage so we could\nleverage our existing media serving infrastructure, which introduced an additional\n1\u20132 seconds of latency.\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 9/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1592, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8f00621d-ed16-4187-ae96-18c225662b4f": {"__data__": {"id_": "8f00621d-ed16-4187-ae96-18c225662b4f", "embedding": null, "metadata": {"page_label": "10", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4d053012-d5a1-4a0b-977b-dcedc8a5a2e0"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc9f1828-9234-4f3e-a26a-5e3c15eecbb0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4d053012-d5a1-4a0b-977b-dcedc8a5a2e0"}, "hash": "109cc4d227749c62ad6df9f0fd5b1f0e3546346a360666cb8492e74e2397c3db", "class_name": "RelatedNodeInfo"}}, "text": "In order to still be performant, we went with a partly asynchronous approach\nwhere, during the live in product request, we only serve postcards that we\u2019ve already\ngenerated and stored internally. If there was a request for a new postcard, we would\ninstead return a fallback postcard and publish an event to a Kafka queue where an\nasync consumer would call the processing service, wait for the asset to be generated\nand then transfer it into our system to be used for future requests.\nAs shown in the diagram below, we fetched the listing information and taxonomy\ninformation in parallel before computing the best matching artwork for the trip.\nBased on a pattern in how the postcards are stored, we would check in our media\nservice to see if the postcard was generated already before either returning the card\nif it was found or kicking off the asynchronous flow if it was not found. At that point,\nour media service\u2019s Kafka consumer would complete the flow by transforming the\nasset into a postcard and storing it in our system.2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 10/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1221, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "512269f4-6819-4397-85db-73fd4dc2c89e": {"__data__": {"id_": "512269f4-6819-4397-85db-73fd4dc2c89e", "embedding": null, "metadata": {"page_label": "11", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0efbe367-121c-42ae-95f8-68e918d56d8c"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b71ba31a-08d2-42b9-987b-f673b7df3fd1", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0efbe367-121c-42ae-95f8-68e918d56d8c"}, "hash": "9c8dccee9c42dcdfdc06e849dd54156b55d9ef893462c8bf977880e739da7919", "class_name": "RelatedNodeInfo"}}, "text": "Architecture Diagram of the Backend Postcard Generation flow\nPre-generation\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 11/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 268, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "267f7280-d569-43ee-9df0-d219764f6d73": {"__data__": {"id_": "267f7280-d569-43ee-9df0-d219764f6d73", "embedding": null, "metadata": {"page_label": "12", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_583b77f7-7f29-49b9-b0c0-7b391142c7c0"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "29354eef-f596-4838-ab17-22df58635013", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_583b77f7-7f29-49b9-b0c0-7b391142c7c0"}, "hash": "20dac8ff89c0815771bcad7ec26efb901b7eeae16ac126618b127049ddb59e8c", "class_name": "RelatedNodeInfo"}}, "text": "We wanted to generate as many of the postcards as possible before the launch. If the\npostcard hasn\u2019t been generated when a guest books a group trip, everyone on the\nbooking will see the default, generic postcard. Our data science team helped\ndetermine top destinations, and we ran those inputs through our postcard\ngeneration pipeline to pre-generate as many postcards as possible and minimize the\nchance of falling back to a default postcard. Within a week of launching, more than\n90% of trips had a custom postcard instead of a default and we inched closer to\ngenerating a postcard for all trips in the months after.\nDashboard numbers on Postcard template selection type and postcard usage hit rate. For illustrative\npurposes only, not real data.\nConclusion\nCreating postcards was a massive effort that required collaboration across multiple\nengineering, product, design, and data science teams to improve Airbnb\u2019s group\ntravel feature. Our frontline insights team continues to receive positive social media\nand external feedback on this update that adds delight to joining a group trip.\nThe solution highlights the importance of having the right internal tooling, image\nand text processing capabilities, and destination matching logic for solving\nsomething at Airbnb\u2019s scale.\nPostcards is one of the first major image processing use cases that the Media team\nbuilt to support a new Airbnb feature. It highlights the power of media capabilities\nand innovative features we can build with them. If you like the type of work we do at\nAirbnb, please contact us & check out our careers page!\nAcknowledgments\nThanks to the following engineers who helped to build this feature: Alan Wright,\nAditya Punjani, Bill Lovotti, Jessica Chen, Miguel Jimenez\nImage Processing Media Processing Internationalization Engineering2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 12/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2003, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "916dca71-aa48-4099-9dbd-d856490fa3b5": {"__data__": {"id_": "916dca71-aa48-4099-9dbd-d856490fa3b5", "embedding": null, "metadata": {"page_label": "13", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b18ed6b3-360a-4adc-b31c-1773ec62b9b4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f4c2ed1a-eb63-44e4-9521-49234f677153", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b18ed6b3-360a-4adc-b31c-1773ec62b9b4"}, "hash": "52e989e4a15e6970c684619eac3060379f0d43c0081e198f30194d96c6b3c50b", "class_name": "RelatedNodeInfo"}}, "text": "Follow\nWritten by Leo Wong\n62 Followers\u00b7Writer for The Airbnb Tech Blog\nNi-howdy\nMore from Leo Wong and The Airbnb Tech BlogFront End Development\nWhat is Redux?\nYou might have heard of this very popular Javascript library called Redux. You might even think\nthat you really need to use it. In this\u2026\nLeo Wong2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 13/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 498, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "958a3879-74c7-4334-a515-cd24fe969dad": {"__data__": {"id_": "958a3879-74c7-4334-a515-cd24fe969dad", "embedding": null, "metadata": {"page_label": "14", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_133a3d20-201c-4a89-9bc4-63799e182343"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "cfd7b93e-cd0f-412b-b8cc-aa99b789983b", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_133a3d20-201c-4a89-9bc4-63799e182343"}, "hash": "d01ad1eaa7877726cd4da94f8a2d69f6ea9441800ee5f1470b3df587bac24474", "class_name": "RelatedNodeInfo"}}, "text": "May 9, 2019\nin\nHow Airbnb Smoothly Upgrades React\nIncrementally modernizing our frontend infrastructure to roll out the latest React features\nwithout downgrades\nJul 23\nin30\nAndre Wiggins The Airbnb Tech Blog\n704 7\nJon Chew The Airbnb Tech Blog2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 14/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 435, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ef5fe84-b6ae-4bac-b997-179506dd931c": {"__data__": {"id_": "5ef5fe84-b6ae-4bac-b997-179506dd931c", "embedding": null, "metadata": {"page_label": "15", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_02b19295-6f75-46b6-90a9-3485108a8073"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "342d9846-0f4c-49be-8709-a782cfef0dc7", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_02b19295-6f75-46b6-90a9-3485108a8073"}, "hash": "d035efd7768237980ada8b3eb4bc4f05c9c9f0188588c3c1909a1151ad0039ea", "class_name": "RelatedNodeInfo"}}, "text": "See all from Leo Wong\nSee all from The Airbnb Tech Blog\nRecommended from MediumAvoiding Double Payments in a Distributed Payments System\nHow we built a generic idempotency framework to achieve eventual consistency and\ncorrectness across our paym ents micro-service\u2026\nApr 16, 2019\nJust 10 Minutes\nIt may seem like very little time, but you\u2019d be surprised what you can accomplish if you just\ninvest a bit of time every day into\u2026\nMay 17, 20193.7K 41\nLeo Wong\n592024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 15/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "62f85a7f-b2ed-4ad0-8c46-28912c5d2bcc": {"__data__": {"id_": "62f85a7f-b2ed-4ad0-8c46-28912c5d2bcc", "embedding": null, "metadata": {"page_label": "16", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ecb9bdbd-d0e9-4961-b38e-660962731312"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "036f2df1-83bd-4fbf-ac24-574f990eac25", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ecb9bdbd-d0e9-4961-b38e-660962731312"}, "hash": "3d7859eeca72e861722b847c4497431b76e56e85c1b74a8d765cab86c6c489b8", "class_name": "RelatedNodeInfo"}}, "text": "in\nGitHub: Pull Request Template\nHelp code developers write better pull request descriptions\n4d ago\nin\nRiverbed: Optimizing Data Access at Airbnb\u2019s Scale\nAn overview of Airbnb\u2019s Data Framework for faster and more reliable read-heavy workloads.\nJul 25, 2023\nZolzaya Luvsandorj Towards Data Science\n69 1\nAmre Shakim The Airbnb Tech Blog\n293 22024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 16/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 532, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0328018c-58a3-4764-86e4-e82f0ba70e9c": {"__data__": {"id_": "0328018c-58a3-4764-86e4-e82f0ba70e9c", "embedding": null, "metadata": {"page_label": "17", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_adaa99b0-1bce-4792-85e0-bd125163fe35"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06193f4e-08e3-4391-bee1-b5c3267519fe", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_adaa99b0-1bce-4792-85e0-bd125163fe35"}, "hash": "0d1679e3f71f30b8024445f7dfbafbab6e3ccc615038c3b2e8a475fb711ba38d", "class_name": "RelatedNodeInfo"}}, "text": "Lists\nLeadership\n61 stories\u00b7475 saves\nLeadership upgrades\n7 stories\u00b7109 saves\nStories to Help You Grow as a Software Developer\n19 stories\u00b71452 saves\nGood Product Thinking\n12 stories\u00b7735 saves\nin\nHow to Showcase Your Value Without Bragging\nValue creation doesn\u2019t happen overnight, and you can\u2019t force others to see your worth\n3d ago\nVinita Code Like A Girl\n468 11\n2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 17/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 555, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f5021a88-0bee-4a68-bad4-e0b5d9853244": {"__data__": {"id_": "f5021a88-0bee-4a68-bad4-e0b5d9853244", "embedding": null, "metadata": {"page_label": "18", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5538190e-4cdc-418f-9564-7414fae0e98a"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c5c59789-93f5-4713-b9f9-15f156b3474a", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5538190e-4cdc-418f-9564-7414fae0e98a"}, "hash": "c5fae72937f5a856698b8273c5000a855a3c57b6cd1ac7a04975060f7c0d762f", "class_name": "RelatedNodeInfo"}}, "text": "in\nFlipkart Enhances AI Safety in E-Commerce: Implementing NVIDIA NeMo\nGuardrails\nIntroduction\nSep 11\nin\nImplementing and Leveraging React Server Components\nIn the last blog, we explored what React Server Components (RSCs) are, why they were\nintroduced, and what makes them so exciting. If you\u2026\nSantosh Singh Flipkart Tech Blog\n84 1\nRaj Varsani Groww Engineering2024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 18/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 554, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d6240849-f445-43d9-b426-56f05f6fe309": {"__data__": {"id_": "d6240849-f445-43d9-b426-56f05f6fe309", "embedding": null, "metadata": {"page_label": "19", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_de7c5241-ed5b-47dc-9bb1-a9f30f89c5cd"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4b65baf1-aeae-4d42-be33-a2ad8710486b", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_de7c5241-ed5b-47dc-9bb1-a9f30f89c5cd"}, "hash": "706c7ef90060a8284f43becc888fd668ca5d1442fa9fefadabc56e95693725c1", "class_name": "RelatedNodeInfo"}}, "text": "See more recomme ndationsJul 22\nin\nUnlocking 66% Faster Experiments with Covariate Adjustment: Power\nAnalysis for Clustered Data\u2026\nTim Hesterberg and Benjamin Knight\nJun 17392 1\nBenjamin Knight tech-at-instacart\n822024/11/2 18:07 Building Postcards for \u201cAirbnb\u201d Scale | by Leo Wong | The Airbnb Tech Blog | Medium\nhttps://medium.com/airbnb-engineering/building-postcards-for-airbnb-scale-dfe0b71b12ec 19/19", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 405, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ebd50a2-ebff-42a1-9235-38c7d09891cc": {"__data__": {"id_": "8ebd50a2-ebff-42a1-9235-38c7d09891cc", "embedding": null, "metadata": {"page_label": "1", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f722e853-cac0-4259-8d5c-0c5e4517284e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4953d18c-2de0-4893-a83a-d257a78a1906", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f722e853-cac0-4259-8d5c-0c5e4517284e"}, "hash": "466305e816ff32146932e03fffecc958f797998a24943e4b13d94685d7b3b82e", "class_name": "RelatedNodeInfo"}}, "text": "\uf0d1\uef3b\nRefactoringAgileArchitectureAboutThoughtworks\uf09e\uf23a\nData Mesh Principles and Logical Architecture\nOur aspiration to augment and improve every aspect of business and life with data,\ndemands a paradigm shift in how we manage data at scale. While the technology\nadvances of the past decade have addressed the scale of volume of data and data\nprocessing compute, they have failed to address scale in other dimensions: changes in the\ndata landscape, proliferation of sources of data, diversity of data use cases and users, and\nspeed of response to change. Data mesh addresses these dimensions, founded in four\nprinciples: domain-oriented decentralized data ownership and architecture, data as a\nproduct, self-serve data infrastructure as a platform, and federated computational\ngovernance. Each principle drives a new logical view of the technical architecture and\norganizational structure.\n03 December 20202024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 1/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1027, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "091ba85c-fac6-4b60-9034-aadfde27e120": {"__data__": {"id_": "091ba85c-fac6-4b60-9034-aadfde27e120", "embedding": null, "metadata": {"page_label": "2", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_70ddcae2-59df-4e97-8a53-fd13bd0865ed"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "27ccebda-b20a-4a9b-8082-7d324e60e6b2", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_70ddcae2-59df-4e97-8a53-fd13bd0865ed"}, "hash": "385a11f4f3400e4537513b4d5d179ec096a5bfbac1bbdb3c6540185db43cb29b", "class_name": "RelatedNodeInfo"}}, "text": "Zhamak Dehghani\nZhamak is the director of emerging\ntechnologies at Thoughtworks North\nAmerica with focus on distributed systems\narchitecture and a deep passion for\ndecentralized solutions. She is a member of\nThoughtworks Technology Advisory Board\nand contributes to the creation of\nThoughtworks Technology Radar.\n\uf023DATA ANALYTICS\n\uf023DATA MESHCONTENTS\nThe great divide of data\nCore principles and logical architecture of data mesh\nDomain Ownership\nLogical architecture: domain-oriented data and\ncompute\nData as a product\nLogical architecture:data product the architectural\nquantum\nSelf-serve data platform\nLogical architecture: a multi-plane data platform\nFederated computational governance\nLogical architecture: computational policies\nembedded in the mesh\nPrinciples Summary and the high level logical architecture\nFor more on Data Mesh, Zhamak went on to\nwrite a full book that covers more details on\nstrategy, implementation, and organizational\ndesign.\nThe original writeup, How to Move Beyond a Monolithic Data Lake to a Distributed\nData Mesh - which I encourage you to read before joining me back here - empathized\nwith today\u2019s pain points of architectural and organizational challenges in order to\nbecome data-driven, use data to compete, or use data at scale to drive value. It offered\nan alternative perspective which since has captured many organizations\u2019 attention, and\ngiven hope for a different future. While the original writeup describes the approach, it\nleaves many details of the design and implementation to one\u2019s imagination. I have no\nintention of being too prescriptive in this article, and kill the imagination and creativity2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 2/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1768, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d5db3e43-6cbc-4789-938b-9e7c8f89ae61": {"__data__": {"id_": "d5db3e43-6cbc-4789-938b-9e7c8f89ae61", "embedding": null, "metadata": {"page_label": "3", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9ece0034-ace3-4fe0-8449-32a8a60afbdc"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1e47060c-0d14-4de2-9b80-a1a54f4c0239", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9ece0034-ace3-4fe0-8449-32a8a60afbdc"}, "hash": "b6dae6fb54c5ad13f4995d1a1264706cbbee9cd64be42a2b84379f6040758666", "class_name": "RelatedNodeInfo"}}, "text": "around data mesh implementation. However I think it\u2019s only responsible to clarify the\narchitectural aspects of data mesh as a stepping stone to move the paradigm forward.\nThis article is written with the intention of a follow up. It summarizes the data mesh\napproach by enumerating its underpinning principles, and the high level logical\narchitecture that the principles drive. Establishing the high level logical model is a\nnecessary foundation before I dive into detailed architecture of data mesh core\ncomponents in future articles. Hence, if you are in search of a prescription around\nexact tools and recipes for data mesh, this article may disappoint you. If you are\nseeking a simple and technology-agnostic model that establishes a common language,\ncome along.\nThe great divide of data\nWhat do we really mean by data? The answer depends on whom you ask. Today\u2019s\nlandscape is divided into operational data and analytical data. Operational data sits in\ndatabases behind business capabilities served with microservices, has a transactional\nnature, keeps the current state and serves the needs of the applications running the\nbusiness. Analytical data is a temporal and aggregated view of the facts of the business\nover time, often modeled to provide retrospective or future-perspective insights; it\ntrains the ML models or feeds the analytical reports.\nThe current state of technology, architecture and organization design is re\u0000ective of\nthe divergence of these two data planes - two levels of existence, integrated yet\nseparate. This divergence has led to a fragile architecture. Continuously failing ETL\n(Extract, Transform, Load) jobs and ever growing complexity of a labyrinth of data\npipelines, is a familiar sight to many who attempt to connect these two planes, \u0000owing\ndata from operational data plane to the analytical plane, and back to the operational\nplane.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 3/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1998, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5bb13d0a-0c0e-4d41-8637-759b7b4b84fe": {"__data__": {"id_": "5bb13d0a-0c0e-4d41-8637-759b7b4b84fe", "embedding": null, "metadata": {"page_label": "4", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a2fc8a19-a7f1-4ece-a6de-991f6df6922b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d6fcc8e6-9c1e-4d53-b521-96e0516b3fc0", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a2fc8a19-a7f1-4ece-a6de-991f6df6922b"}, "hash": "196e18b382da3d196a322a33adcbe784d258f7b775b7f3b0e9925f128d1b6d56", "class_name": "RelatedNodeInfo"}}, "text": "Figure 1\u0000 The great divide of data\nAnalytical data plane itself has diverged into two main architectures and technology\nstacks: data lake and data warehouse; with data lake supporting data science access\npatterns, and data warehouse supporting analytical and business intelligence reporting\naccess patterns. For this conversation, I put aside the dance between the two\ntechnology stacks: data warehouse attempting to onboard data science work\u0000ows and\ndata lake attempting to serve data analysts and business intelligence. The original\nwriteup on data mesh explores the challenges of the existing analytical data plane\narchitecture.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 4/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 757, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d98277da-972a-47da-a4db-dbb14fd4a2a0": {"__data__": {"id_": "d98277da-972a-47da-a4db-dbb14fd4a2a0", "embedding": null, "metadata": {"page_label": "5", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_571f41af-d4ec-4e2f-9f38-4255538d32dd"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a0aa37b6-4652-40cb-b4a1-4b1265a7d405", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_571f41af-d4ec-4e2f-9f38-4255538d32dd"}, "hash": "4dfedc8d100742bfacd8f9bf72bb81a6856d234d4c7155fad0b0ebb18d67d6c5", "class_name": "RelatedNodeInfo"}}, "text": "Figure 2\u0000 Further divide of analytical data - warehouse2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 5/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 181, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4da2745e-0b0d-4c74-b0e6-061629287d5c": {"__data__": {"id_": "4da2745e-0b0d-4c74-b0e6-061629287d5c", "embedding": null, "metadata": {"page_label": "6", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c76ec4e2-de2e-48f7-bae7-501559af5016"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1aeae644-84ac-4441-bfa7-5454889b28a2", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c76ec4e2-de2e-48f7-bae7-501559af5016"}, "hash": "a2956b13324132ee3f0ab02ddc06176b77f7a2be943c52aa0bf454b9e40fc31c", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3\u0000 Further divide of analytical data - lake\nData mesh recognizes and respects the differences between these two planes: the\nnature and topology of the data, the differing use cases, individual personas of data\nconsumers, and ultimately their diverse access patterns. However it attempts to\nconnect these two planes under a different structure - an inverted model and topology\nbased on domains and not technology stack - with a focus on the analytical data plane.\nDifferences in today's available technology to manage the two archetypes of data,\nshould not lead to separation of organization, teams and people who work on them. In\nmy opinion, the operational and transactional data technology and topology is\nrelatively mature, and driven largely by the microservices architecture; data is hidden\non the inside of each microservice, controlled and accessed through the microserivce\u2019s\nAPIs. Yes there is room for innovation to truly achieve multi-cloud-native operational\ndatabase solutions, but from the architectural perspective it meets the needs of the\nbusiness. However it\u2019s the management and access to the analytical data that remains a\npoint of friction at scale. This is where data mesh focuses.\nI do believe that at some point in the future our technologies will evolve to bring these\ntwo planes even closer together, but for now, I suggest we keep their concerns\nseparate.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 6/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1514, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2644b374-3d3e-4b54-abcf-7eda9786abaa": {"__data__": {"id_": "2644b374-3d3e-4b54-abcf-7eda9786abaa", "embedding": null, "metadata": {"page_label": "7", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d9f139cf-88ea-41e8-a414-428506e128af"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ff3c6086-ca55-4930-9659-6fed26414247", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d9f139cf-88ea-41e8-a414-428506e128af"}, "hash": "b5db2bcfedd0900a09df32b374e85109e0aea989bfcc846bde61f4756093585b", "class_name": "RelatedNodeInfo"}}, "text": "Core principles and logical architecture of data mesh\nData mesh objective is to create a foundation for getting value from analytical data and\nhistorical facts at scale - scale being applied to constant change of data landscape,\nproliferation of both sources of data and consumers, diversity of transformation and\nprocessing that use cases require, speed of response to change. To achieve this objective, I\nsuggest that there are four underpinning principles that any data mesh\nimplementation embodies to achieve the promise of scale, while delivering quality and\nintegrity guarantees needed to make data usable : 1) domain-oriented decentralized\ndata ownership and architecture, 2) data as a product, 3) self-serve data infrastructure\nas a platform, and 4) federated computational governance.\nWhile I expect the practices, technologies and implementations of these principles\nvary and mature over time, these principles remain unchanged.\nI have intended for the four principles to be collectively necessary and suf\u0000cient; to\nenable scale with resiliency while addressing concerns around siloing of incompatible\ndata or increased cost of operation. Let's dive into each principle and then design the\nconceptual architecture that supports it.\nDomain Ownership\nData mesh, at core, is founded in decentralization and distribution of responsibility to\npeople who are closest to the data in order to support continuous change and\nscalability. The question is, how do we decompose and decentralize the components of\nthe data ecosystem and their ownership. The components here are made of analytical\ndata, its metadata, and the computation necessary to serve it.\nData mesh follows the seams of organizational units as the axis of decomposition. Our\norganizations today are decomposed based on their business domains. Such\ndecomposition localizes the impact of continuous change and evolution - for the most\npart - to the domain\u2019s bounded context. Hence, making the business domain\u2019s bounded\ncontext a good candidate for distribution of data ownership.\nIn this article, I will continue to use the same use case as the original writeup, \u2018a digital\nmedia company\u2019. One can imagine that the media company divides its operation, hence\nthe systems and teams that support the operation, based on domains such as2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 7/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2423, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d83c8768-a112-45c0-9791-99ac24e81c4f": {"__data__": {"id_": "d83c8768-a112-45c0-9791-99ac24e81c4f", "embedding": null, "metadata": {"page_label": "8", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a7234322-a32a-4e75-94e3-b79fe17c6485"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2e371e51-35fd-45ff-b21a-3bbd71734641", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a7234322-a32a-4e75-94e3-b79fe17c6485"}, "hash": "9a5ab0186743377bac8aa73b00f416626dca63cb8d3cea0b5334668be3908155", "class_name": "RelatedNodeInfo"}}, "text": "\u2018podcasts\u2019, teams and systems that manage podcast publication and their hosts; \u2018artists\u2019,\nteams and systems that manage onboarding and paying artists, and so on. Data mesh\nargues that the ownership and serving of the analytical data should respect these\ndomains. For example, the teams who manage \u2018podcasts\u2019, while providing APIs for\nreleasing podcasts, should also be responsible for providing historical data that\nrepresents \u2018released podcasts\u2019 over time with other facts such as \u2018listenership\u2019 over\ntime. For a deeper dive into this principle see Domain-oriented data decomposition\nand ownership.\nLogical architecture: domain-oriented data and compute\nTo promote such decomposition, we need to model an architecture that arranges the\nanalytical data by domains. In this architecture, the domain\u2019s interface to the rest of\nthe organization not only includes the operational capabilities but also access to the\nanalytical data that the domain serves. For example, \u2018podcasts\u2019 domain provides\noperational APIs to \u2018create a new podcast episode\u2019 but also an analytical data endpoint\nfor retrieving \u2018all podcast episodes data over the last <n> months\u2019. This implies that the\narchitecture must remove any friction or coupling to let domains serve their analytical\ndata and release the code that computes the data, independently of other domains. To\nscale, the architecture must support autonomy of the domain teams with regard to the\nrelease and deployment of their operational or analytical data systems.\nThe following example demonstrates the principle of domain oriented data ownership.\nThe diagrams are only logical representations and exemplary. They aren't intended to\nbe complete.\nEach domain can expose one or many operational APIs, as well as one or many\nanalytical data endpoints\nFigure 4\u0000 Notation: domain, its analytical data and operational capabilities2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 8/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1987, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e4a91b6b-8e83-4992-bb97-2b2a39ac06e1": {"__data__": {"id_": "e4a91b6b-8e83-4992-bb97-2b2a39ac06e1", "embedding": null, "metadata": {"page_label": "9", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cfd8e9d9-8ceb-4d4f-b57b-7ce4a07e5a1a"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "95cadd8f-bad4-4c54-9f56-c41bc2c99ead", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cfd8e9d9-8ceb-4d4f-b57b-7ce4a07e5a1a"}, "hash": "2539f92834c7942864d582202f23d5ca4e8018e57a8c0bdcf13ea51acd835707", "class_name": "RelatedNodeInfo"}}, "text": "Naturally, each domain can have dependencies to other domains' operational and\nanalytical data endpoints. In the following example, 'podcasts' domain consumes\nanalytical data of 'users updates' from the 'users' domain, so that it can provide a\npicture of the demographic of podcast listeners through its 'Podcast listeners\ndemographic' dataset.\nFigure 5\u0000 Example: domain oriented ownership of analytical data in addition to operational capabilities\nNote: In the example, I have used an imperative language for accessing the operational\ndata or capabilities, such as 'Pay artists'. This is simply to emphasize the difference\nbetween the intention of accessing operational data vs. analytical data. I do recognize\nthat in practice operational APIs are implemented through a more declarative interface\nsuch as accessing a RESTful resource or a GraphQL query.\nData as a product2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 9/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 999, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d66c1a9d-d697-455c-b117-bfb424cf0a22": {"__data__": {"id_": "d66c1a9d-d697-455c-b117-bfb424cf0a22", "embedding": null, "metadata": {"page_label": "10", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b2495c04-624f-41b2-bf9d-3f8fd61539e7"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "83cbe7cc-473b-45aa-8ddb-448d94bbed6b", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b2495c04-624f-41b2-bf9d-3f8fd61539e7"}, "hash": "895b6d36af84bcd07f9a92426127f5b0d9d6efe7e5b42d01d557bc6ed74adb1d", "class_name": "RelatedNodeInfo"}}, "text": "One of the challenges of existing analytical data architectures is the high friction and\ncost of discovering, understanding, trusting, and ultimately using quality data. If not\naddressed, this problem only exacerbates with data mesh, as the number of places and\nteams who provide data - domains - increases. This would be the consequence of our\n\u0000rst principle of decentralization. Data as a product principle is designed to address\nthe data quality and age-old data silos problem; or as Gartner calls it dark data - \u201cthe\ninformation assets organizations collect, process and store during regular business\nactivities, but generally fail to use for other purposes\u201d. Analytical data provided by the\ndomains must be treated as a product, and the consumers of that data should be\ntreated as customers - happy and delighted customers.\nThe original article enumerates a list of capabilities, including discoverability, security,\nexplorability, understandability, trustworthiness, etc., that a data mesh implementation\nshould support for a domain data to be considered a product. It also details the roles\nsuch as domain data product owner that organizations must introduce, responsible for\nthe objective measures that ensure data is delivered as a product. These measures\ninclude data quality, decreased lead time of data consumption, and in general data user\nsatisfaction through net promoter score. Domain data product owner must have a deep\nunderstanding of who the data users are, how do they use the data, and what are the\nnative methods that they are comfortable with consuming the data. Such intimate\nknowledge of data users results in design of data product interfaces that meet their\nneeds. In reality, for the majority of data products on the mesh, there are a few\nconventional personas with their unique tooling and expectations, data analysts and\ndata scientists. All data products can develop standardized interfaces to support them.\nThe conversation between users of the data and product owners is a necessary piece\nfor establishing the interfaces of data products.\nEach domain will include data product developer roles, responsible for building,\nmaintaining and serving the domain's data products. Data product developers will be\nworking alongside other developers in the domain. Each domain team may serve one\nor multiple data products. It\u2019s also possible to form new teams to serve data products\nthat don\u2019t naturally \u0000t into an existing operational domain.\nNote: this is an inverted model of responsibility compared to past paradigms. The\naccountability of data quality shifts upstream as close to the source of the data as possible.\nLogical architecture:data product the architectural quantum\nArchitecturally, to support data as a product that domains can autonomously serve or\nconsume, data mesh introduces the concept of data product as its architectural\nquantum. Architectural quantum, as de\u0000ned by Evolutionary Architecture, is the2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 10/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3073, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "251ecb84-4941-471d-80f7-99878bfa12a5": {"__data__": {"id_": "251ecb84-4941-471d-80f7-99878bfa12a5", "embedding": null, "metadata": {"page_label": "11", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_00530a43-bda3-493a-ab79-c8e92f98a843"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "66d34dbe-b05f-408a-b513-dc4e05e944f1", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_00530a43-bda3-493a-ab79-c8e92f98a843"}, "hash": "dc0e2be539fae65ec89e7fe5635107870008213957d821a6dd46756bb6313bfe", "class_name": "RelatedNodeInfo"}}, "text": "smallest unit of architecture that can be independently deployed with high functional\ncohesion, and includes all the structural elements required for its function.\nData product is the node on the mesh that encapsulates three structural components\nrequired for its function, providing access to the domain's analytical data as a product.\nCode: it includes (a) code for data pipelines responsible for consuming,\ntransforming and serving upstream data - data received from domain\u2019s operational\nsystem or an upstream data product; (b) code for APIs that provide access to data,\nsemantic and syntax schema, observability metrics and other metadata; (c) code for\nenforcing traits such as access control policies, compliance, provenance, etc.\nData and Metadata: well that\u2019s what we are all here for, the underlying analytical\nand historical data in a polyglot form. Depending on the nature of the domain data\nand its consumption models, data can be served as events, batch \u0000les, relational\ntables, graphs, etc., while maintaining the same semantic. For data to be usable\nthere is an associated set of metadata including data computational documentation,\nsemantic and syntax declaration, quality metrics, etc; metadata that is intrinsic to\nthe data e.g. its semantic de\u0000nition, and metadata that communicates the traits\nused by computational governance to implement the expected behavior e.g. access\ncontrol policies.\nInfrastructure: The infrastructure component enables building, deploying and\nrunning the data product's code, as well as storage and access to big data and\nmetadata.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 11/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "746f5273-241e-4b90-b12f-d313726edb61": {"__data__": {"id_": "746f5273-241e-4b90-b12f-d313726edb61", "embedding": null, "metadata": {"page_label": "12", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0e4fede4-a73b-4345-9b5e-8be248095c37"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "890696bb-913d-48b7-8bf0-178c85f587dd", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0e4fede4-a73b-4345-9b5e-8be248095c37"}, "hash": "62f845e98e1e78db8259bc2370e5469d1dd5a829e81efa241f33e12ddc51acdb", "class_name": "RelatedNodeInfo"}}, "text": "Figure 6\u0000 Data product components as one architectural quantum\nThe following example builds on the previous section, demonstrating the data product\nas the architectural quantum. The diagram only includes sample content and is not\nintended to be complete or include all design and implementation details. While this is\nstill a logical representation it is getting closer to the physical implementation.\nFigure 7\u0000 Notation: domain, its (analytical) data product and operational system2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 12/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 609, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b5d85c14-d5a7-4a60-897e-0ffabbc0a6b0": {"__data__": {"id_": "b5d85c14-d5a7-4a60-897e-0ffabbc0a6b0", "embedding": null, "metadata": {"page_label": "13", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_df583342-d740-42d9-bfde-75819edc431e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4ca97086-52c7-4623-a6ee-6ab22c932d9d", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_df583342-d740-42d9-bfde-75819edc431e"}, "hash": "be0f2347f52745d58683f4e87a5c9fa2afe5bd52e458601b0f597ca732cc8760", "class_name": "RelatedNodeInfo"}}, "text": "Figure 8\u0000 Data products serving the domain-oriented analytical data\nNote: Data mesh model differs from the past paradigms where pipelines (code) are\nmanaged as independent components from the data they produce; and often\ninfrastructure, like an instance of a warehouse or a lake storage account, is shared among\nmany datasets. Data product is a composition of all components - code, data and\ninfrastructure - at the granularity of a domain's bounded context.\nSelf-serve data platform\nAs you can imagine, to build, deploy, execute, monitor, and access a humble hexagon -\na data product - there is a fair bit of infrastructure that needs to be provisioned and\nrun; the skills needed to provision this infrastructure are specialized and would be\ndif\u0000cult to replicate in each domain. Most importantly, the only way that teams can\nautonomously own their data products is to have access to a high-level abstraction of2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 13/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1039, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83edb6e7-4a1f-4573-91d7-3a8a0c5f55c0": {"__data__": {"id_": "83edb6e7-4a1f-4573-91d7-3a8a0c5f55c0", "embedding": null, "metadata": {"page_label": "14", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_de3aaa68-9e5e-4bdb-93ec-a441aa2da71f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58d6ed7f-4172-4844-9aec-42f5dc0b3a72", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_de3aaa68-9e5e-4bdb-93ec-a441aa2da71f"}, "hash": "62c74fe437fadeda3a6829baf1dd1741c60674c000941c856921b860e15ed8e7", "class_name": "RelatedNodeInfo"}}, "text": "infrastructure that removes complexity and friction of provisioning and managing the\nlifecycle of data products. This calls for a new principle, Self-serve data infrastructure\nas a platform to enable domain autonomy.\nThe data platform can be considered an extension of the delivery platform that already\nexists to run and monitor the services. However the underlying technology stack to\noperate data products, today, looks very different from the delivery platform for\nservices. This is simply due to divergence of big data technology stacks from\noperational platforms. For example, domain teams might be deploying their services as\nDocker containers and the delivery platform uses Kubernetes for their orchestration;\nHowever the neighboring data product might be running its pipeline code as Spark jobs\non a Databricks cluster. That requires provisioning and connecting two very different\nsets of infrastructure, that prior to data mesh did not require this level of\ninteroperability and interconnectivity. My personal hope is that we start seeing a\nconvergence of operational and data infrastructure where it makes sense. For example,\nperhaps running Spark on the same orchestration system, e.g. Kubernetes.\nIn reality, to make analytical data product development accessible to generalist\ndevelopers, to the existing pro\u0000le of developers that domains have, the self-serve\nplatform needs to provide a new category of tools and interfaces in addition to\nsimplifying provisioning. A self-serve data platform must create tooling that supports a\ndomain data product developer\u2019s work\u0000ow of creating, maintaining and running data\nproducts with less specialized knowledge that existing technologies assume; self-serve\ninfrastructure must include capabilities to lower the current cost and specialization\nneeded to build data products. The original writeup includes a list of capabilities that a\nself-serve data platform provides, including access to scalable polyglot data storage,\ndata products schema, data pipeline declaration and orchestration, data products lineage,\ncompute and data locality, etc.\nLogical architecture: a multi-plane data platform\nThe self-serve platform capabilities fall into multiple categories or planes as called in\nthe model. Note: A plane is representative of a level of existence - integrated yet\nseparate. Similar to physical and consciousness planes, or control and data planes in\nnetworking. A plane is neither a layer and nor implies a strong hierarchical access\nmodel.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 14/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2625, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "648e18b0-4c6d-40a4-ab38-5aa0a801b990": {"__data__": {"id_": "648e18b0-4c6d-40a4-ab38-5aa0a801b990", "embedding": null, "metadata": {"page_label": "15", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8c97c3e2-daa1-4846-82b0-42e8619c0e1b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bc6bfe10-9059-4bb8-9344-6e0a9a44453a", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8c97c3e2-daa1-4846-82b0-42e8619c0e1b"}, "hash": "ac92dfd69cbb2cf753e5313c6eb90224931b69e73213a623ee1858fbe1324fb3", "class_name": "RelatedNodeInfo"}}, "text": "Figure 9\u0000 Notation: A platform plane that provides a number of related capabilities through self-serve interfaces\nA self-serve platform can have multiple planes that each serve a different pro\u0000le of\nusers. In the following example, lists three different data platform planes:\nData infrastructure provisioning plane: supports the provisioning of the\nunderlying infrastructure, required to run the components of a data product and\nthe mesh of products. This includes provisioning of a distributed \u0000le storage,\nstorage accounts, access control management system, the orchestration to run data\nproducts internal code, provisioning of a distributed query engine on a graph of\ndata products, etc. I would expect that either other data platform planes or only\nadvanced data product developers use this interface directly. This is a fairly low\nlevel data infrastructure lifecycle management plane.\nData product developer experience plane: this is the main interface that a typical\ndata product developer uses. This interface abstracts many of the complexities of\nwhat entails to support the work\u0000ow of a data product developer. It provides a\nhigher level of abstraction than the 'provisioning plane'. It uses simple declarative\ninterfaces to manage the lifecycle of a data product. It automatically implements the\ncross-cutting concerns that are de\u0000ned as a set of standards and global\nconventions, applied to all data products and their interfaces.\nData mesh supervision plane: there are a set of capabilities that are best provided\nat the mesh level - a graph of connected data products - globally. While the\nimplementation of each of these interfaces might rely on individual data products\ncapabilities, it\u2019s more convenient to provide these capabilities at the level of the\nmesh. For example, ability to discover data products for a particular use case, is2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 15/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1979, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d9fc8522-7228-4fc9-9f0f-b5b778065903": {"__data__": {"id_": "d9fc8522-7228-4fc9-9f0f-b5b778065903", "embedding": null, "metadata": {"page_label": "16", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d3973557-686e-4493-88de-c0d99a2bb828"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "89145a0d-9d8b-419e-99e3-1f2f994c90af", "node_type": "4", "metadata": {"page_label": "16", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d3973557-686e-4493-88de-c0d99a2bb828"}, "hash": "81e9e2e9096c308e4762b8daae3c750b96a26b7f09bd2d5c6b1de599ad8c525f", "class_name": "RelatedNodeInfo"}}, "text": "best provided by search or browsing the mesh of data products; or correlating\nmultiple data products to create a higher order insight, is best provided through\nexecution of a data semantic query that can operate across multiple data products\non the mesh.\nThe following model is only exemplary and is not intending to be complete. While a\nhierarchy of planes is desirable, there is no strict layering implied below.\nFigure 10\u0000 Multiple planes of self-serve data platform *DP stands for a data product\nFederated computational governance\nAs you can see, data mesh follows a distributed system architecture; a collection of\nindependent data products, with independent lifecycle, built and deployed by likely\nindependent teams. However for the majority of use cases, to get value in forms of\nhigher order datasets, insights or machine intelligence there is a need for these2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 16/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 995, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "59de3b7c-d2a5-48c9-aa5b-6307d72db195": {"__data__": {"id_": "59de3b7c-d2a5-48c9-aa5b-6307d72db195", "embedding": null, "metadata": {"page_label": "17", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ce53cd18-30f6-4d15-99cb-afcf665f5ce0"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e01f050f-8ce6-400b-8e5a-bf121354d4b3", "node_type": "4", "metadata": {"page_label": "17", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ce53cd18-30f6-4d15-99cb-afcf665f5ce0"}, "hash": "78c4af992aa1f40f89dae2b04034abbf2686ea3135da8617f93db3cad8bcdc30", "class_name": "RelatedNodeInfo"}}, "text": "independent data products to interoperate; to be able to correlate them, create unions,\n\u0000nd intersections, or perform other graphs or set operations on them at scale. For any\nof these operations to be possible, a data mesh implementation requires a governance\nmodel that embraces decentralization and domain self-sovereignty, interoperability\nthrough global standardization, a dynamic topology and most importantly automated\nexecution of decisions by the platform. I call this a federated computational governance.\nA decision making model led by the federation of domain data product owners and data\nplatform product owners, with autonomy and domain-local decision making power,\nwhile creating and adhering to a set of global rules - rules applied to all data products\nand their interfaces - to ensure a healthy and interoperable ecosystem. The group has\na dif\u0000cult job: maintaining an equilibrium between centralization and decentralization;\nwhat decisions need to be localized to each domain and what decisions should be made\nglobally for all domains. Ultimately global decisions have one purpose, creating\ninteroperability and a compounding network effect through discovery and composition\nof data products.\nThe priorities of the governance in data mesh are different from traditional governance\nof analytical data management systems. While they both ultimately set out to get value\nfrom data, traditional data governance attempts to achieve that through centralization\nof decision making, and establishing global canonical representation of data with\nminimal support for change. Data mesh's federated computational governance, in\ncontrast, embraces change and multiple interpretive contexts.\nPlacing a system in a straitjacket of constancy can cause fragility to evolve.\n-- C.S. Holling, ecologist\nLogical architecture: computational policies embedded in the mesh\nA supportive organizational structure, incentive model and architecture is necessary for\nthe federated governance model to function: to arrive at global decisions and standards\nfor interoperability, while respecting autonomy of local domains, and implement global\npolicies effectively.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 17/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2280, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "6409eccd-8f11-4996-aa96-428f4b1d9111": {"__data__": {"id_": "6409eccd-8f11-4996-aa96-428f4b1d9111", "embedding": null, "metadata": {"page_label": "18", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_416446a8-c076-4685-b3ee-1efc9df19e8b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b7ea9f0d-6b30-4c7a-af36-1261e652e802", "node_type": "4", "metadata": {"page_label": "18", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_416446a8-c076-4685-b3ee-1efc9df19e8b"}, "hash": "af6f2ef48328ed946751f55fb2ce327d87818cde3ba686173f79aed7e4679012", "class_name": "RelatedNodeInfo"}}, "text": "Figure 11\u0000 Notation: federated computational governance model\nAs mentioned earlier, striking a balance between what shall be standardized globally,\nimplemented and enforced by the platform for all domains and their data products, and\nwhat shall be left to the domains to decide, is an art. For instance the domain data\nmodel is a concern that should be localized to a domain who is most intimately familiar\nwith it. For example, how the semantic and syntax of 'podcast audienceship' data model\nis de\u0000ned must be left to the 'podcast domain' team. However in contrast, the decision\naround how to identify a 'podcast listener' is a global concern. A podcast listener is a\nmember of the population of 'users' - its upstream bounded context - who can cross\nthe boundary of domains and be found in other domains such as 'users play streams'.\nThe uni\u0000ed identi\u0000cation allows correlating information about 'users' who are both\n'podcast listeners' and 'stream listeners'.\nThe following is an example of elements involved in the data mesh governance model.\nIt\u2019s not a comprehensive example and only demonstrative of concerns relevant at the\nglobal level.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 18/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1272, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "591a3c26-67e7-4e26-b48e-3bc315127b37": {"__data__": {"id_": "591a3c26-67e7-4e26-b48e-3bc315127b37", "embedding": null, "metadata": {"page_label": "19", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9794d139-2209-48b8-8bfe-d55ca8f50989"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "bdbfcd10-ecab-48cd-a241-66da56e47009", "node_type": "4", "metadata": {"page_label": "19", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9794d139-2209-48b8-8bfe-d55ca8f50989"}, "hash": "b94e29dab2cdccd1b6a721c384d16acc3528223c04c8825dbff7bb51123503ac", "class_name": "RelatedNodeInfo"}}, "text": "Figure 12\u0000 : Example of elements of a federated computational governance: teams, incentives, automated\nimplementation, and globally standardized aspects of data mesh\nMany practices of pre-data-mesh governance, as a centralized function, are no longer\napplicable to the data mesh paradigm. For example, the past emphasis on certi\u0000cation\nof golden datasets - the datasets that have gone through a centralized process of\nquality control and certi\u0000cation and marked as trustworthy - as a central function of\ngovernance is no longer relevant. This had stemmed from the fact that in the previous\ndata management paradigms, data - in whatever quality and format - gets extracted\nfrom operational domain\u2019s databases and gets centrally stored in a warehouse or a lake\nthat now requires a centralized team to apply cleansing, harmonization and encryption\nprocesses to it; often under the custodianship of a centralized governance group. Data\nmesh completely decentralizes this concern. A domain dataset only becomes a data\nproduct after it locally, within the domain, goes through the process of quality\nassurance according to the expected data product quality metrics and the global\nstandardization rules. The domain data product owners are best placed to decide how\nto measure their domain\u2019s data quality knowing the details of domain operations\nproducing the data in the \u0000rst place. Despite such localized decision making and\nautonomy, they need to comply with the modeling of quality and speci\u0000cation of SLOs\nbased on a global standard, de\u0000ned by the global federated governance team, and\nautomated by the platform.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 19/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1736, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "987d3b75-e830-40c8-9953-6f069806614f": {"__data__": {"id_": "987d3b75-e830-40c8-9953-6f069806614f", "embedding": null, "metadata": {"page_label": "20", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_34eb14c5-dfea-4b96-a378-d488a7d4fd1e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "81d2085b-7ec5-4ff4-814c-14dac6356574", "node_type": "4", "metadata": {"page_label": "20", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_34eb14c5-dfea-4b96-a378-d488a7d4fd1e"}, "hash": "5e030e68cd01cc38b5a17187999a3cda7d085accfa1d8493acb7139ed4691b33", "class_name": "RelatedNodeInfo"}}, "text": "The following table shows the contrast between centralized (data lake, data warehouse)\nmodel of data governance, and data mesh.\nPre data mesh governance\naspectData mesh governance aspect\nCentralized team Federated team\nResponsible for data qualityResponsible for de\u0000ning how to model what constitutes\nquality\nResponsible for data securityResponsible for de\u0000ning aspects of data security i.e.\ndata sensitivity levels for the platform to build in and\nmonitor automatically\nResponsible for complying with\nregulationResponsible for de\u0000ning the regulation requirements\nfor the platform to build in and monitor automatically\nCentralized custodianship of data Federated custodianship of data by domains\nResponsible for global canonical\ndata modelingResponsible for modeling polysemes - data elements\nthat cross the boundaries of multiple domains\nTeam is independent from domains Team is made of domains representatives\nAiming for a well de\u0000ned static\nstructure of dataAiming for enabling e\u0000ective mesh operation\nembracing a continuously changing and a dynamic\ntopology of the mesh\nCentralized technology used by\nmonolithic lake/warehouseSelf-serve platform technologies used by each domain\nMeasure success based on number\nor volume of governed data (tables)Measure success based on the network e\u0000ect - the\nconnections representing the consumption of data on\nthe mesh\nManual process with human\ninterventionAutomated processes implemented by the platform\nPrevent errorDetect error and recover through platform\u2019s automated\nprocessing\nPrinciples Summary and the high level logical\narchitecture2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 20/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1709, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "67c885c3-e792-4e3e-bfdf-1645c2152a6c": {"__data__": {"id_": "67c885c3-e792-4e3e-bfdf-1645c2152a6c", "embedding": null, "metadata": {"page_label": "21", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_18b73c9b-3ea9-4881-a21e-9dc830392865"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f6f6c489-68de-42ea-b61b-22d7e077719f", "node_type": "4", "metadata": {"page_label": "21", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_18b73c9b-3ea9-4881-a21e-9dc830392865"}, "hash": "c7d68845b9f438d9cce6c4c5490a3bc78618b21274810f4fc4d548538f360ca6", "class_name": "RelatedNodeInfo"}}, "text": "Let\u2019s bring it all together, we discussed four principles underpinning data mesh:\nDomain-oriented\ndecentralized data\nownership and\narchitectureSo that the ecosystem creating and consuming data can scale out\nas the number of sources of data, number of use cases, and\ndiversity of access models to the data increases; simply increase\nthe autonomous nodes on the mesh.\nData as a productSo that data users can easily discover, understand and securely\nuse high quality data with a delightful experience; data that is\ndistributed across many domains.\nSelf-serve data\ninfrastructure as a\nplatformSo that the domain teams can create and consume data products\nautonomously using the platform abstractions, hiding the\ncomplexity of building, executing and maintaining secure and\ninteroperable data products.\nFederated\ncomputational\ngovernanceSo that data users can get value from aggregation and correlation\nof independent data products - the mesh is behaving as an\necosystem following global interoperability standards; standards\nthat are baked computationally into the platform.\nThese principles drive a logical architectural model that while bringing analytical data\nand operational data closer together under the same domain, it respects their\nunderpinning technical differences. Such differences include where the analytical data\nmight be hosted, different compute technologies for processing operational vs.\nanalytical services, different ways of querying and accessing the data, etc.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 21/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1607, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "963d2eb3-cb6c-4f0e-86a2-7b478db1a8d8": {"__data__": {"id_": "963d2eb3-cb6c-4f0e-86a2-7b478db1a8d8", "embedding": null, "metadata": {"page_label": "22", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0b73ceef-2cae-49d7-bf3c-d505dcb51b7b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "d41a8915-3c59-461a-a299-39e74e2efbf6", "node_type": "4", "metadata": {"page_label": "22", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0b73ceef-2cae-49d7-bf3c-d505dcb51b7b"}, "hash": "739d62da9925a279a297381b852f1cb031ac72f6e5ae9185ccef7f3c08cf57a7", "class_name": "RelatedNodeInfo"}}, "text": "Figure 13\u0000 Logical architecture of data mesh approach\nI hope by this point, we have now established a common language and a logical mental\nmodel that we can collectively take forward to detail the blueprint of the components\nof the mesh, such as the data product, the platform, and the required standardizations.2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 22/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 439, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "335f40cb-086a-4dae-9e21-34b5db01163b": {"__data__": {"id_": "335f40cb-086a-4dae-9e21-34b5db01163b", "embedding": null, "metadata": {"page_label": "23", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_822e7338-e179-4853-acd1-1ddd91643d44"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "de5232a8-8d6e-403a-be1d-a3cf0c1f054f", "node_type": "4", "metadata": {"page_label": "23", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_822e7338-e179-4853-acd1-1ddd91643d44"}, "hash": "efceac1127f834997b7fdb97905917c42b512259e93180b9a47d5d3eed0aa0d4", "class_name": "RelatedNodeInfo"}}, "text": "Acknowledgments\nI am grateful to Martin Fowler for helping me re\u0000ne the narrative and structure of this article, and for\nhosting it.\nSpecial thanks to many ThoughtWorkers who have been helping create and distill the ideas in this article\nthrough client implementations and workshops.\nAlso thanks to the following early reviewers who provided invaluable feedback: Chris Ford, David Colls and\nPramod Sadalage.\nSigni\u0000cant Revisions\n\uef3b\n\u00a9 Martin Fowler | Privacy Policy | Disclosures2024/11/2 18:09 Data Mesh Principles and Logical Architecture\nhttps://martinfowler.com/articles/data-mesh-principles.html 23/23", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 604, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "658e9d6d-c953-4d1a-a60d-03f50e9934c9": {"__data__": {"id_": "658e9d6d-c953-4d1a-a60d-03f50e9934c9", "embedding": null, "metadata": {"page_label": "1", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_64d0fe8b-624e-4942-b8f1-a6d3527c3878"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2a0533b6-6b80-4f19-a67f-29ada59f36c3", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_64d0fe8b-624e-4942-b8f1-a6d3527c3878"}, "hash": "068610eba4a9d411639cdedc68053d28939e0a715631ead65ae9e35f5ecc8d6e", "class_name": "RelatedNodeInfo"}}, "text": "\uf0d1\uef3b\nRefactoringAgileArchitectureAboutThoughtworks\uf09e\uf23a\nGoverning data products using \u0000tness functions\nDecentralized data management requires automation to scale governance effectively.\nFitness functions are a powerful automated governance technique we've applied to data\nproducts within the context of a Data Mesh. Since data products serve as the foundational\nbuilding blocks (architectural quanta) of a data mesh, ensuring robust governance around\nthem signi\u0000cantly increases the chances of a successful data mesh transformation. In this\narticle, we\u2019ll explore how to implement this technique. We'll start by designing simple\ntests to assess key architectural characteristics of a data product, and then we'll explore\nhow to automate their execution by leveraging metadata about the data products.\n05 September 2024\nKiran Prakash\nKiran is a principal engineer at Thoughtworks\nwith a focus on data. An avid practitioner of\nextreme programming, he is passionate about\nmicroservices, platform modernization, data\nengineering, and data strategy. As a senior\nleader in the Data and AI service line, he helps\nlarge, strategic clients in leveraging data to\nachieve business success\n\uf023DATA MESHCONTENTS\nData product as an architectural quantum of the mesh\nWhat are architectural \u0000tness functions\nArchitectural characteristics of a Data Product\nPatterns\nRunning assertions within the catalog\nRunning assertions outside the catalog\nUsing LLMs to interpret metadata\nPublishing the results\nNecessary but not su\u0000cient\nConclusion\nThe key idea behind data mesh is to improve data management in large organizations\nby decentralizing ownership of analytical data. Instead of a central team managing all\nanalytical data, smaller autonomous domain-aligned teams own their respective data\nproducts. This setup allows for these teams to be responsive to evolving business\nTable of Contents2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 1/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2004, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0755e2b9-e6a7-4ead-9d77-421982f2d2aa": {"__data__": {"id_": "0755e2b9-e6a7-4ead-9d77-421982f2d2aa", "embedding": null, "metadata": {"page_label": "2", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5810ab49-72ef-400d-a746-554a0072d35e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "63a056cc-7a11-4afa-848b-a33aa74fe1ae", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5810ab49-72ef-400d-a746-554a0072d35e"}, "hash": "f7d00c63f75cc03d23f515d8dce5942531df6736d7319abd31378940c82ac834", "class_name": "RelatedNodeInfo"}}, "text": "needs and effectively apply their domain knowledge towards data driven decision\nmaking.\nHow to Move Beyond a Monolithic Data\nLake to a Distributed Data Mesh\nMany enterprises are investing in their next\ngeneration data lake, with the hope of\ndemocratizing data at scale to provide business\ninsights and ultimately make automated\nintelligent decisions. Data platforms based on the\ndata lake architecture have common failure\nmodes that lead to unful\u0000lled promises at scale.\nTo address these failure modes we need to shift\nfrom the centralized paradigm of a lake, or its\npredecessor data warehouse. We need to shift to\na paradigm that draws from modern distributed\narchitecture: considering domains as the \u0000rst\nclass concern, applying platform thinking to\ncreate self-serve data infrastructure, and\ntreating data as a product.\nby Zhamak Dehghani\n20 May 2019\nRead more\u2026ARTICLE\n\uf023ENTERPRISE ARCHITECTURE \uf023DATA\nANALYTICS \uf023DOMAIN DRIVEN DESIGN \uf023DATA\nMESH\nHaving smaller autonomous teams presents different sets of governance challenges\ncompared to having a central team managing all of analytical data in a central data\nplatform. Traditional ways of enforcing governance rules using data stewards work\nagainst the idea of autonomous teams and do not scale in a distributed setup. Hence\nwith the data mesh approach, the emphasis is to use automation to enforce\ngovernance rules. In this article we'll examine how to use the concept of \u0000tness\nfunctions to enforce governance rules on data products in a data mesh.1\nThis is particularly important to ensure that the data products meet a minimum\ngovernance standard which in turn is crucial for their interoperability and the network\neffects that data mesh promises.2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 2/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1842, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "737b0b7f-7022-45c8-82eb-6b2f3f1fdf0b": {"__data__": {"id_": "737b0b7f-7022-45c8-82eb-6b2f3f1fdf0b", "embedding": null, "metadata": {"page_label": "3", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_80d4d653-bc2d-4992-8c0a-f223ce6e7739"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8480a70b-c272-4202-9914-49f28cc03380", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_80d4d653-bc2d-4992-8c0a-f223ce6e7739"}, "hash": "f179afef9545dc705f8f3bdfc38a53dea96f8243324b8902042b57965fd43cb2", "class_name": "RelatedNodeInfo"}}, "text": "Data product as an architectural quantum of the\nmesh\nThe term \u201cdata product\u201c has unfortunately taken on various self-serving meanings, and\nfully disambiguating them could warrant a separate article. However, this highlights the\nneed for organizations to strive for a common internal de\u0000nition, and that's where\ngovernance plays a crucial role.\nFor the purposes of this discussion let's agree on the de\u0000nition of a data product as an\narchitectural quantum of data mesh. Simply put, it's a self-contained, deployable, and\nvaluable way to work with data. The concept applies the proven mindset and\nmethodologies of software product development to the data space.\nIn modern software development, we decompose software systems into easily\ncomposable units, ensuring they are discoverable, maintainable, and have committed\nservice level objectives (SLOs). Similarly, a data product is the smallest valuable unit of\nanalytical data, sourced from data streams, operational systems, or other external\nsources and also other data products, packaged speci\u0000cally in a way to deliver\nmeaningful business value. It includes all the necessary machinery to ef\u0000ciently\nachieve its stated goal using automation.\nWhat are architectural \u0000tness functions\nAs described in the book Building Evolutionary Architectures, a \u0000tness function is a\ntest that is used to evaluate how close a given implementation is to its stated design\nobjectives.\nBy using \u0000tness functions, we're aiming to \u201cshift left\u201d on governance, meaning we\nidentify potential governance issues earlier in the timeline of the software value\nstream. This empowers teams to address these issues proactively rather than waiting\nfor them to be caught upon inspections.\nWith \u0000tness functions, we prioritize 2:\nGovernance by rule over Governance by inspection.\nEmpowering teams to discover problems over Independent audits\nContinuous governance over Dedicated audit phase2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 3/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2046, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9791766d-d5ed-460a-8c28-23fac18b7adc": {"__data__": {"id_": "9791766d-d5ed-460a-8c28-23fac18b7adc", "embedding": null, "metadata": {"page_label": "4", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2ed34d4e-c64a-40a1-9da0-d6829ef25aa1"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a378b593-77ab-42f0-974f-daeb5f4b0100", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2ed34d4e-c64a-40a1-9da0-d6829ef25aa1"}, "hash": "2cf03efc8743ba5bf786c6f486c5dc28afa7f49c67c0de706d3111d580358fd9", "class_name": "RelatedNodeInfo"}}, "text": "Since data products are the key building blocks of the data mesh architecture, ensuring\nthat they meet certain architectural characteristics is paramount. It\u2019s a common\npractice to have an organization wide data catalog to index these data products, they\ntypically contain rich metadata about all published data products. Let's see how we can\nleverage all this metadata to verify architectural characteristics of a data product using\n\u0000tness functions.\nArchitectural characteristics of a Data Product\nIn her book Data Mesh: Delivering Data-Driven Value at Scale, Zhamak lays out a few\nimportant architectural characteristics of a data product. Let's design simple\nassertions that can verify these characteristics. Later, we can automate these\nassertions to run against each data product in the mesh.\nDiscoverability Assert that using a name in a keyword search in the catalog or a\ndata product marketplace surfaces the data product in top-n\nresults.\nAddressability Assert that the data product is accessible via a unique URI.\nSelf\nDescriptivenessAssert that the data product has a proper English description\nexplaining its purpose\nAssert for existence of meaningful \u0000eld-level descriptions.\nSecure Assert that access to the data product is blocked for\nunauthorized users.\nInteroperability Assert for existence of business keys, e.g. customer_id,\nproduct_id.\nAssert that the data product supplies data via locally agreed and\nstandardized data formats like CSV, Parquet etc.\nAssert for compliance with metadata registry standards such as\n\u201cISO/IEC 11179\u201d2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 4/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1689, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d76ed900-7cfd-4104-865f-434f521fe7de": {"__data__": {"id_": "d76ed900-7cfd-4104-865f-434f521fe7de", "embedding": null, "metadata": {"page_label": "5", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_60fd93d3-0c5f-49b5-87c0-8d0f02a4919a"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1465d6ff-9d30-497e-a50e-f8244d6da58b", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_60fd93d3-0c5f-49b5-87c0-8d0f02a4919a"}, "hash": "e0bb3978573301260ee42d432db2ea5182a9b9e79f353cdcae6f5f7ccf822562", "class_name": "RelatedNodeInfo"}}, "text": "Trustworthiness Assert for existence of published SLOs and SLIs\nAsserts that adherence to SLOs is good\nValuable on its ownAssert - based on the data product name, description and\ndomain name - that the data product represents a cohesive\ninformation concept in its domain.\nNatively AccessibleAssert that the data product supports output ports tailored for\nkey personas, e.g. REST API output port for developers, SQL\noutput port for data analysts.\nPatterns\nMost of the tests described above (except for the discoverability test) can be run on the\nmetadata of the data product which is stored in the catalog. Let's look at some\nimplementation options.\nRunning assertions within the catalog\nModern day data catalogs like Collibra and Datahub provide hooks using which we can\nrun custom logic. For eg. Collibra has a feature called work\u0000ows and Datahub has a\nfeature called Metadata Tests where one can execute these assertions on the metadata\nof the data product.\nFigure 1\u0000 Running assertions using custom hooks\nIn a recent implementation of data mesh where we used Collibra as the catalog, we\nimplemented a custom business asset called \u201cData Product\u201d that made it2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 5/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1299, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4e0948a7-6439-4e59-a1b4-60c72ef8b72d": {"__data__": {"id_": "4e0948a7-6439-4e59-a1b4-60c72ef8b72d", "embedding": null, "metadata": {"page_label": "6", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a2d19996-c490-4462-97fe-204bb928b1a2"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "a8548aac-f163-4ade-9fdb-695f29dfcd7d", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a2d19996-c490-4462-97fe-204bb928b1a2"}, "hash": "1b425b7182c842864853d5cdb3ddad1f847df12bb0605313b1fe64bd73cfeae4", "class_name": "RelatedNodeInfo"}}, "text": "straightforward to fetch all data assets of type \u201cdata product\u201d and run assertions on\nthem using work\u0000ows.\nRunning assertions outside the catalog\nNot all catalogs provide hooks to run custom logic. Even when they do, it can be\nseverely restrictive. We might not be able to use our favorite testing libraries and\nframeworks for assertions. In such cases, we can pull the metadata from the catalog\nusing an API and run the assertions outside the catalog in a separate process.\nFigure 2\u0000 Using catalog APIs to retrieve data product metadata and run assertions in a separate process\nLet's consider a basic example. As part of the \u0000tness functions for Trustworthiness, we\nwant to ensure that the data product includes published service level objectives (SLOs).\nTo achieve this, we can query the catalog using a REST API. Assuming the response is in\nJSON format, we can use any JSON path library to verify the existence of the relevant\n\u0000elds for SLOs.\nimport json\nfrom jsonpath_ng import parse\nillustrative_get_dataproduct_response = '''{\n  \"entity\": {\n    \"urn\": \"urn:li:dataProduct:marketing_customer360\",\n    \"type\": \"DATA_PRODUCT\",\n    \"aspects\": {\n      \"dataProductProperties\": {\n        \"name\": \"Marketing Customer 360\",\n        \"description\": \"Comprehensive view of customer data for marketing.\",\n        \"domain\": \"urn:li:domain:marketing\",\n        \"owners\": [\n          {\n            \"owner\": \"urn:li:corpuser:jdoe\",\n            \"type\": \"DATAOWNER\"\n          }\n        ],\n        \"uri\": \"https://example.com/dataProduct/marketing_customer360\"\n      },\n      \"dataProductSLOs\": {\n        \"slos\": [\n          {\n            \"name\": \"Completeness\",2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 6/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1787, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d26b1aad-57d3-4d07-9ffb-6180f7eadf28": {"__data__": {"id_": "d26b1aad-57d3-4d07-9ffb-6180f7eadf28", "embedding": null, "metadata": {"page_label": "7", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_572bed3c-87eb-4b07-9bf3-0c5c68d74030"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "b8e71907-ed07-4ad2-900d-c952ee1e15f1", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_572bed3c-87eb-4b07-9bf3-0c5c68d74030"}, "hash": "f1534c2c5f7569d32600c87d47bb3e8c1b455d68b7e410f99256e3985730f73a", "class_name": "RelatedNodeInfo"}}, "text": "\"description\": \"Row count consistency between deployments\",\n            \"target\": 0.95\n          }\n        ]\n      }\n    }\n  }\n}'''\ndef test_existence_of_service_level_objectives():\n    response = json.loads(illustrative_get_dataproduct_response)\n    jsonpath_expr = parse('$.entity.aspects.dataProductSLOs.slos')\n    matches = jsonpath_expr.find(response)\n    data_product_name = parse('$.entity.aspects.dataProductProperties.name').find(response)[0].value\n    assert matches, \"Service Level Objectives are missing for data product : \" + data_product_name\n    assert matches[0].value, \"Service Level Objectives are missing for data product : \" + data_product_name\nUsing LLMs to interpret metadata\nMany of the tests described above involve interpreting data product metadata like \u0000eld\nand job descriptions and assessing their \u0000tness, we believe Large Language Models\n(LLMs) are well-suited for this task.\nLet\u2019s take one of the trickier \u0000tness tests, the test for valuable on its own and explore\nhow to implement it. A similar approach can be used for the self descriptiveness \u0000tness\ntest and the interoperability \u0000tness test for compliance with metadata registry\nstandards.3\nI will use the Function calling feature of OpenAI models to extract structured output\nfrom the evaluations. For simplicity, I performed these evaluations using the OpenAI\nPlayground with GPT-4 as our model. The same results can be achieved using their API.\nOnce you have structured output from a large language model (LLM) in JSON format,\nyou can write assertions similar to those described above.\nSystem Prompt\nYou are a data product evaluator. Your job is to look at the meta data about a data\nproduct provided and evaluate if certain architectural properties of the data product\nholds true or not.\nFunctions: 4\nTip: Asking LLMs to provide reason for it's evaluation improves accuracy of the evaluations and has a2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 7/12", "mimetype": "text/plain", "start_char_idx": 12, "end_char_idx": 2041, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5be204de-4c9a-4ab6-8e65-a6445f1df6f7": {"__data__": {"id_": "5be204de-4c9a-4ab6-8e65-a6445f1df6f7", "embedding": null, "metadata": {"page_label": "8", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_04e80f6b-392d-4b70-b6a6-e3ace2f7f961"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "71ff7763-70c1-4ba0-b3e0-381ab5f7e326", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_04e80f6b-392d-4b70-b6a6-e3ace2f7f961"}, "hash": "691c1ea3f5af94304220a525b425804862164853c932ace5049f3f15b07d595d", "class_name": "RelatedNodeInfo"}}, "text": "nice side effect that we can pass on this reason to data product owner when a \u0000tness function fails\nFunctions\n{\n  \"name\": \"get_data_product_fitness\",\n  \"description\": \"Determine if data product is fit for purpose\",\n  \"strict\": false,\n  \"parameters\": {\n    \"type\": \"object\",\n    \"required\": [],\n    \"properties\": {\n      \"valuable_on_its_own\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"is_fit\": {\n            \"type\": \"boolean\",\n            \"description\": \"True if the data product is valuable on it's own, false otherwise\"\n          },\n          \"reason\": {\n            \"type\": \"string\",\n            \"description\": \"Reason why the data product is valuable on it's own or not\"\n          }\n        },\n        \"description\": \"Determines if data product represents a cohesive\n                        information concept in its domain. Has value on its own\n                        and can be used independent of other data products\"\n      }\n    }\n  }\n}\nWe can then send the data product metadata to the LLM to evaluate them. Here's a\ncouple of results.\nCustomer data product: We expect this data product to pass the test for\nvaluable_on_its_own\nUser:\n{\n  \"entity\": {\n    \"urn\": \"urn:li:dataProduct:marketing_customer360\",\n    \"type\": \"DATA_PRODUCT\",\n    \"aspects\": {\n      \"dataProductProperties\": {\n        \"name\": \"Marketing Customer 360\",\n        \"description\": \"Comprehensive view of customer data for marketing.\",\n        \"domain\": \"urn:li:domain:marketing\"\n      }\n    }\n  }\n}\nAssistant:\n get_data_product_fitness({\n  \"valuable_on_its_own\": {2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 8/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1702, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3594404b-fa6d-4464-a267-444e42d6e91a": {"__data__": {"id_": "3594404b-fa6d-4464-a267-444e42d6e91a", "embedding": null, "metadata": {"page_label": "9", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b784c0a5-c084-444f-b631-719313332d4c"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2b60e37-7467-42ba-b8ed-65360f08e9d7", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b784c0a5-c084-444f-b631-719313332d4c"}, "hash": "ba6ae654f18113711c2a15cd578ea3e663493597f642e8c0418a201a93c2ec66", "class_name": "RelatedNodeInfo"}}, "text": "\"is_fit\": true,\n    \"reason\": \"The 'Marketing Customer 360' data product presumably provides a\n               comprehensive view of customer data, which is valuable for marketing\n               purposes. It could offer insights into customer behaviors, preferences,\n               and engagement with marketing campaigns, all of which can be used to\n               improve marketing strategies.\"\n  }\n})\nProduct_Id data product: We expect this data product to fail the test for\nvaluable_on_its_own. As product id needs to be always joined with product or\ncustomer to have any use.\nUser:\n{\n  \"entity\": {\n    \"urn\": \"urn:li:dataProduct:product_id\",\n    \"type\": \"DATA_PRODUCT\",\n    \"aspects\": {\n      \"dataProductProperties\": {\n        \"name\": \"Product_Id\",\n        \"description\": \"Table representing product ids per customer\",\n        \"domain\": \"urn:li:domain:marketing\"\n      }\n    }\n  }\n}\nAssistant: \n get_data_product_fitness({\n  \"valuable_on_its_own\": {\n    \"is_fit\": false,\n    \"reason\": \"The 'Product_Id' data product may not be valuable on its own. It simply\n               represents product ids per customer and lacks contextual information\n               about what those products are. For it to be meaningful, it would\n               likely need to be used in conjunction with other data products that\n               provide details about the products themselves.\"\n  }\n})\nPublishing the results\nOnce we have the results of the assertions, we can display them on a dashboard. Tools\nlike Dashing and Dash are well-suited for creating lightweight dashboards. Additionally,\nsome data catalogs offer the capability to build custom dashboards as well.2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 9/12", "mimetype": "text/plain", "start_char_idx": 4, "end_char_idx": 1796, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "20c65302-a4de-461c-8d84-7ae13a230db1": {"__data__": {"id_": "20c65302-a4de-461c-8d84-7ae13a230db1", "embedding": null, "metadata": {"page_label": "10", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6d829122-2128-47a7-9107-b54ce984c04f"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9dc86724-6e17-4cd4-80a0-58f5532c16d9", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6d829122-2128-47a7-9107-b54ce984c04f"}, "hash": "d1914395d14da97ef6ef97d635e6d06092cef0b1ccf70a01709beac4379909a5", "class_name": "RelatedNodeInfo"}}, "text": "Figure 3\u0000 A dashboard with green and red data products, grouped by domain, with the ability to drill down and\nview the failed \u0000tness tests\nPublicly sharing these dashboards within the organization can serve as a powerful\nincentive for the teams to adhere to the governance standards. After all, no one wants\nto be the team with the most red marks or un\u0000t data products on the dashboard.\nData product consumers can also use this dashboard to make informed decisions about\nthe data products they want to use. They'd naturally prefer data products that are \u0000t\nover those that are not.\nNecessary but not suf\u0000cient\nWhile these \u0000tness functions are typically run centrally within the data platform, it\nremains the responsibility of the data product teams to ensure their data products pass\nthe \u0000tness tests. It is important to note that the primary goal of the \u0000tness functions is\nto ensure adherence to the basic governance standards. However, this does not absolve\nthe data product teams from considering the speci\u0000c requirements of their domain\nwhen building and publishing their data product.\nFor example, merely ensuring that the access is blocked by default is not suf\u0000cient to\nguarantee the security of a data product containing clinical trial data. Such teams may\nneed to implement additional measures, such as differential privacy techniques, to\nachieve true data security.\nHaving said that, \u0000tness functions are extremely useful. For instance, in one of our\nclient implementations, we found that over 80% of published data products failed to\npass basic \u0000tness tests when evaluated retrospectively.2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 10/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1741, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "87775247-affe-4a3c-81d2-8decc8b52bcd": {"__data__": {"id_": "87775247-affe-4a3c-81d2-8decc8b52bcd", "embedding": null, "metadata": {"page_label": "11", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1dac3ce6-4be6-46cf-ac80-d125191130a2"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "22521c83-657b-493c-9771-0769af46ea67", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1dac3ce6-4be6-46cf-ac80-d125191130a2"}, "hash": "70df256c50a0044642bfa023130a39823350f72a8d67dcf3bbce315207e5dc45", "class_name": "RelatedNodeInfo"}}, "text": "Conclusion\nWe have learnt that \u0000tness functions are an effective tool for governance in Data Mesh.\nGiven that the term \u201cData Product\u201d is still often interpreted according to individual\nconvenience, \u0000tness functions help enforce governance standards mutually agreed\nupon by the data product teams . This, in turn, helps us to build an ecosystem of data\nproducts that are reusable and interoperable.\nHaving to adhere to the standards set by \u0000tness functions encourages teams to build\ndata products using the established \u201cpaved roads\u201d provided by the platform, thereby\nsimplifying the maintenance and evolution of these data products. Publishing results of\n\u0000tness functions on internal dashboards enhances the perception of data quality and\nhelps build con\u0000dence and trust among data product consumers.\nWe encourage you to adopt the \u0000tness functions for data products described in this\narticle as part of your Data Mesh journey.\nAcknowledgements\nThanks Ammara Gafoor, Ian Murdoch, Omar Khawaja and Wolfgang Mai for putting these ideas into action.\nThanks Syed Atif Akhtar, Pablo Porto, Chris Ford, Markus Buhmann and Jim Gumbly for proof reading the\nearly drafts. Special thanks to Martin Fowler for gently nudging me to write this.\nFootnotes\n1\u0000 Fitness functions are only one aspect of automated data governance. The wider topic of the varied\ntechniques that are available is outside the scope of the this article. For an outline of the broader topic of\ngovernance, see Dehghani.\n2\u0000 Expressed using the style of Comparative Values\n3\u0000 At Thoughtworks, we\u2019ve developed an as-yet internal tool called Pendant, currently in use with a few\nhealthcare clients, to assist data product owners in creating glossary de\u0000nitions. Pendant leverages a LLM to\napply ISO 11179 Part 4 requirements and recommendations, evaluating whether a provided de\u0000nition meets\nthese standards. If a de\u0000nition falls short, Pendant suggests improvements to ensure compliance\n4\u0000 In the context of the OpenAI API, functions are prede\u0000ned operations or tasks that a model can reference\nduring a conversation. These functions are included in the prompt and guide the model in generating\nstructured responses, including all necessary parameters for function invocation. For example, we\u2019ve de\u0000ned\na function called get_data_product_fitness, which takes in a parameter named valuable_on_its_own. This\nparameter is an object with two properties: is_fit, a boolean indicating if the data product is independently\nvaluable, and reason, a text \u0000eld explaining the model's rationale.\nSigni\u0000cant Revisions2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 11/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2699, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bcfb5cb1-b538-4ba9-ae12-ec64661d77b2": {"__data__": {"id_": "bcfb5cb1-b538-4ba9-ae12-ec64661d77b2", "embedding": null, "metadata": {"page_label": "12", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_09c94c05-e2f0-42d0-9703-dbe3d0bdb3ee"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aac21f64-7b12-4106-92e6-5c1befea94b5", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_09c94c05-e2f0-42d0-9703-dbe3d0bdb3ee"}, "hash": "6f147158daa87e20a45b143bc84835075445dfaa2016f580a1eff05d0e3e4882", "class_name": "RelatedNodeInfo"}}, "text": "\uef3b\n\u00a9 Martin Fowler | Privacy Policy | Disclosures2024/11/2 18:09 Governing data products using fitness functions\nhttps://martinfowler.com/articles/fitness-functions-data-products.html 12/12", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 188, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "707eaeeb-ba1e-413f-8771-dd770b4fe935": {"__data__": {"id_": "707eaeeb-ba1e-413f-8771-dd770b4fe935", "embedding": null, "metadata": {"page_label": "1", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bb0f1965-4dff-4ed9-8169-419e1e970fa6"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c1614896-fd33-4eec-86eb-a93b610e695b", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bb0f1965-4dff-4ed9-8169-419e1e970fa6"}, "hash": "856fe58890486b9c0b106071f17ab681e36261e6e7492f18c1b51908b188e059", "class_name": "RelatedNodeInfo"}}, "text": "Sandcastle: data/AI apps for everyone\nDaniel Miller\u00b7Follow\nPublished inThe Airbnb Tech Blog\n8 min read\u00b7Sep 24, 2024\nListen Share\nAirbnb made it easy to bring data/AI ideas to life through a platform for prototyping\nweb applications.\nBy: Dan Miller\nWarm, friendly beach capturing the playful nature of prototyping.\nIntroduction\n2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 1/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 536, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "16add095-16fb-402d-a2bb-ef3d959e75a4": {"__data__": {"id_": "16add095-16fb-402d-a2bb-ef3d959e75a4", "embedding": null, "metadata": {"page_label": "2", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_73c38132-d5c7-4cd8-9b35-36bb2ded2528"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9ed0ca2a-38bd-44e2-90f2-646efc7ef16f", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_73c38132-d5c7-4cd8-9b35-36bb2ded2528"}, "hash": "56ffeacbd6487d9d45a055364fafc2c48662ec6a2f11db8bf94d021eaba6a6a8", "class_name": "RelatedNodeInfo"}}, "text": "Trustworthy data has always been a part of Airbnb\u2019s technical DNA. However, it is\nchallenging for our data scientists and ML practitioners to bring data- and AI-\npowered product ideas to life in a way that resonates with our design-focused\nleadership. Slide decks with screenshots, design documents with plots, and even\nFigmas are insufficient to capture ideas that need to be experienced in order to be\nunderstood. This was especially true as large language models (LLMs) took the\nworld by storm, since they are typically used interactively in chat interfaces.\nIn this blog post, we\u2019ll focus on Sandcastle, an Airbnb-internal prototyping platform\nthat enables data scientists, engineers, and even product managers to bring data/AI\nideas to life as internal web applications for our design and product teams. Through\nSandcastle, hundreds of individuals can be \u201ccereal entrepreneurs\u201d \u2014 empowered to\ndirectly iterate on and share their ideas. We\u2019ll talk through common industry\nchallenges involved in sharing web applications internally, give an overview of how\nAirbnb solved these challenges by building on top of its existing cloud\ninfrastructure, and showcase the scale of our results.\nChallenges\nImagine a data scientist is working on a typical data science problem at Airbnb:\noptimizing the positive milestones guests reach along their user journey, visualizing\nthat journey, or improving explainability and statistical power in mathematically\nchallenging scenarios like company-wide launches without A/B, or measuring brand\nperception. The data scientist has a brilliant LLM-powered idea. They want to\ndemonstrate the capability their idea exposes in an interactive way, ideally one that\ncan easily \u201cgo viral\u201d with non-technical stakeholders. Standing between the idea and\nstakeholders are several challenges.\nLeadership & non-technical stakeholders will not want to run a Jupyter notebook,\nbut they can click around in a UI and try out different input assumptions, choose\ndifferent techniques, and deep-dive into outputs.2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 2/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2235, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "70a41518-a768-45eb-8b46-c2a2604ab947": {"__data__": {"id_": "70a41518-a768-45eb-8b46-c2a2604ab947", "embedding": null, "metadata": {"page_label": "3", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_08a380b7-a6fd-4c1f-b3bd-1ab0a1842eda"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9052fea2-9663-4c44-bf9b-e10215740360", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_08a380b7-a6fd-4c1f-b3bd-1ab0a1842eda"}, "hash": "ad9692643df8f6b1b05441e61761bc8aa44caa313918a0592765c85f96d54761", "class_name": "RelatedNodeInfo"}}, "text": "Sandcastle app development\nData scientists are most comfortable writing Python code, and are quite unfamiliar\nwith the world of modern web development (TypeScript, React, etc.). How can they\ncapture their idea in an interactive application, even in their own development\nenvironment? Traditionally, this is done by collaborating with a frontend\nengineering team, but that brings its own set of challenges. Engineering bandwidth\nis typically limited, so prototyping new ideas must go through lengthy planning and\nprioritization cycles. Worse, it is nearly impossible for data scientists to iterate on\nthe science behind their ideas, since any change must go through reprioritization\nand implementation.\nSuppose we can surmount the challenge of capturing an idea in a locally-run\ninteractive web application. How do we package and share it in a way that other\ndata scientists can easily reproduce using standard infrastructure?\nHow can a data science organization handle infrastructure, networking with other\nparts of Airbnb\u2019s complex tech stack, authentication so their apps don\u2019t leak\nsensitive data, and storage for any temporary or intermediate data. How can they\ncreate easily shareable \u201chandles\u201d for their web applications that can easily go viral\ninternally?\nSandcastle\nAirbnb\u2019s solution to the challenges above is called Sandcastle. It brings together\nOnebrain: Airbnb\u2019s packaging framework for data science / prototyping code, kube-\n2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 3/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1649, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "0a89dcb6-ee6e-481d-9821-5c1223e960be": {"__data__": {"id_": "0a89dcb6-ee6e-481d-9821-5c1223e960be", "embedding": null, "metadata": {"page_label": "4", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_03d109dc-9256-4ae8-ae42-472c7e268a4a"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "53319711-0487-4af7-a8de-4ea21f7578ec", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_03d109dc-9256-4ae8-ae42-472c7e268a4a"}, "hash": "4235b914964349e6eba81e706e7e69804c43f0c0a69fe2568b662af101642373", "class_name": "RelatedNodeInfo"}}, "text": "gen: Airbnb\u2019s infrastructure for generated Kubernetes configuration, and OneTouch:\nAirbnb\u2019s infrastructure layer for dynamically scaled Kubernetes clusters. Sandcastle\nis accessible for data scientists, software developers, and even product managers,\nwhether their preferred language is Python, TypeScript, R, or something else. We\nhave had team members use Sandcastle to go from \u201cidea\u201d to \u201clive internal app\u201d in\nless than an hour.\nOnebrain\nThe open source ecosystem solves our first challenge, interactivity. Frameworks like\nStreamlit, Dash, and FastAPI, make it a delight for non-frontend developers to get an\napplication up and running in their own development environment. Onebrain\nsolves the second challenge: how to package a working set of code in a reproducible\nmanner. We presented on Onebrain in detail at KDD 2023 but include a brief\nsummary here. Onebrain assumes you arrange your code in \u201cprojects\u201d: collections\nof arbitrary source code around a onebrain.yml file which looks like below.\nname: youridea\nversion: 1.2.3\ndescription: Example Sandcastle app\nauthors: ['Jane Doe <jane.doe@airbnb.email>']\nbuild_enabled: true\nentry_points:\n  main:\n    type: shell\n    command: streamlit run app.py --server.port {{port}}\n    parameters:\n      port: {type: int, default: 8880}\nenv:\n  python:\n    pip: {streamlit: ==1.34.0}\n2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 4/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1538, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2223dc2a-477f-4d2a-8b2c-3d65cc836096": {"__data__": {"id_": "2223dc2a-477f-4d2a-8b2c-3d65cc836096", "embedding": null, "metadata": {"page_label": "5", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3b537167-9d1b-4303-9579-8cabf75c0b55"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "58a34927-bc19-4122-99e8-8c6aac4e1340", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3b537167-9d1b-4303-9579-8cabf75c0b55"}, "hash": "e8fba60af84822cad20e588dea2a69a3b8140f76484a7608e25a610d8bdd37bd", "class_name": "RelatedNodeInfo"}}, "text": "This \u201cproject file\u201d includes metadata like name, version, authorship, along with a\ncollection of command line entry points that may run shell scripts, Python code,\netc. and an environment specification directing which Python and R packages are\nneeded to run. A developer may run \u201cbrain run\u201d in the same directory as their\nproject file for interactive development. Onebrain is integrated with Airbnb\u2019s\ncontinuous integration, so every commit of the project will be published to our\nsnapshot service. The snapshot service is a lightweight mechanism for storing\nimmutable copies of source code that may be easily downloaded from anywhere\nelse in Airbnb\u2019s tech stack. Services may invoke\nbrain run youridea --port 9877\nto resolve the latest snapshot of the project, bootstrap any dependencies, and invoke\nthe parameterized shell command. This decouples rapid iteration on application\nlogic with slower CI/CD against the service configuration we\u2019ll talk about below.\nkube-gen\nCloud infrastructure is challenging to configure correctly, especially for data\nscientists. Fortunately, Airbnb has built a code-generation layer on top of\nKubernetes called kube-gen, which handles most of authentication, tracing, and\ncross-service communication for you. Sandcastle further simplifies things by using\nkube-gen hooks to generate all but one service configuration file on the developer\u2019s\nbehalf during build. The kube-gen configuration for a typical application would\ninclude environment-specific service parameters, Kubernetes app + container\nconfiguration, Spinnaker \u2122  pipeline definitions, and configuration for Airbnb\u2019s\nnetwork proxy. Sandcastle generates sensible defaults for all of that configuration\non-the-fly, so that all an app developer needs to write is a simple container\nconfiguration file like below. Multiple developers have raised support threads\nbecause the configuration was so simple, they thought they were making a mistake!\nname: sandcastle-youridea\nimage: {{ .Env.Params.pythonImage }}\ncommand:\n  - brain\n  - download-and-run2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 5/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2245, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9dd82802-042c-4cde-9fc9-1559daa32726": {"__data__": {"id_": "9dd82802-042c-4cde-9fc9-1559daa32726", "embedding": null, "metadata": {"page_label": "6", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_85809132-9272-44df-aef5-546589826383"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "1215ada6-70ee-44a0-9294-a9577923c618", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_85809132-9272-44df-aef5-546589826383"}, "hash": "9b60beaad04b20c70fe326fce04eb2a9c27e3fceaa4aff7b1efea9b4f0b2085b", "class_name": "RelatedNodeInfo"}}, "text": "- youridea\n  - --port\n  - {{ .Env.Params.port }}\nresources: {{ ToInlineYaml .Env.Params.containerResources }}\nThe file above allows an app developer to configure which Onebrain project to run,\nwhich port it exposes a process on, and customize the underlying Docker image and\nCPU+RAM resources if necessary.\nWithin 10 \u2013 15 minutes of checking in a file like above, the app will be live at an easily\nshareable URL like https://youridea.airbnb.proxy/ , where it can be shared with\nanyone at the company who has a working corporate login. Sandcastle also handles\n\u201cidentity propagation\u201d from visiting users to the underlying data warehouse\ninfrastructure, to ensure that applications respect user permissions around\naccessing sensitive metrics and tables.\nReplicating Sandcastle\nProduct ideas powered by data and AI are best developed through rapid iteration on\nshareable, lightweight live prototypes, instead of static proposals. There are multiple\nchallenges to facilitating the creation of secure internal prototypes. Open source\nframeworks like Streamlit and Dash help, but aren\u2019t enough: you also need a hosting\nplatform. It doesn\u2019t make sense to open source Sandcastle, because the answers to\n\u201chow does my service talk to others\u201d or \u201chow does authentication work\u201d are so\ndifferent across company infrastructures. Instead, any company can use\nSandcastle\u2019s approach as a recipe: 1) Application: adapt open source web\napplication frameworks to their bespoke tech stack with 2) Hosting platform: that\nhandles authentication, networking and provides shareable links.\nHere is a quick summary of the things you\u2019ll need to think about if you hope to build\na \u201cSandcastle\u201d for your own company:\nOpen source web application framework(s): At Airbnb we largely use Streamlit\nfor data science prototyping, with a bit of FastAPI and React for more bespoke\nprototypes. Prioritize ease of development (especially hot reload), a rich\necosystem of open source components, and performant UIs via caching.\nPackaging system: a way of publishing snapshots of \u201cdata/AI prototype code\u201d\nfrom DS/ML development environments to somewhere consumable from2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 6/15", "mimetype": "text/plain", "start_char_idx": 2, "end_char_idx": 2337, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "04d6b4ca-275e-47d4-b6aa-c4d15d4352f7": {"__data__": {"id_": "04d6b4ca-275e-47d4-b6aa-c4d15d4352f7", "embedding": null, "metadata": {"page_label": "7", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_78e23aca-bc0a-4201-808a-045aa997b174"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8983ea67-a0cb-4495-aa8b-91d3c049d545", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_78e23aca-bc0a-4201-808a-045aa997b174"}, "hash": "a8acb55ce3a9f029b298a08f07ee260cfe576276bfd192052ef2f45e15fc61ce", "class_name": "RelatedNodeInfo"}}, "text": "elsewhere in your tech stack. At Airbnb we use Onebrain, but there are many\npaid public alternatives.\nReproducible runs of DS/ML code: this should include Python / Conda\nenvironment management. Airbnb uses Onebrain for this as well, but you may\nconsider pip.\nIn addition, you\u2019ll need prototyping-friendly solutions for the three pillars of cloud\ncomputing:\nCompute: spin up a remote hosting environment with little or ideally no\ncomplicated infrastructure configuration required.\nStorage: access to ephemeral storage for caching and, more importantly, access\nto your company\u2019s data warehouse infrastructure so prototypes can query your\noffline data.\nNetworking: an authentication proxy that allows internal users to access\nprototypes, ideally via easily memorable domains like appname.yourproxy.io,\nand passes along user information so prototypes can pass visitor credentials\nthrough to the data warehouse or other services. Also, read-only access to other\ninternal services so prototypes can query live data.\nBuild with a view towards \u201cgoing viral\u201d, and you\u2019ll end up with a larger internal\naudience than you expect, especially if your platform is deliberately flexible. This\nallows their developers to focus on leveraging the rich open source prototyping\necosystem. More importantly, key stakeholders will be able to directly experience\ndata/AI ideas at an early stage.\nConclusion\nSandcastle unlocked fast and easy deployment and iteration of new ideas, especially\nin the data and ML (including LLMs, generative AI) spaces. For the first time, data\nscientists and PMs are able to directly iterate on interactive versions of their ideas,\nwithout needing lengthy cycles for prioritization with an engineering team.\nAirbnb\u2019s data science, engineering, and product management community developed\nover 175 live prototypes in the last year, 6 of which were used for high-impact use\ncases. These were visited by over 3.5k unique internal visitors across over 69k\ndistinct active days. Hundreds of internal users a week visit one of our manyOpen in app Sign up Sign in\nSearch\n2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 7/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2279, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "2ae01f91-64ec-4665-b86f-f3943f2d80e5": {"__data__": {"id_": "2ae01f91-64ec-4665-b86f-f3943f2d80e5", "embedding": null, "metadata": {"page_label": "8", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f8329204-0c62-4c2f-9418-d4e662df1ee8"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "24bbb848-2d8d-4747-9078-8cf167723313", "node_type": "4", "metadata": {"page_label": "8", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f8329204-0c62-4c2f-9418-d4e662df1ee8"}, "hash": "eeff6734de988edcdf8a6ce099677d6a7a3f16444c3779da793d496d6e677bf4", "class_name": "RelatedNodeInfo"}}, "text": "internal prototypes to directly interact with them. This led to an ongoing cultural\nshift from using decks / docs to using live prototypes\nIf this type of work interests you, check out some of our related positions:\nStaff data scientist \u2014 Algorithms, Trust\nSenior Staff Software Engineer, AI for Developer Productivity\n\u2026 and more at Careers at Airbnb!\nYou can also learn more about data science and AI at Airbnb by checking out Airbnb\nat KDD 2023, Airbnb Brandometer: Powering Brand Perception Measurement on\nSocial Media Data with AI, and Chronon, Airbnb\u2019s ML Feature Platform, Is Now\nOpen Source.\nAcknowledgments\nThanks to:\nChris C Williams, Erik Iverson, Mike Dodge, Patrick Srail \u2014 for being early\nadopters whose feedback was critical in shaping Sandcastle\u2019s evolution.\nAlex Deng, Carolina Barcenas, Navin Sivanandam \u2014 for their leadership\nsupport.\nFollow\nWritten by Daniel Miller\n22 Followers\u00b7Writer for The Airbnb Tech BlogEngineering Data Science AI Machine Learning Technology\n2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 8/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8ebfdcd1-7ba9-49e4-a940-a413c71e6d4d": {"__data__": {"id_": "8ebfdcd1-7ba9-49e4-a940-a413c71e6d4d", "embedding": null, "metadata": {"page_label": "9", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f1d75350-2514-43a9-9a5b-31bcd529cda4"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "9cd57a24-3832-40ee-adce-40f60d196789", "node_type": "4", "metadata": {"page_label": "9", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f1d75350-2514-43a9-9a5b-31bcd529cda4"}, "hash": "c834f65c3403536be30cab3b69fc8e61ab51c7fda2cdd80431ca4fef4aa1145a", "class_name": "RelatedNodeInfo"}}, "text": "More from Daniel Miller and The Airbnb Tech Blog\nin\nHow Airbnb Smoothly Upgrades React\nIncrementally modernizing our frontend infrastructure to roll out the latest React features\nwithout downgrades\nJul 23\nAndre Wiggins The Airbnb Tech Blog\n704 72024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 9/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 454, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1ef52f19-eeb7-46c8-b3f8-30e64dd29116": {"__data__": {"id_": "1ef52f19-eeb7-46c8-b3f8-30e64dd29116", "embedding": null, "metadata": {"page_label": "10", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_49786212-2190-4aa4-9b9b-cc767e21842b"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "616ca7cb-6011-4c91-adc7-825db5716800", "node_type": "4", "metadata": {"page_label": "10", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_49786212-2190-4aa4-9b9b-cc767e21842b"}, "hash": "0c0abf066589d52e5a14f341a6000e47056838c078bfa1eb663ad0d5885767fa", "class_name": "RelatedNodeInfo"}}, "text": "in\nAvoiding Double Payments in a Distributed Payments System\nHow we built a generic idempotency framework to achieve eventual consistency and\ncorrectness across our paym ents micro-service\u2026\nApr 16, 2019\nin\nUsing Machine Learning to Predict Value of Homes On Airbnb\nby Robert Chang\nJul 17, 2017\nJon Chew The Airbnb Tech Blog\n3.7K 41\nRobert Chang The Airbnb Tech Blog\n5K 222024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 10/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 581, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "9a3beea0-c8c8-4b22-bd9a-7cd8d9952251": {"__data__": {"id_": "9a3beea0-c8c8-4b22-bd9a-7cd8d9952251", "embedding": null, "metadata": {"page_label": "11", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_61a85fb6-ea26-4e14-bf39-e12d1794b0eb"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc293cc2-4188-49ea-8db7-c4c03957d035", "node_type": "4", "metadata": {"page_label": "11", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_61a85fb6-ea26-4e14-bf39-e12d1794b0eb"}, "hash": "20f488aafddcbb44684438ccf208797abcb9e1341a94ded989784edd84e99482", "class_name": "RelatedNodeInfo"}}, "text": "See all from Daniel Miller\nSee all from The Airbnb Tech Blog\nRecommended from Mediumin\nA Deep Dive into Airbnb\u2019s Server-Driven UI System\nHow Airbnb ships features faster across web, iOS, and Android using a server-driven UI system\nnamed Ghost Platform \ud83d\udc7b .\nJun 29, 2021\nin\nForget LangChain, CrewAI and AutoGen \u2014 Try This Framework and Never\nLook Back\nRyan Brooks The Airbnb Tech Blog\n4.1K 41\nKenny Vaneetvelde Generative AI2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 11/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 632, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f8f61c07-ad1b-4d12-b989-79a86bf01626": {"__data__": {"id_": "f8f61c07-ad1b-4d12-b989-79a86bf01626", "embedding": null, "metadata": {"page_label": "12", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_06d9d901-1493-48a3-b852-b621a4f56cd1"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e71d4d19-5fbf-43f0-b7aa-58b12ff21e7f", "node_type": "4", "metadata": {"page_label": "12", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_06d9d901-1493-48a3-b852-b621a4f56cd1"}, "hash": "b576003e1dc49cd73a4d73b0411371f40f18c4223d306447c78973811918103a", "class_name": "RelatedNodeInfo"}}, "text": "Lists\nPredictive Modeling w/ Python\n20 stories\u00b71629 saves\nThe New Chatbots: ChatGPT, Bard, and Beyond\n12 stories\u00b7494 saves\nChatGPT prompts\n50 stories\u00b72166 saves\nPractical Guides to Machine Learning\n10 stories\u00b71993 savesIn the rapidly evolving field of artificial intelligence, developers are inundated with frameworks\nand tools promising to simplify the\u2026\nOct 21\nin\nAI Agent Workflows: A Complete Guide on Whether to Build With\nLangGraph or LangChain\nLangChain and LangGraph \u2014 A deep dive on key building blocks, how they handle core pieces\nof their functionality, and deciding between\u2026\nOct 25\n1.2K 19\nSandi Besen Towards Data Science\n619 7\n2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 12/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 850, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31ac444b-25c4-4fd6-9d30-26fd1ddf40b0": {"__data__": {"id_": "31ac444b-25c4-4fd6-9d30-26fd1ddf40b0", "embedding": null, "metadata": {"page_label": "13", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_305e2b09-598c-46e5-8819-38f46f88c12d"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "f27b4b8f-63b3-4014-a96e-981309692954", "node_type": "4", "metadata": {"page_label": "13", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_305e2b09-598c-46e5-8819-38f46f88c12d"}, "hash": "a66a575a7372aa4c38354dc1d04ba8d1fe4ca70ac31a2506e20104c42923ee06", "class_name": "RelatedNodeInfo"}}, "text": "in\nThe Slow, Painful Death Of Agile and Jira\nAgile has stopped being agile, so it\u2019s time for Agile to go and take Jira with it\nSep 26\nin\nSequence models for Contextual Recommendations at Instacart\nAuthors: Pradeep Karuturi, Young Rao, Sharath Rao, Shishir Kumar Prasad Key contributors:\nBrian Lin, Cheng Jia, Karuna Ahuja, Shrikar\u2026\nJoe Procopio Entrepreneurship Handbook\n3.9K 151\npradeep karuturi tech-at-instacart2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 13/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 624, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "3c5fddc5-2ba2-4983-a20c-ddd02e6fc511": {"__data__": {"id_": "3c5fddc5-2ba2-4983-a20c-ddd02e6fc511", "embedding": null, "metadata": {"page_label": "14", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_097eae81-8bed-4c34-84ab-aca49f7c2a26"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "539e6964-7a4a-4993-a8ae-925fae6721c8", "node_type": "4", "metadata": {"page_label": "14", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_097eae81-8bed-4c34-84ab-aca49f7c2a26"}, "hash": "98d1d3531186dfd6c7052f922e88145998983b008b5ff1ae3dd5fdfc4667aed0", "class_name": "RelatedNodeInfo"}}, "text": "Oct 23\nApple Speaks the Truth About AI. It\u2019s Not Good.\nAre We Being Lied To?\nOct 23\nin\nPython is No More The King of Data Science195 1\nIgnacio de Gregorio\n4.7K 145\nAbdu r Rahman Stackademic2024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 14/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 399, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "31ed4aac-1f33-4182-b246-17e1fb8e010f": {"__data__": {"id_": "31ed4aac-1f33-4182-b246-17e1fb8e010f", "embedding": null, "metadata": {"page_label": "15", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_dda18760-a2d3-42e6-aee8-d5716cc743e9"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "622a6480-5f66-4e0b-af7c-a23d1f34274b", "node_type": "4", "metadata": {"page_label": "15", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_dda18760-a2d3-42e6-aee8-d5716cc743e9"}, "hash": "536d0f4e5c0f9248d0fd911c22279b8e435ed179723be82bdca0be0c4b9008df", "class_name": "RelatedNodeInfo"}}, "text": "See more recomme ndations5 Reasons Why Python is Losing Its Crown\nOct 22 1.5K 152024/11/2 18:06 Sandcastle: data/AI apps for everyone | by Daniel Miller | The Airbnb Tech Blog | Sep, 2024 | Medium\nhttps://medium.com/airbnb-engineering/sandcastle-data-ai-apps-for-everyone-439f3b78b223 15/15", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 290, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e482d595-4d7d-48e0-9928-e741f044985b": {"__data__": {"id_": "e482d595-4d7d-48e0-9928-e741f044985b", "embedding": null, "metadata": {"page_label": "1", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1d36443f-3fb4-48c3-b2bd-6be72f061afa"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "744cc8f4-262a-41a5-93ce-da03dc77eff2", "node_type": "4", "metadata": {"page_label": "1", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1d36443f-3fb4-48c3-b2bd-6be72f061afa"}, "hash": "494871d2b3f375d291aa7cfe0c3e6867fd32f9d21f9fb4739d22205daaf51c56", "class_name": "RelatedNodeInfo"}}, "text": "I\u02bcm Joel Spolsky, a so\u0000ware developer in New York City. More\nabout me.JOEL ON SOFTWARE 2024/11/2 18:08 Things You Should Never Do, Part I \u2013 Joel on Software\nhttps://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/ 1/7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 237, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "84022dfd-21cb-416b-9c0a-664704213651": {"__data__": {"id_": "84022dfd-21cb-416b-9c0a-664704213651", "embedding": null, "metadata": {"page_label": "2", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8949e47c-fe8e-4607-adf4-c106afe78209"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "aa3c1681-c090-44a9-93c8-d54877d1f90d", "node_type": "4", "metadata": {"page_label": "2", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8949e47c-fe8e-4607-adf4-c106afe78209"}, "hash": "17b09acb0281ce3cf3f9e9baf42205516886491b434972e3241436f0acb2791a", "class_name": "RelatedNodeInfo"}}, "text": "APRIL 6, 2000 by JOEL SPOLSKY\nThings You Should Never Do, Part\nI\nNetscape 6.0 is finally going into its first public beta. There never was a version\n5.0. The last major release, version 4.0, was released almost three years ago.\nThree years is an awfully long time in the Internet world. During this time,\nNetscape sat by, helplessly, as their market share plummeted.\nIt\u02bcs a bit smarmy of me to criticize them for waiting so long between releases.\nThey didn\u02bct do it on purpose, now, did they?\nWell, yes. They did. They did it by making the single worst strategic mistake\nthat any so\u0000ware company can make:\nThey decided to rewrite the code from\nscratch.\nNetscape wasn\u02bct the first company to\nmake this mistake. Borland made the\nsame mistake when they bought Arago\nand tried to make it into dBase for\nWindows, a doomed project that took so\nlong that Microso\u0000 Access ate their lunch, then they made it again in rewriting\nQuattro Pro from scratch and astonishing people with how few features it had.\nMicroso\u0000 almost made the same mistake, trying to rewrite Word for Windows\nfrom scratch in a doomed project called Pyramid which was shut down, thrown\naway, and swept under the rug. Lucky for Microso\u0000, they had never stopped\nworking on the old code base, so they had something to ship, making it merely a\nfinancial disaster, not a strategic one.\nWe\u02bcre programmers. Programmers are, in their hearts, architects, and the first\nthing they want to do when they get to a site is to bulldoze the place flat and\nbuild something grand. We\u02bcre not excited by incremental renovation: tinkering,\nimproving,plantingflowerbeds.TOP 10, CEO, NEWS \u00002024/11/2 18:08 Things You Should Never Do, Part I \u2013 Joel on Software\nhttps://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/ 2/7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1775, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "bbe3eb96-8bcd-432c-b760-dd12bf47c8e5": {"__data__": {"id_": "bbe3eb96-8bcd-432c-b760-dd12bf47c8e5", "embedding": null, "metadata": {"page_label": "3", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e6bfef7a-651a-4535-8d99-e1752e00f2c9"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "ed023010-941c-46bc-ade2-b034dc4ac6d4", "node_type": "4", "metadata": {"page_label": "3", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e6bfef7a-651a-4535-8d99-e1752e00f2c9"}, "hash": "6c645da618b9e514869df0b9606d6ce674835009f6d929773fdcaa752c385118", "class_name": "RelatedNodeInfo"}}, "text": "improving, planting flower beds.\nThere\u02bcs a subtle reason that programmers always want to throw away the code\nand start over. The reason is that they think the old code is a mess. And here is\nthe interesting observation: they are probably wrong. The reason that they think\nthe old code is a mess is because of a cardinal, fundamental law of\nprogramming:\nIt\u02bcs harder to read code than to write it.\nThis is why code reuse is so hard. This is why everybody on your team has a\ndi\u0000erent function they like to use for splitting strings into arrays of strings. They\nwrite their own function because it\u02bcs easier and more fun than figuring out how\nthe old function works.\nAs a corollary of this axiom, you can ask almost any\nprogrammer today about the code they are working\non. \u201cIt\u02bcs a big hairy mess,\u201d they will tell you. \u201cI\u02bcd like\nnothing better than to throw it out and start over.\u201d\nWhy is it a mess?\n\u201cWell,\u201d they say, \u201clook at this function. It is two\npages long! None of this stu\u0000 belongs in there! I\ndon\u02bct know what half of these API calls are for.\u201d\nBefore Borland\u02bcs new spreadsheet for Windows\nshipped, Philippe Kahn, the colorful founder of\nBorland, was quoted a lot in the press bragging\nabout how Quattro Pro would be much better than\nMicroso\u0000 Excel, because it was written from scratch. All new source code! As if\nsource code rusted.\nThe idea that new code is better than old is patently absurd. Old code has been\nused. It has been tested. Lots of bugs have been found, and they\u02bcve been fixed.\nThere\u02bcs nothing wrong with it. It doesn\u02bct acquire bugs just by sitting around on\nyour hard drive. Au contraire, baby! Is so\u0000ware supposed to be like an old Dodge\nDart, that rusts just sitting in the garage? Is so\u0000ware like a teddy bear that\u02bcs kind\nof gross if it\u02bcs not made out of all new material?\nBack to that two page function. Yes, I know, it\u02bcs just a simple function to display\na window, but it has grown little hairs and stu\u0000 on it and nobody knows why.2024/11/2 18:08 Things You Should Never Do, Part I \u2013 Joel on Software\nhttps://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/ 3/7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2104, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d082078b-b919-4868-9e79-c0609a175c27": {"__data__": {"id_": "d082078b-b919-4868-9e79-c0609a175c27", "embedding": null, "metadata": {"page_label": "4", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f17f2784-6943-44c7-b73c-d2434097b176"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "be67dabc-5f36-43d8-a4b5-bd21080b6919", "node_type": "4", "metadata": {"page_label": "4", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f17f2784-6943-44c7-b73c-d2434097b176"}, "hash": "1cbac1fc234395041eaf326ab8aeae2b61fdfc66db6d021c05d799d7588cfb7a", "class_name": "RelatedNodeInfo"}}, "text": "Well, I\u02bcll tell you why: those are bug fixes. One of them fixes that bug that Nancy\nhad when she tried to install the thing on a computer that didn\u02bct have Internet\nExplorer. Another one fixes that bug that occurs in low memory conditions.\nAnother one fixes that bug that occurred when the file is on a floppy disk and the\nuser yanks out the disk in the middle. That LoadLibrary call is ugly but it makes\nthe code work on old versions of Windows 95.\nEach of these bugs took weeks of real-world usage before they were found. The\nprogrammer might have spent a couple of days reproducing the bug in the lab\nand fixing it. If it\u02bcs like a lot of bugs, the fix might be one line of code, or it might\neven be a couple of characters, but a lot of work and time went into those two\ncharacters.\nWhen you throw away code and start from scratch, you are throwing away all\nthat knowledge. All those collected bug fixes. Years of programming work.\nYou are throwing away your market leadership. You are giving a gi\u0000 of two or\nthree years to your competitors, and believe me, that is a long time in so\u0000ware\nyears.\nYou are putting yourself in an extremely dangerous position where you will be\nshipping an old version of the code for several years, completely unable to make\nany strategic changes or react to new features that the market demands,\nbecause you don\u02bct have shippable code. You might as well just close for\nbusiness for the duration.\nYou are wasting an outlandish amount of money writing code that already\nexists.\n2024/11/2 18:08 Things You Should Never Do, Part I \u2013 Joel on Software\nhttps://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/ 4/7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1657, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5b77c31e-4be9-459a-9a67-371935c11422": {"__data__": {"id_": "5b77c31e-4be9-459a-9a67-371935c11422", "embedding": null, "metadata": {"page_label": "5", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_95256bab-38be-4e1c-933b-71dca3105152"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0f09bbff-fe27-4862-acf9-3034a08791d6", "node_type": "4", "metadata": {"page_label": "5", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_95256bab-38be-4e1c-933b-71dca3105152"}, "hash": "9d839ef35f041d42eea60feaa97dd0b4a986fb8c8fe9f1eab79ff95e32c4936f", "class_name": "RelatedNodeInfo"}}, "text": "Is there an alternative? The consensus seems to be that the old Netscape code\nbase was really bad. Well, it might have been bad, but, you know what? It\nworked pretty darn well on an awful lot of real world computer systems.\nWhen programmers say that their code is a holy mess (as they always do), there\nare three kinds of things that are wrong with it.\nFirst, there are architectural problems. The code is not factored correctly. The\nnetworking code is popping up its own dialog boxes from the middle of\nnowhere; this should have been handled in the UI code. These problems can be\nsolved, one at a time, by carefully moving code, refactoring, changing interfaces.\nThey can be done by one programmer working carefully and checking in his\nchanges all at once, so that nobody else is disrupted. Even fairly major\narchitectural changes can be done without throwing away the code. On the Juno\nproject we spent several months rearchitecting at one point: just moving things\naround, cleaning them up, creating base classes that made sense, and creating\nsharp interfaces between the modules. But we did it carefully, with our existing\ncode base, and we didn\u02bct introduce new bugs or throw away working code.\nA second reason programmers think that their code is a mess is that it is\nine\u0000icient. The rendering code in Netscape was rumored to be slow. But this\nonly a\u0000ects a small part of the project, which you can optimize or even rewrite.\nYou don\u02bct have to rewrite the whole thing. When optimizing for speed, 1% of the\nwork gets you 99%  of the bang.\nThird, the code may be doggone ugly. One project I worked on actually had a\ndata type called a FuckedString. Another project had started out using the\nconvention of starting member variables with an underscore, but later switched\nto the more standard \u201cm_\u201d. So half the functions started with \u201c_\u201d and half with\n\u201cm_\u201d, which looked ugly. Frankly, this is the kind of thing you solve in five\nminutes with a macro in Emacs, not by starting from scratch.\nIt\u02bcs important to remember that when you start from scratch there is absolutely\nno reason to believe that you are going to do a better job than you did the first\ntime. First of all, you probably don\u02bct even have the same programming team\nthat worked on version one, so you don\u02bct actually have \u201cmore experience\u201d.\nYou\u02bcre just going to make most of the old mistakes again, and introduce some\nnew problems that weren\u02bct in the original version.\nThe old mantra build one to throw away is\ndangerouswhenappliedtolargescale2024/11/2 18:08 Things You Should Never Do, Part I \u2013 Joel on Software\nhttps://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/ 5/7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2654, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "37f21a04-918c-4e62-81b6-5a65ddfdf6ae": {"__data__": {"id_": "37f21a04-918c-4e62-81b6-5a65ddfdf6ae", "embedding": null, "metadata": {"page_label": "6", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f87d2026-2682-4d11-9dfb-6bf85028787e"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "204dcbc6-3086-436d-ab1e-9e879a1476b8", "node_type": "4", "metadata": {"page_label": "6", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f87d2026-2682-4d11-9dfb-6bf85028787e"}, "hash": "0f788be1969e83dbf0fc47fe42f326aa99b016a58c4cf4aaac23957b7bc3c95b", "class_name": "RelatedNodeInfo"}}, "text": "\u2190 PREVIOUS POST\n2000/04/06\ndangerous when applied to large scale\ncommercial applications. If you are\nwriting code experimentally, you may\nwant to rip up the function you wrote last\nweek when you think of a better\nalgorithm. That\u02bcs fine. You may want to\nrefactor a class to make it easier to use.\nThat\u02bcs fine, too. But throwing away the whole program is a dangerous folly, and\nif Netscape actually had some adult supervision with so\u0000ware industry\nexperience, they might not have shot themselves in the foot so badly.\nSUBSCRIBE!\nYou\u02bcre reading Joel on So\u0000ware, stu\u0000ed with years and years of completely raving mad articles\nabout so\u0000ware development, managing so\u0000ware teams, designing user interfaces, running\nsuccessful so\u0000ware companies, and rubber duckies.\nIf you want to know when I publish something new, I recommend getting an RSS reader like\nNewsBlur and subscribing to my RSS feed.\n ABOUT THE AUTHOR.\nIn 2000 I co-founded Fog Creek So\u0000ware, where we created\nlots of cool things like the FogBugz bug tracker, Trello, and\nGlitch. I also worked with Je\u0000 Atwood to create Stack\nOverflow and served as CEO of Stack Overflow from 2010-\n2019. Today I serve as the chairman of the board for Stack\nOverflow, Glitch, and HASH.\nNEXTPOST \u21922024/11/2 18:08 Things You Should Never Do, Part I \u2013 Joel on Software\nhttps://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/ 6/7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 1382, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7cfe8f32-a81b-4bca-a518-bf4d0678d828": {"__data__": {"id_": "7cfe8f32-a81b-4bca-a518-bf4d0678d828", "embedding": null, "metadata": {"page_label": "7", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bafc8714-c1ab-48d5-b0a9-40af0a9a2eac"}, "excluded_embed_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["file_name", "file_type", "file_size", "creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "31cc4bcf-e5c2-456e-8c93-d36f571def8a", "node_type": "4", "metadata": {"page_label": "7", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bafc8714-c1ab-48d5-b0a9-40af0a9a2eac"}, "hash": "a95b923ab458b78456d125d4d96366724f39417a9310d9cc9b4b18b806f3ddb6", "class_name": "RelatedNodeInfo"}}, "text": "PROUDLY POWERED BY WORDPRESS  OS \n2000/04/102024/11/2 18:08 Things You Should Never Do, Part I \u2013 Joel on Software\nhttps://www.joelonsoftware.com/2000/04/06/things-you-should-never-do-part-i/ 7/7", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 194, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"0b4382ff-e79b-47c3-84d5-167c60e1053a": {"doc_hash": "274109002ff0028723f6b470bbec85eb35cd9c38a7a7ee4ac0d35ed0cdf0315f", "ref_doc_id": "470aea9e-eee4-4af0-8a39-60b8b670815c"}, "470aea9e-eee4-4af0-8a39-60b8b670815c": {"doc_hash": "274109002ff0028723f6b470bbec85eb35cd9c38a7a7ee4ac0d35ed0cdf0315f"}, "7e43b9b6-413a-44f0-bd29-d84376d7b73b": {"doc_hash": "0d01bff272f50e4b56234d3e40fc59faa8974824ad589deba45c6428b0ac10ac", "ref_doc_id": "559ad40b-e79b-47cb-82f0-052cd2898e26"}, "559ad40b-e79b-47cb-82f0-052cd2898e26": {"doc_hash": "0d01bff272f50e4b56234d3e40fc59faa8974824ad589deba45c6428b0ac10ac"}, "c0b87291-6364-437a-8497-9c194e92c071": {"doc_hash": "7649cf654311d7e6e7039306ea23c2616311f064a819ddc2c21545b20c8d25ad", "ref_doc_id": "bc53427c-1dc4-4c9f-bfa4-f0d38a0043ef"}, "bc53427c-1dc4-4c9f-bfa4-f0d38a0043ef": {"doc_hash": "7649cf654311d7e6e7039306ea23c2616311f064a819ddc2c21545b20c8d25ad"}, "4c776d2e-0f89-4772-9183-963b6b147d22": {"doc_hash": "5ec976fd11066aca930019cd237626be72a83cc0c3d05d9f4c13d390f830f32e", "ref_doc_id": "f70ecc57-7f31-4023-b1ce-8e853d34e49f"}, "f70ecc57-7f31-4023-b1ce-8e853d34e49f": {"doc_hash": "5ec976fd11066aca930019cd237626be72a83cc0c3d05d9f4c13d390f830f32e"}, "ad582ac4-aa80-4733-8ea7-f44adb1d7da3": {"doc_hash": "606f6101aecdf00d98505621ed63d7fb3b9c8cb5f56a51c46acd86d68d33af1a", "ref_doc_id": "36c33ffa-c012-4413-bb86-9fac6a6d7796"}, "36c33ffa-c012-4413-bb86-9fac6a6d7796": {"doc_hash": "606f6101aecdf00d98505621ed63d7fb3b9c8cb5f56a51c46acd86d68d33af1a"}, "8ebcabc4-cd46-493c-863b-42dd322f897b": {"doc_hash": "6dac533186a3f659923bb09a33dc2a847731f5671a1a27e3eb4b13c08d6b7ebb", "ref_doc_id": "946a0243-320b-4693-980b-8f2487746a62"}, "946a0243-320b-4693-980b-8f2487746a62": {"doc_hash": "6dac533186a3f659923bb09a33dc2a847731f5671a1a27e3eb4b13c08d6b7ebb"}, "77fd6bf5-07f0-4d57-b737-58e66fdd1501": {"doc_hash": "7d36f7f23e4a10684e4343806be82dc4b265e77205d8500e5a9bd38a295d2be2", "ref_doc_id": "d3adebe1-0b0e-4af3-99d9-acd66ee11ceb"}, "d3adebe1-0b0e-4af3-99d9-acd66ee11ceb": {"doc_hash": "7d36f7f23e4a10684e4343806be82dc4b265e77205d8500e5a9bd38a295d2be2"}, "61e16bc0-bc65-4de0-a789-040f618f55b3": {"doc_hash": "4a53e914ecc63432997b0701095afa298986d006bf92d746bdae29f0118d69da", "ref_doc_id": "498a0a43-3e14-4aad-8bd9-1fa4da3532c9"}, "498a0a43-3e14-4aad-8bd9-1fa4da3532c9": {"doc_hash": "4a53e914ecc63432997b0701095afa298986d006bf92d746bdae29f0118d69da"}, "d77ebf21-4520-4290-a7ab-1e26f432afc3": {"doc_hash": "d975dd611f362d85680ace60436cbef581a04818929ec81ca0c1ae1bbe2b3148", "ref_doc_id": "9fd7324b-270e-417d-af59-c5120c5ec543"}, "9fd7324b-270e-417d-af59-c5120c5ec543": {"doc_hash": "d975dd611f362d85680ace60436cbef581a04818929ec81ca0c1ae1bbe2b3148"}, "bc3f5579-5a90-4b4a-94da-6e525fdab5ba": {"doc_hash": "d96ce698dd9748be0c65039d8f6b64ecb43c02f43b673e3d0eecde551e2a8657", "ref_doc_id": "8f24fdaf-2516-4867-b76f-fd94516ced8e"}, "8f24fdaf-2516-4867-b76f-fd94516ced8e": {"doc_hash": "d96ce698dd9748be0c65039d8f6b64ecb43c02f43b673e3d0eecde551e2a8657"}, "55281e51-de8e-4f60-adac-9db5fd0abe0b": {"doc_hash": "60eb93a17d323e5be4b3b1ccccdf5fd81e6101a7ef51cbae4e9d3ec76d26be26", "ref_doc_id": "d08a5b73-885a-4e44-82de-25d5bf880e7b"}, "58bd27ca-5842-4807-943a-869aa89e462f": {"doc_hash": "1b0cc30ebc297c92bbdc39a2ac27456fa5c1c04101eec59b154b42327228abf4", "ref_doc_id": "d08a5b73-885a-4e44-82de-25d5bf880e7b"}, "d08a5b73-885a-4e44-82de-25d5bf880e7b": {"doc_hash": "b4f1c602876a8b8c03eaa8f20e6838a2b33e99382a5d4739f113cceb92540616"}, "d9e816d6-31f4-4317-8ead-45f7492d2385": {"doc_hash": "96b4d30367d24e85800a0f46eac2156bdb3568e24fba92508cff96465685a2f4", "ref_doc_id": "3af6ca8e-7e4e-47db-bdfe-6741e98c3914"}, "4feaa313-6415-4226-ae75-7e71941e5bd2": {"doc_hash": "e822cbebd31bccf01c9be2eee05410f33d22205e38d7bc7ac21ac70bd99bd7f0", "ref_doc_id": "3af6ca8e-7e4e-47db-bdfe-6741e98c3914"}, "3af6ca8e-7e4e-47db-bdfe-6741e98c3914": {"doc_hash": "8ff8705a6ade8dd281192e65e56c9d2150455d1c6d77fe1971ab3f68bb4768c3"}, "8a0d5d11-faa7-4ac6-813b-15fdc8145bd6": {"doc_hash": "d7d6727ede3d68c10a7b891b453a06ee6482c036e123cd2725da6d550c5c2a5e", "ref_doc_id": "8a2697e0-fed1-497f-a593-185353a9ddea"}, "8a2697e0-fed1-497f-a593-185353a9ddea": {"doc_hash": "d7d6727ede3d68c10a7b891b453a06ee6482c036e123cd2725da6d550c5c2a5e"}, "7dcd8ff1-d1af-493e-b40f-4bf5cc3e859a": {"doc_hash": "e556e86bc30fcff6513743b816243c9bd03c90eb63302f27f5d4211ab126f9f2", "ref_doc_id": "8f5a4a0b-6113-41ce-9dbf-bc9fc9314320"}, "8f5a4a0b-6113-41ce-9dbf-bc9fc9314320": {"doc_hash": "e556e86bc30fcff6513743b816243c9bd03c90eb63302f27f5d4211ab126f9f2"}, "3a7c4884-ddab-45aa-a83d-76f52219974d": {"doc_hash": "c3f363b0dca6f380ff8d2aacdaa102ec912f7b9d71d130ac012ec87f2605e50d", "ref_doc_id": "87285a91-9ba5-40a7-9dfa-84198b5909aa"}, "87285a91-9ba5-40a7-9dfa-84198b5909aa": {"doc_hash": "c3f363b0dca6f380ff8d2aacdaa102ec912f7b9d71d130ac012ec87f2605e50d"}, "933b6e1d-70c8-49ab-a6f4-2e1a5998251c": {"doc_hash": "1fa4370a0345d0af9bed9f6882cdc5c99940b0d912b561a940f55c3ec256b1a9", "ref_doc_id": "85a0df51-845b-4fdb-a435-da3523d5bfb1"}, "7e6ca033-72d3-40a5-b249-98bb915ed504": {"doc_hash": "c565a529d1f0ebc47e9ed780825973629f2c4e813b504ead0fd06157a9bf12c6", "ref_doc_id": "85a0df51-845b-4fdb-a435-da3523d5bfb1"}, "85a0df51-845b-4fdb-a435-da3523d5bfb1": {"doc_hash": "59182b6e893fa74edaa3ae4e47c64aa12ebc1860352a3340013b3ffa22bac584"}, "6d979b58-50c8-430a-9495-0a300f5611c0": {"doc_hash": "14b85c0c79c00c5a6f436a156d44a3d2593a1045013e2def86a7524406539e05", "ref_doc_id": "d31c81d4-dfb9-4f5e-81c0-a5c43d4dfe8e"}, "d31c81d4-dfb9-4f5e-81c0-a5c43d4dfe8e": {"doc_hash": "14b85c0c79c00c5a6f436a156d44a3d2593a1045013e2def86a7524406539e05"}, "434584a4-d097-4253-8a26-2c2e7a527ee7": {"doc_hash": "bb8aee01b3873933f6c0807f34f32a9351ae8107e756e9a659f85ef0037783b3", "ref_doc_id": "46526b9f-1f3a-4e70-b3fa-20180f9d4314"}, "8129a709-e3f8-4da2-ab8f-53a3433a0503": {"doc_hash": "da8fd4abc3cceccefa69552fc2fcfa7e0ba9a4c75bc9869704477d70f264fa58", "ref_doc_id": "46526b9f-1f3a-4e70-b3fa-20180f9d4314"}, "46526b9f-1f3a-4e70-b3fa-20180f9d4314": {"doc_hash": "0dc6ba7fb28c11d68d7c461b7954f1c86c2f1f70ddf40340ccc38b20110c28db"}, "86b540ae-99af-4bef-8904-98b4dbc29483": {"doc_hash": "5994c0d934891cb3526f2a3c4cc38a7e9a2ffc96cb8c1a896836017630ee068d", "ref_doc_id": "39bc83ab-eb42-43c5-96d1-5a85037c19a0"}, "7532052f-ef7a-461a-9cac-b72e5d2f8eb0": {"doc_hash": "65e62da18bea5e74185eb7ec2c9f11dda14c34486a31e787dcfba4c7f3c723bf", "ref_doc_id": "39bc83ab-eb42-43c5-96d1-5a85037c19a0"}, "39bc83ab-eb42-43c5-96d1-5a85037c19a0": {"doc_hash": "8aa4140ba475b61c9957886a6ed98ac1d30b77e9c535ebc6c30085d7738b00bc"}, "5e1e065a-6143-4f12-94df-1fe1b836a80e": {"doc_hash": "a94fe0542108bb038f9e58dcbd3dd984e084da700865480c579ff07af93e6100", "ref_doc_id": "02a04001-61fb-47f8-b44c-26bc83dba291"}, "92b5701e-3b44-4d50-8979-f61a2c937454": {"doc_hash": "bf94c393a21b8c2cd3f57b00f22542ee11388d6ad8e395606f6538b6e792ee4d", "ref_doc_id": "02a04001-61fb-47f8-b44c-26bc83dba291"}, "02a04001-61fb-47f8-b44c-26bc83dba291": {"doc_hash": "e90a0e7a3b9ffbe8772d15b3109b5f6ad8c41c56035a41b9299b0073c63dc451"}, "d7f4b22c-88e2-4368-a647-11b90447bcef": {"doc_hash": "c893ae585975679ec83178eb59ea7021aecf01f79e23eaf188f79dd56ae99484", "ref_doc_id": "592c25bb-04c1-4e15-8772-d7b481524bfc"}, "bbaee324-9db7-4409-954b-90fd96e4c8b9": {"doc_hash": "82d7fefaefc8fb96dc1d05c3dde80cf9f3276ad94d9b8ac2b82795448e1e0012", "ref_doc_id": "592c25bb-04c1-4e15-8772-d7b481524bfc"}, "30c9f378-e2ed-4dd9-9709-d28beb9e3505": {"doc_hash": "66d4a6414006bb9a2791df1535946b5b7bfbb313f61c9eee881c855460c398eb", "ref_doc_id": "592c25bb-04c1-4e15-8772-d7b481524bfc"}, "592c25bb-04c1-4e15-8772-d7b481524bfc": {"doc_hash": "e3a4d171ec3490bf744406e92cd2a502c05bf26db1ac26db3847b52d7b6b710b"}, "07889096-ec34-4975-a907-a7cbfccb9999": {"doc_hash": "bccc714cd01a645c53796c141edb6ec7cb4e20900f5c2ed34b65bbe42d2200c7", "ref_doc_id": "def5b079-93bf-469b-899a-36e94a9d8207"}, "f96973d9-40a4-4ca1-baf3-7e5d9625ae63": {"doc_hash": "aaba0eb0000d6da585964e6f743268a9c29a9733d4685ee5a38eec281647936b", "ref_doc_id": "def5b079-93bf-469b-899a-36e94a9d8207"}, "def5b079-93bf-469b-899a-36e94a9d8207": {"doc_hash": "8c9070c4844d558248ff0618b89d4f458a9fc71297917ae0dc2dd513719a4c4f"}, "56416986-896f-4655-b157-b3ea3dce833f": {"doc_hash": "77b7dc3c7e6198be17800ba8e7dfcb48567e0a45af5481c2faf4052a9d4e2b1c", "ref_doc_id": "01522fc9-219c-4bf1-96a9-4c2fad52e98c"}, "dd0a30c7-587a-40d2-bf70-2176b4271e34": {"doc_hash": "0da034ceecaab26ac0a46dc3569d29b2593ab5cfeccd0928d4753e879147f238", "ref_doc_id": "01522fc9-219c-4bf1-96a9-4c2fad52e98c"}, "01522fc9-219c-4bf1-96a9-4c2fad52e98c": {"doc_hash": "6a572cda39ea4551fb638146c5ddb6f5d04fc90b4f975fd8232f7ff7ee72cea8"}, "0a6b4a6b-e898-4b30-8f51-50002e8942b6": {"doc_hash": "2a38abcefe3e03c50068b2964ff455b0c8e9013ad587a3d7b7ef3b9df194f551", "ref_doc_id": "49cbdf01-4090-4204-95c4-01ad5603b94e"}, "c636e520-5899-4ea3-98ba-8bea265995fb": {"doc_hash": "a4d5a5c72d277b4b8ddbb05f8ff2248525e341015c96b4b389f22eebdc1c3ad8", "ref_doc_id": "49cbdf01-4090-4204-95c4-01ad5603b94e"}, "49cbdf01-4090-4204-95c4-01ad5603b94e": {"doc_hash": "7c87e1fbbbf376c1b2496f969e079d750e86ee137e9861239c957b4d9890c903"}, "602e12b0-0f54-4601-9905-f72c7e1942ac": {"doc_hash": "a4ee2151f8f70810885576527d73ebc0f5f0f610773fac80c531a8cccee2817a", "ref_doc_id": "2a860910-1b8b-4660-b2cb-417ae28697e4"}, "a13418ca-c784-4a3f-9873-22957bbc0efc": {"doc_hash": "1dcea6a1784973f6921f99bacf035a6fe1c1e5645f7653d85ae55c2164fa6d05", "ref_doc_id": "2a860910-1b8b-4660-b2cb-417ae28697e4"}, "a10d4ab2-ee04-4885-bfc0-2476d924d4b2": {"doc_hash": "eaa05308963ffc69ee90e61caeaea255c6b9800d04ca896712b7a058405073e1", "ref_doc_id": "2a860910-1b8b-4660-b2cb-417ae28697e4"}, "2a860910-1b8b-4660-b2cb-417ae28697e4": {"doc_hash": "1497107c23178b4c3f128e7f975d2a2f3e5a64d4b8dfcb21df3f21c150dd65ad"}, "94922eb9-7e11-4ee1-9704-9dd91cb56b4c": {"doc_hash": "a2f461e9ac4619e52c4a505f04012c4c84f7a198cf75589632ad4e4240c3090b", "ref_doc_id": "30d293d0-e0d5-44ab-bbce-e81fba386379"}, "89a52eae-0cb8-4f6b-8d57-f6a52c682ba9": {"doc_hash": "ca60fedfcd3a4d40af8693ca0368d02151de6c9f6ba640c9e5d069d3c8ea0c89", "ref_doc_id": "30d293d0-e0d5-44ab-bbce-e81fba386379"}, "38d14119-7377-45a1-9ab1-fadc969337a5": {"doc_hash": "51528a2f264f0a8b0a8f5f363f7214d863ac87cc8c3337e354571a944d0ac96a", "ref_doc_id": "30d293d0-e0d5-44ab-bbce-e81fba386379"}, "30d293d0-e0d5-44ab-bbce-e81fba386379": {"doc_hash": "3d8b05315d288ad11a26ec3f21cee77c2373c3391a415c1e2186699a229043ed"}, "b90671e3-f228-46b4-b1e5-e78f40e19b97": {"doc_hash": "41428ec704c214f5c5d765577a90c83aecae026fe5f2875c3efa2383120f8b27", "ref_doc_id": "034d8c0c-def8-400e-b2da-ccbfa9936911"}, "034d8c0c-def8-400e-b2da-ccbfa9936911": {"doc_hash": "41428ec704c214f5c5d765577a90c83aecae026fe5f2875c3efa2383120f8b27"}, "369ce22b-8016-4316-b431-055171a4a389": {"doc_hash": "48973d1da3caf1ce7aff0eb5850611fe368d7cc7197ea086812e0b41b58a012d", "ref_doc_id": "ed1514d2-a15f-4122-8e0e-18429e7ee461"}, "ed1514d2-a15f-4122-8e0e-18429e7ee461": {"doc_hash": "48973d1da3caf1ce7aff0eb5850611fe368d7cc7197ea086812e0b41b58a012d"}, "c4c9f6af-096a-4d0f-949d-c3bf905bde11": {"doc_hash": "6e32a8fcecb37c51721f86d50458b3d8b528cab22c8298aa2be32d38085a1e4e", "ref_doc_id": "0c9f1ee8-4654-48f2-a6d7-040f73acd552"}, "29ea649f-2b24-433a-86f7-b66f1374a98f": {"doc_hash": "e0b46c19776659cde8965c96af851526aec6f5e09e10caab3ffb2b2139637ad1", "ref_doc_id": "0c9f1ee8-4654-48f2-a6d7-040f73acd552"}, "0c9f1ee8-4654-48f2-a6d7-040f73acd552": {"doc_hash": "ec6da65102bfea7519a44e4f91da3a9352b36fbe2a5715c601426b313e6b8c47"}, "60bc9dac-c6c2-4156-b6fa-393bb15dc70c": {"doc_hash": "75b84133ec68de4cd24239bcb1a3231d1e19e0f95e15707eeeceb4718381d3aa", "ref_doc_id": "15c4482b-227d-46c2-b83c-acdda1dee2c5"}, "e1d8b38e-104b-4e8f-bd88-21f293331864": {"doc_hash": "239825ba5105e5a7ee0aa38f5fe41e951463ce7c8af12a72a257601542b0dcaf", "ref_doc_id": "15c4482b-227d-46c2-b83c-acdda1dee2c5"}, "15c4482b-227d-46c2-b83c-acdda1dee2c5": {"doc_hash": "ad1777bee7506ca9ba88425e5895e7b09f89c81423a9002002b06376c8f9f07e"}, "d2d15fd9-cc29-4a55-81c2-13838a163477": {"doc_hash": "86c54e8a73880c9181c66e8843409df6f7344c1196733d1fdae2afe750bf28f2", "ref_doc_id": "5072742f-e98c-4a85-8707-d5b5efa8db16"}, "5072742f-e98c-4a85-8707-d5b5efa8db16": {"doc_hash": "86c54e8a73880c9181c66e8843409df6f7344c1196733d1fdae2afe750bf28f2"}, "856351f4-a520-4d5b-9691-b8a5d5b876c7": {"doc_hash": "9078132c457b7d4b319ad5ad499f22239d86500cf93821e1b9dbf03cc01a8477", "ref_doc_id": "d1287402-82be-46d5-81b6-14fd6871ca95"}, "dad94d54-e5ce-468a-a670-f6965bbe5af8": {"doc_hash": "595c31230501dc0edc246277c2d7177c924177bd1ae785b8875fd17ce5c2f973", "ref_doc_id": "d1287402-82be-46d5-81b6-14fd6871ca95"}, "d1287402-82be-46d5-81b6-14fd6871ca95": {"doc_hash": "7597d068e8bd2f2ce47bdacf59364dd868e706a29da3a821e7af0a715b3cd36d"}, "5060bb58-5c0c-462a-afe5-4df6dd1f0ffa": {"doc_hash": "0239ce45e9ea3ae8e66c421a85a8ef5c073252889c97e71514815ac289fb1b53", "ref_doc_id": "95c6886c-d5ec-404d-a1a8-432f29273609"}, "95c6886c-d5ec-404d-a1a8-432f29273609": {"doc_hash": "0239ce45e9ea3ae8e66c421a85a8ef5c073252889c97e71514815ac289fb1b53"}, "f1f6c62f-2b96-430f-9fc5-f0cbca4fb67f": {"doc_hash": "77422b94a7b89e71f8a163d0fefbae565297e93a54e30d2a9c5624dfd50d886f", "ref_doc_id": "92378fd6-ae0e-4a53-b8dd-bc475f9eb590"}, "92378fd6-ae0e-4a53-b8dd-bc475f9eb590": {"doc_hash": "77422b94a7b89e71f8a163d0fefbae565297e93a54e30d2a9c5624dfd50d886f"}, "17db4a73-9f33-48b3-9a6b-13dbddad6343": {"doc_hash": "86e44be3954d84a1c16eaa2997ba0d6b65ecc874c18e91e6e5dadb4328c4e187", "ref_doc_id": "eccb7f88-4aaa-406a-9219-89680f6831ef"}, "eccb7f88-4aaa-406a-9219-89680f6831ef": {"doc_hash": "86e44be3954d84a1c16eaa2997ba0d6b65ecc874c18e91e6e5dadb4328c4e187"}, "3a6ac91e-0da2-4a54-9c3a-00a52d34588a": {"doc_hash": "aae42d330c5af3d76fb3b5b74197f79542bf91d767c78661329bec1917a726a5", "ref_doc_id": "f08efdad-6495-4710-8127-30cfb86600ab"}, "c44d2e4a-5e73-4951-8b16-b4153a0365d3": {"doc_hash": "6debe62d280a23ca39f5c768ba9d3e7e156bbdbf1bca3028fde95821921b2507", "ref_doc_id": "f08efdad-6495-4710-8127-30cfb86600ab"}, "f08efdad-6495-4710-8127-30cfb86600ab": {"doc_hash": "261c63cba1d01bb409ddb57458383afac54eafd6a5c0a7ebcfedcb874fb617de"}, "8ecf8252-87f9-4cbb-b69e-bea552152f30": {"doc_hash": "23ad69e1ffb27b4618132e23acdcd0dfee9becac2f3bfb4129cf8726991e3b7e", "ref_doc_id": "14465d6c-5f3b-4863-991b-684c25c5bf1a"}, "6a6eb347-0e69-48f6-af84-daaac08f6355": {"doc_hash": "2e0d38caae1186d5544a21f0cc6c7be9ecc43154c5cc401fb5852911341bf1af", "ref_doc_id": "14465d6c-5f3b-4863-991b-684c25c5bf1a"}, "14465d6c-5f3b-4863-991b-684c25c5bf1a": {"doc_hash": "3ed44482c0f8dd7fbd31528209f15a75a3be0443595181ae014e63063d9475dd"}, "841d08f0-3a27-4218-929e-ff14240f2f3a": {"doc_hash": "ef13e0ae7076161901fd386a756b261825abda884ba3815327439308caa9ff14", "ref_doc_id": "501ec93a-c994-4e7e-860c-318c90c44142"}, "c2e46d11-89a0-4122-8ff4-25a3e15be4ef": {"doc_hash": "95f1cbc7cf0eddf183bb54e37e5c3e18ad41420c6ab57c386c1713b2cc258ca5", "ref_doc_id": "501ec93a-c994-4e7e-860c-318c90c44142"}, "501ec93a-c994-4e7e-860c-318c90c44142": {"doc_hash": "aa68bf2266a252c1bc4f7480467400dc164552d7ba7328cebfb24fb97e300bc1"}, "647d350b-56c9-412a-8b91-a1e8526e3184": {"doc_hash": "fc016bf515941bfce11786c3eeab619cdaadf2b509e773ba1ef0a05493ac932a", "ref_doc_id": "71fea522-d983-4f0e-9557-e0b26bc36823"}, "c1a1a760-5b90-4ed1-9368-a68c1ca8e2e3": {"doc_hash": "0032bd3179a5e1ef8ec1ea6c58651e0f55dd70cd4c41f239384f8af58d339f57", "ref_doc_id": "71fea522-d983-4f0e-9557-e0b26bc36823"}, "71fea522-d983-4f0e-9557-e0b26bc36823": {"doc_hash": "ddbb41037edb481a7933e522464e59ed8bf063b810f38053c895c6ea5b4b9ce4"}, "03dfb7fa-b0a9-4cfe-9f82-b38378099f10": {"doc_hash": "149d087c88392da84722f0e4ca03a3fc2b10996fea6a3d5ae830abb9707ac95f", "ref_doc_id": "802300c2-4daf-4fe8-99e1-1924e2cd511e"}, "32d7c58d-efcc-405b-a8c1-061ae3a03116": {"doc_hash": "96a90522c851380c9e0851979cd9dd9b110b7e5f06c82a4c75e6c8f73f173732", "ref_doc_id": "802300c2-4daf-4fe8-99e1-1924e2cd511e"}, "802300c2-4daf-4fe8-99e1-1924e2cd511e": {"doc_hash": "47c1e0277c699cc4900388e3f973126781d357863eb1120b0cdc913c3199500f"}, "a22b1072-7159-444f-aa83-a7cfc529e368": {"doc_hash": "ce64dafaea42618e62e4e6d060d84fb09cb34b8375656d303e93a8d0cff8c182", "ref_doc_id": "98cca184-30b6-4198-9722-c21e4c60125a"}, "e6514127-9a28-4232-9458-8dd6c5e918d6": {"doc_hash": "48864970d06656912fe8c220bc19d692b2bcf004623cd781bcd5c33425964c69", "ref_doc_id": "98cca184-30b6-4198-9722-c21e4c60125a"}, "98cca184-30b6-4198-9722-c21e4c60125a": {"doc_hash": "6ae9b9e0f4bf913b0dadad16edc80979816feecf2826570188aa0d762eaaba24"}, "77c53922-b0ca-4b30-a1f1-90ce837551bb": {"doc_hash": "74b127354c81d040cda702a8d2400f6fb3a46f743496b16c1c16196538465bfd", "ref_doc_id": "eeb59a00-0960-4c74-9148-c2c6f1429a62"}, "543ca04c-c292-460b-8e5d-fa488f5f358a": {"doc_hash": "2c3304e5f7603fc067fd220b109eafe5828449103ae3ed6862ba04ecf02c5282", "ref_doc_id": "eeb59a00-0960-4c74-9148-c2c6f1429a62"}, "eeb59a00-0960-4c74-9148-c2c6f1429a62": {"doc_hash": "1e08d12827761074f68698066b951fa624e0c07a39f3bdfefd019ae79fd0850f"}, "518f595f-2fa7-477e-9361-7f9bb066070c": {"doc_hash": "4e779c7b5a8d838e0b8a71afedf8e2eb8c052bafa6742df735c4bebf43998789", "ref_doc_id": "50d9a972-2a4f-4073-8d2c-6a24ab48a4e3"}, "b1739833-1579-488f-a85b-74e93ad67776": {"doc_hash": "f7c316fe27250f8a208fb1da596d55c4a6483feac115754f3b051e76dd75d868", "ref_doc_id": "50d9a972-2a4f-4073-8d2c-6a24ab48a4e3"}, "50d9a972-2a4f-4073-8d2c-6a24ab48a4e3": {"doc_hash": "63863179df1ab2fb6754321cb56ccb173c5b3865507ce1df7f155b2f720ea7b4"}, "d8eb5ba9-60c7-4089-897f-b5697fc91149": {"doc_hash": "f943485b28c11bd199c941d12802fca8b50a919c33f2ac15c9e6d220a773e9e5", "ref_doc_id": "abb572ed-1551-4ae3-8943-a12d8835c3e7"}, "b5278636-8d50-4393-8c5c-c2a819ce110f": {"doc_hash": "e9bc79c8dccd3eafeacd02a381eb30c961517d6ee5862744581fdf02e892729f", "ref_doc_id": "abb572ed-1551-4ae3-8943-a12d8835c3e7"}, "abb572ed-1551-4ae3-8943-a12d8835c3e7": {"doc_hash": "efb71e2370348032ec854e5eb9f141949a761ee632d0826371f04eee0fd61767"}, "601ada8f-3e76-4699-aeb5-f69bf3f29639": {"doc_hash": "094c85cc95215eeb3d9b3587e9d28dad273aebad05380b4e5adcda48442a3329", "ref_doc_id": "ff89670f-ba51-4423-8cdc-7b213c639795"}, "fc5f97cb-9e8b-4782-b573-e24bbf51c116": {"doc_hash": "50bad8d32d3e06f8026a48a950cbca4c54e68f7f4eddc43d7d2c1a647219b3f1", "ref_doc_id": "ff89670f-ba51-4423-8cdc-7b213c639795"}, "c15e477b-e5f1-4e92-896f-c60e049075a0": {"doc_hash": "216024ddfa947f81ba22c2135ac62d94a4560fda7b4830c7040d42b29d9431b6", "ref_doc_id": "ff89670f-ba51-4423-8cdc-7b213c639795"}, "ff89670f-ba51-4423-8cdc-7b213c639795": {"doc_hash": "f92c44bf08162c7c59e8f559e98b79f0f38c2357081f3a67c7a6eb3676cdf125"}, "d38b250e-dbd7-4dfd-85b6-248e84170994": {"doc_hash": "1f308540be7c6179632cf4e050507ff6299c88b8e3c4084306510690b1c444e4", "ref_doc_id": "5dd2c0e5-292b-40ba-acf6-8d1dea9b268d"}, "ba60fc36-16b0-4dc3-9dbf-0308eb6cfefb": {"doc_hash": "9cada44326afe6d0a46788012e54327d2bcfcb634078e024bf502db8fdd4fdd6", "ref_doc_id": "5dd2c0e5-292b-40ba-acf6-8d1dea9b268d"}, "5dd2c0e5-292b-40ba-acf6-8d1dea9b268d": {"doc_hash": "45c706908e760bd4bdf8d4cb39eab35a5029a36d5bf5bc87c9b476933c2f76c5"}, "d073bf17-10f9-4aa2-9788-da597bd500b7": {"doc_hash": "5fb048120e100f622241053b12be05a34b77986a6cb47ec8a60429fffe817155", "ref_doc_id": "00e7b1f7-432c-4848-977b-9ec3b4a3a407"}, "e1c1ccb1-01ef-4d17-83a3-13ecf2efcd2d": {"doc_hash": "39bc11bae166718a53c6c5ff2f16fefc4155b5a53eaabd2e98dfa531c088550e", "ref_doc_id": "00e7b1f7-432c-4848-977b-9ec3b4a3a407"}, "00e7b1f7-432c-4848-977b-9ec3b4a3a407": {"doc_hash": "12d9bc002b4aec865edad74f378237273853bcee5f63e82f681e4879cfec2f0f"}, "90f9f8f8-7bd0-4e8e-89cf-8c0bf8ff889e": {"doc_hash": "98efd27fba8ff8df5c32b53c80503ce72d55ed2182afd483e678e9bf1edff2f3", "ref_doc_id": "4e83275c-cf23-44da-829c-cdcd7bc51fbe"}, "4e83275c-cf23-44da-829c-cdcd7bc51fbe": {"doc_hash": "98efd27fba8ff8df5c32b53c80503ce72d55ed2182afd483e678e9bf1edff2f3"}, "a8f12285-2738-4995-b991-614aabed5148": {"doc_hash": "e80d8eadb2b2ea8cf83ffb2ed2a318fa64aa261baae2d58412ffdf625b4863e8", "ref_doc_id": "6859c1bb-c50b-4803-b0ce-dfa0a48f3beb"}, "b0d805a2-0282-47b6-b69e-3076580f3657": {"doc_hash": "7523bf700f10bc18b5b7f8011f81d176c4d42239e81238a5274f4f544dcd6c4a", "ref_doc_id": "6859c1bb-c50b-4803-b0ce-dfa0a48f3beb"}, "6859c1bb-c50b-4803-b0ce-dfa0a48f3beb": {"doc_hash": "f42b66b3b0de9b133ea99057a6a5b31443c61471fdfaca6fa9e21d72f317a821"}, "adf4a0ac-2f86-49a7-9d45-52a30efb9abc": {"doc_hash": "038378dd96d54f466772556347eb4da88d59ee7fe04b0679d9e0ef6902415197", "ref_doc_id": "fc34b172-50b9-44e1-9eef-a4791db20196"}, "ca459feb-df3e-40fa-afed-22e119d52897": {"doc_hash": "c33835aa6faf2e16ee1ed23be72ca874a66d0f1e3a0f4273510a87a028a84968", "ref_doc_id": "fc34b172-50b9-44e1-9eef-a4791db20196"}, "fc34b172-50b9-44e1-9eef-a4791db20196": {"doc_hash": "75b07414f18ef6ae829531ac812ffb6c4a38dc469fde7087ed706bf789a78574"}, "7d06fef1-fd60-4538-9c5c-857ee46acea5": {"doc_hash": "336739167bcea26fb7c6a8a4d1d840578bab4fd89277378248b3de2cc105371d", "ref_doc_id": "b2917322-74be-47ef-ae3e-fccaa3adc5b2"}, "b2917322-74be-47ef-ae3e-fccaa3adc5b2": {"doc_hash": "336739167bcea26fb7c6a8a4d1d840578bab4fd89277378248b3de2cc105371d"}, "3ac6a8b0-8e86-499b-85db-04a2eb0d3e30": {"doc_hash": "e3d79c197640e1761666ac0de25af0a4534daf75ad483777d29cc48989d3d2a7", "ref_doc_id": "779401ef-90b8-4679-9548-559274acb5f7"}, "779401ef-90b8-4679-9548-559274acb5f7": {"doc_hash": "e3d79c197640e1761666ac0de25af0a4534daf75ad483777d29cc48989d3d2a7"}, "ef08506a-120a-4d68-8e4a-ac5819ad773d": {"doc_hash": "bc47cec71135bbb94a5c756496275b46ced99b064566d213a8db5a8f078eedad", "ref_doc_id": "7a378539-d952-4a89-8ccb-4f086d42ad74"}, "7a378539-d952-4a89-8ccb-4f086d42ad74": {"doc_hash": "bc47cec71135bbb94a5c756496275b46ced99b064566d213a8db5a8f078eedad"}, "251f6155-11fd-4e02-92ce-0bcdbf62b78e": {"doc_hash": "87e8846fa566ae525aa0971780229694d30657e2e768a3903e59e54005676405", "ref_doc_id": "9dc9dd4d-e649-4072-9a7f-60e5c67396d6"}, "9dc9dd4d-e649-4072-9a7f-60e5c67396d6": {"doc_hash": "87e8846fa566ae525aa0971780229694d30657e2e768a3903e59e54005676405"}, "4f6053de-8d7c-4faa-866c-692e7ae8f441": {"doc_hash": "baecad96838e367abdeab0e039cc4a6aeebe4c8be409a135bc74a3644b8821d7", "ref_doc_id": "01a68519-f0f3-489d-b3b6-d4df2c6e4e82"}, "01a68519-f0f3-489d-b3b6-d4df2c6e4e82": {"doc_hash": "baecad96838e367abdeab0e039cc4a6aeebe4c8be409a135bc74a3644b8821d7"}, "59cdccb8-0afe-49a7-8cb7-6f2809625247": {"doc_hash": "3a413e4a8493a36cd82f9c7d604c03d01ce34c71ad6cd32229ed0592bef5843e", "ref_doc_id": "60a75ca6-8db9-475f-b54c-e80872815c7b"}, "60a75ca6-8db9-475f-b54c-e80872815c7b": {"doc_hash": "3a413e4a8493a36cd82f9c7d604c03d01ce34c71ad6cd32229ed0592bef5843e"}, "4afde38d-53eb-49a3-88f5-c8ee8fbea4bc": {"doc_hash": "a22d294db2c1039697a48ca2dd6a788fd5d3f1804afe93c8570b7495243c6b7a", "ref_doc_id": "c4a45799-713c-4fc5-afcb-82734244de4d"}, "c4a45799-713c-4fc5-afcb-82734244de4d": {"doc_hash": "a22d294db2c1039697a48ca2dd6a788fd5d3f1804afe93c8570b7495243c6b7a"}, "91de5452-3f8f-45b5-bbc2-676b212fdce0": {"doc_hash": "036b8d0157af17ba2856e91d932ec8eb7197aeab6408bc351917d81fe84007a3", "ref_doc_id": "604a01df-8600-40d5-93e1-254dd6851b92"}, "604a01df-8600-40d5-93e1-254dd6851b92": {"doc_hash": "036b8d0157af17ba2856e91d932ec8eb7197aeab6408bc351917d81fe84007a3"}, "8d44515f-4b60-4deb-af41-240875710fed": {"doc_hash": "331d34507308715ba5006f6be685271cd228c94a21b9603f2e1a51ba19c61c64", "ref_doc_id": "72e129a9-eebe-4070-adb0-68d5e9f84ffe"}, "72e129a9-eebe-4070-adb0-68d5e9f84ffe": {"doc_hash": "331d34507308715ba5006f6be685271cd228c94a21b9603f2e1a51ba19c61c64"}, "b67e027f-83e3-45db-a159-4998ca46b3c7": {"doc_hash": "ca1b940a14ab4bbaf376f80bf316d12a11fea34eb3b79e8cb58d3df2b0d806c8", "ref_doc_id": "543724c9-7fed-46ca-b5c1-93102a3248ad"}, "543724c9-7fed-46ca-b5c1-93102a3248ad": {"doc_hash": "ca1b940a14ab4bbaf376f80bf316d12a11fea34eb3b79e8cb58d3df2b0d806c8"}, "6494b310-f406-4ac0-bd89-75c376dbcb91": {"doc_hash": "5032ace32ca4f14dfd9cecf6258e9a5b887fa987e0bceeb3577c444f02e80e4a", "ref_doc_id": "db535ebe-8647-4bc9-9a4e-c4c52e1413f1"}, "db535ebe-8647-4bc9-9a4e-c4c52e1413f1": {"doc_hash": "5032ace32ca4f14dfd9cecf6258e9a5b887fa987e0bceeb3577c444f02e80e4a"}, "253dbe28-b07f-4508-8819-fcd23f9998cd": {"doc_hash": "7c44d3ab6e3855991b1844226c883ada0f68b15fb03210cf088a7e15b4e4c712", "ref_doc_id": "dbd40c26-12f9-4e09-a012-454e9fc2c044"}, "dbd40c26-12f9-4e09-a012-454e9fc2c044": {"doc_hash": "7c44d3ab6e3855991b1844226c883ada0f68b15fb03210cf088a7e15b4e4c712"}, "1d818f0a-ac5d-4381-92d3-3a0aac77fe7c": {"doc_hash": "293656fce4ccb670e35001093441bf7c9b11f8bf3c5e4c6a117c4309354c5694", "ref_doc_id": "38f586c2-848f-4e10-b324-c5563b75327d"}, "38f586c2-848f-4e10-b324-c5563b75327d": {"doc_hash": "293656fce4ccb670e35001093441bf7c9b11f8bf3c5e4c6a117c4309354c5694"}, "9937aee3-5133-4cd6-ae34-eb9f21fbcb54": {"doc_hash": "cbd3787cc4e2fe366497b0e9fc6f5431aadb37582b506c2dd46a9fead0ec7801", "ref_doc_id": "5f5ae570-fe7a-4b5f-a47c-033cded12afe"}, "5f5ae570-fe7a-4b5f-a47c-033cded12afe": {"doc_hash": "cbd3787cc4e2fe366497b0e9fc6f5431aadb37582b506c2dd46a9fead0ec7801"}, "9c6eeeeb-1e66-40ba-a1bb-7edf2a48770a": {"doc_hash": "e6ef41aa5622383d5bab7e1146cc15d0ca989179a9c8b807817ad8ebe1a99f52", "ref_doc_id": "5c1060b1-f36c-4b7d-ba7a-b2a4e14cd003"}, "5c1060b1-f36c-4b7d-ba7a-b2a4e14cd003": {"doc_hash": "e6ef41aa5622383d5bab7e1146cc15d0ca989179a9c8b807817ad8ebe1a99f52"}, "07980e2e-5313-456e-a6bd-332f80dc576a": {"doc_hash": "116fec671ee98193343534f7d1d7467588875a8291a32da51c010e3a6aeb891f", "ref_doc_id": "d7bab682-4ab2-43c4-b519-ea7dcb8e1b40"}, "d7bab682-4ab2-43c4-b519-ea7dcb8e1b40": {"doc_hash": "116fec671ee98193343534f7d1d7467588875a8291a32da51c010e3a6aeb891f"}, "8a89f9f4-9275-4677-b924-0fe7d550d1f6": {"doc_hash": "338d631e694de1e9bf7b44fb5b5caafc4af6d70367426e47ee40e0666a0b0eec", "ref_doc_id": "3472a329-83f4-4d07-a790-65a5ffb83dbc"}, "3472a329-83f4-4d07-a790-65a5ffb83dbc": {"doc_hash": "338d631e694de1e9bf7b44fb5b5caafc4af6d70367426e47ee40e0666a0b0eec"}, "09865a19-1fbb-4f0b-8348-ce9f0d7d6654": {"doc_hash": "c150f47aa0c50ad0259a98e2c59f139ff758b23124f818395bc07f429319a653", "ref_doc_id": "4d4eb934-5107-4d9e-b758-c4262ed402e6"}, "4d4eb934-5107-4d9e-b758-c4262ed402e6": {"doc_hash": "c150f47aa0c50ad0259a98e2c59f139ff758b23124f818395bc07f429319a653"}, "2e60d2e5-4225-4435-b242-2f6488f1f9d0": {"doc_hash": "aa0698e0d3261f053a7ba24d34f0a2f67ea9d6bbdae2102b4ca463a2fe413cb0", "ref_doc_id": "e46848cb-ec72-4cf0-a171-79e04a845412"}, "e46848cb-ec72-4cf0-a171-79e04a845412": {"doc_hash": "aa0698e0d3261f053a7ba24d34f0a2f67ea9d6bbdae2102b4ca463a2fe413cb0"}, "bc920deb-4a2b-4ae8-94a5-bdf82fea8c74": {"doc_hash": "7efb35299f80028fbc0e363865a2ded944ca24ced8860a485b91925741c3c11a", "ref_doc_id": "1d49d86a-c943-405a-83a9-99913a035eba"}, "1d49d86a-c943-405a-83a9-99913a035eba": {"doc_hash": "7efb35299f80028fbc0e363865a2ded944ca24ced8860a485b91925741c3c11a"}, "89dbf294-2f08-4cef-a27e-dd378b3f0cb1": {"doc_hash": "20d099de2d76cb839a13b557cbda54f68c771ee137a09b200e011f28ff926f89", "ref_doc_id": "33338075-ddad-4455-86ae-283d5d1ed844"}, "33338075-ddad-4455-86ae-283d5d1ed844": {"doc_hash": "20d099de2d76cb839a13b557cbda54f68c771ee137a09b200e011f28ff926f89"}, "332aa17a-47c6-4955-a213-0d28d3fbdc41": {"doc_hash": "5791f2c5983cc477f54295c99fd29dce7d0134964303b628c834b4f6e352f5f0", "ref_doc_id": "a654cc66-ef2b-4ba5-9e8e-a67fceaf8a51"}, "a654cc66-ef2b-4ba5-9e8e-a67fceaf8a51": {"doc_hash": "5791f2c5983cc477f54295c99fd29dce7d0134964303b628c834b4f6e352f5f0"}, "a5a7716e-4ef0-4343-ab17-d942a83dde25": {"doc_hash": "18f7ca26a792e18a626045c3b6f2cdb0680fdd99219dde52e70c147263edb052", "ref_doc_id": "b98b1920-525b-4ed9-a8dc-8e0a3093c2bd"}, "b98b1920-525b-4ed9-a8dc-8e0a3093c2bd": {"doc_hash": "18f7ca26a792e18a626045c3b6f2cdb0680fdd99219dde52e70c147263edb052"}, "e3da756e-e157-4df5-b737-7c9e2ec6369f": {"doc_hash": "715bbba5c93169f05cdd201a24caf8e4469ce5e0a5f90603c80eb704a52e97d4", "ref_doc_id": "be75985d-5ecd-4f00-b4db-262165bad0ef"}, "be75985d-5ecd-4f00-b4db-262165bad0ef": {"doc_hash": "715bbba5c93169f05cdd201a24caf8e4469ce5e0a5f90603c80eb704a52e97d4"}, "b60c463a-4ed7-4844-b7b2-6a7f1e308574": {"doc_hash": "b79287baf154bc13c1d65274aad4a7454f66f486c86d12f9ff2fd60ef0eb5127", "ref_doc_id": "4a0c00d3-a12f-421f-b8d6-c26c62a527c3"}, "4a0c00d3-a12f-421f-b8d6-c26c62a527c3": {"doc_hash": "b79287baf154bc13c1d65274aad4a7454f66f486c86d12f9ff2fd60ef0eb5127"}, "5832350e-2635-45c3-a614-27ebde354f82": {"doc_hash": "320117e7e51ac45573a45789767b1ffa4d067a2b0fabd080cf443f452e5c14a5", "ref_doc_id": "77be2933-da9f-4bd8-b9d0-0d1f68eba7d1"}, "77be2933-da9f-4bd8-b9d0-0d1f68eba7d1": {"doc_hash": "320117e7e51ac45573a45789767b1ffa4d067a2b0fabd080cf443f452e5c14a5"}, "b08cbc2e-d583-4ee2-b247-8104de647145": {"doc_hash": "6ede6847654bcb79801c9926add78277d1dfdee545fe301c0db259a90a60e572", "ref_doc_id": "410d56d2-5c10-4cb4-b6b3-e3393b6de3f6"}, "410d56d2-5c10-4cb4-b6b3-e3393b6de3f6": {"doc_hash": "6ede6847654bcb79801c9926add78277d1dfdee545fe301c0db259a90a60e572"}, "cc95ac90-67a9-4ca8-ae14-951b35d188f9": {"doc_hash": "38447b7ca7ad4bf7a45b2117396d34ed5bc91162457eae12106b375c50250537", "ref_doc_id": "d7b9b335-ea19-4484-84b0-105fba91184d"}, "d7b9b335-ea19-4484-84b0-105fba91184d": {"doc_hash": "38447b7ca7ad4bf7a45b2117396d34ed5bc91162457eae12106b375c50250537"}, "277fe873-efaa-400f-b9ad-7efa7fd80937": {"doc_hash": "3bd6892cbf0acad0f181c0d361257896e7f7d98c37014b05b068f1d379975e05", "ref_doc_id": "300edfd3-f525-439b-9a94-b6f734a28b67"}, "300edfd3-f525-439b-9a94-b6f734a28b67": {"doc_hash": "3bd6892cbf0acad0f181c0d361257896e7f7d98c37014b05b068f1d379975e05"}, "9c3dcb2e-bbbe-4b65-8abc-c34a9babe51e": {"doc_hash": "609273a419ea92f85fcaa4a74eb39a81d38a05a72be7bf17c2eb1cd78eafc396", "ref_doc_id": "64ccfb61-5620-4c58-a944-a0563665400e"}, "64ccfb61-5620-4c58-a944-a0563665400e": {"doc_hash": "609273a419ea92f85fcaa4a74eb39a81d38a05a72be7bf17c2eb1cd78eafc396"}, "96747efd-59e0-440a-a14b-fc6a71d032f1": {"doc_hash": "cb7e3a39cb68663a5f0e52fba7ba1d80af33470df2b19296a78823a654a8db08", "ref_doc_id": "d4dbc210-6e53-4013-b11b-7578d8abe32f"}, "d4dbc210-6e53-4013-b11b-7578d8abe32f": {"doc_hash": "cb7e3a39cb68663a5f0e52fba7ba1d80af33470df2b19296a78823a654a8db08"}, "0eb14a51-8eb5-48f2-afd1-1b3afa4f57f0": {"doc_hash": "fbf6f2da35e2715759bc19a8ab82af4431d8b911d8ffa40209aa965acf409231", "ref_doc_id": "ae22ce76-cfaf-4e45-9665-2357f82e8068"}, "ae22ce76-cfaf-4e45-9665-2357f82e8068": {"doc_hash": "fbf6f2da35e2715759bc19a8ab82af4431d8b911d8ffa40209aa965acf409231"}, "6d473d84-8fd1-4d4a-b1af-9df92ec2f6b6": {"doc_hash": "0ad1f1b072675f97d0f9a3f04516090b61d32a2ee266c3ac0d4c3e2e525dc042", "ref_doc_id": "826b1795-b7fe-4e57-8212-db8707f48c54"}, "1066dd4b-c724-4852-9a51-eaeadfed5730": {"doc_hash": "e40e0b8b80ecde415e0c7363bc53eacc24a89718e38b7e4927d2531e29e1bdc3", "ref_doc_id": "826b1795-b7fe-4e57-8212-db8707f48c54"}, "826b1795-b7fe-4e57-8212-db8707f48c54": {"doc_hash": "4f634eadc1ab1a9a6daad5b469cd3c7561975c8bb88314228c81ff31c56319ba"}, "34369b72-c313-4efc-a9b7-0d4555a887f7": {"doc_hash": "b9d94b6241cd18f0dd405c4924f94d7515bc909e84bc875ba411f18beb03b74e", "ref_doc_id": "30963d4e-02f5-4aba-85e0-4eb3ce99e2cb"}, "fb5a13a1-ba78-45b5-bab6-47548951ff3b": {"doc_hash": "ab1ae217ac02aedb92c94c8270a0355f72ff93fb18c1db8ff904845bff6971a7", "ref_doc_id": "30963d4e-02f5-4aba-85e0-4eb3ce99e2cb"}, "30963d4e-02f5-4aba-85e0-4eb3ce99e2cb": {"doc_hash": "35c2a54a0ea4a538186a52729418a24046bbc58af385e2f8c8547a60fe44d981"}, "c833d849-20fb-4755-a2f7-05d3bbb2a5dd": {"doc_hash": "14e809f35fcaee56631133dcec6e4cc060a731ea740ec541924a284256301ca7", "ref_doc_id": "00613d84-f8ed-44db-aefc-b6da8eddb05e"}, "60fa2e2e-467f-47af-9c8d-4ca736fda5b7": {"doc_hash": "95f355e6efd183293a02607da62be7060ca0b8898ec20be7c3ab9fa05dfab8ac", "ref_doc_id": "00613d84-f8ed-44db-aefc-b6da8eddb05e"}, "00613d84-f8ed-44db-aefc-b6da8eddb05e": {"doc_hash": "2d3e752e5f5fb3c30325eca6acf3fd5079b5e17d06f510add389d6aff3ae59e2"}, "8412566c-c24e-4c79-972a-14c06fe3f274": {"doc_hash": "49b5afd613677561798b745ad9b7e99e79a63b6a5673da175c685b73fe5d0ef8", "ref_doc_id": "d13c4898-bb9c-4b73-be61-a69bb164fcf1"}, "be58266b-8ea1-4897-85b1-535a585ec408": {"doc_hash": "1f38f29ec0d3199465404cccd4c0ece9c8031da2056ba9e2319c2e47a7d05050", "ref_doc_id": "d13c4898-bb9c-4b73-be61-a69bb164fcf1"}, "d13c4898-bb9c-4b73-be61-a69bb164fcf1": {"doc_hash": "8fbb016e14cddb1e829b50be7a3690d790cd57f84481657ed84b474d85e682fe"}, "28d8bef0-3738-4d8f-9fef-407049b02846": {"doc_hash": "c8ae921cf93c1748f68f176de835e324092bc5a13646335349728fe387f4f5c3", "ref_doc_id": "b37aa547-b538-454c-9fc1-0ce11a484017"}, "af0ae41b-dd4d-4b62-a0ff-5ad65a451429": {"doc_hash": "d1e9744311ef14ff368416091f685714580246681aca9c0d81d5e86689289cb8", "ref_doc_id": "b37aa547-b538-454c-9fc1-0ce11a484017"}, "2ff9a882-6bb1-456c-a38e-7a145c057965": {"doc_hash": "99042adbad79dc402ac9ea8c9041a426ba09601422c941f0ab0d56637a5343b1", "ref_doc_id": "b37aa547-b538-454c-9fc1-0ce11a484017"}, "b37aa547-b538-454c-9fc1-0ce11a484017": {"doc_hash": "2ef9ce56660bc5fa873cc104e3744653feef8ea13e1ae116c6d39a0d10e3483d"}, "81a4ce5b-16b6-4db0-bc78-16ae83d85915": {"doc_hash": "fc6a1ec05744b645f2b30e82cf47408cf57b86f9bf64571c07eb5444e22d6df0", "ref_doc_id": "f718058d-2869-4b2e-b2e4-97460929cf24"}, "8cfad21e-475a-4e7b-85ea-a0236d3a58a4": {"doc_hash": "3d956a95aaa3634915f1dcf76e6e58140a42e8f7b05102a0d3707110311aefeb", "ref_doc_id": "f718058d-2869-4b2e-b2e4-97460929cf24"}, "f718058d-2869-4b2e-b2e4-97460929cf24": {"doc_hash": "f916f2d41f62629cc8399301b478d8192621bcaf1afa5592649c7d72420da558"}, "8cb8898d-7c0a-4126-a248-15230bd5bce0": {"doc_hash": "f393cbbfe61dc0aa6ecfc35ff806cef64af6a4b991689bfaa40475ae4c5ea2b4", "ref_doc_id": "47a8e021-eb0b-41b1-8b0f-93a2c3bd6141"}, "47a8e021-eb0b-41b1-8b0f-93a2c3bd6141": {"doc_hash": "f393cbbfe61dc0aa6ecfc35ff806cef64af6a4b991689bfaa40475ae4c5ea2b4"}, "793f4ba8-6baf-4a12-9ede-d8b78773383c": {"doc_hash": "0aab70dd685ca019963e78429612b66a03a8216bb9f72ea5f00db9bff6394109", "ref_doc_id": "b9526a89-aa44-402d-be62-77035f653e63"}, "b9526a89-aa44-402d-be62-77035f653e63": {"doc_hash": "0aab70dd685ca019963e78429612b66a03a8216bb9f72ea5f00db9bff6394109"}, "2de54eb5-6f37-4943-8059-a0e1b4733243": {"doc_hash": "77b6e2a745206deb8a3c0f71c87a57a63510d02531d29de4c64e1f0d91ddb95e", "ref_doc_id": "7aa20ff5-f215-4b96-86b4-e9913d75d654"}, "7aa20ff5-f215-4b96-86b4-e9913d75d654": {"doc_hash": "77b6e2a745206deb8a3c0f71c87a57a63510d02531d29de4c64e1f0d91ddb95e"}, "118aa5f5-8279-46cb-b111-2364b433e4c5": {"doc_hash": "3c69ec7b652022b967cf00dc016953be25d840e81a925a887b013f589e4ae14f", "ref_doc_id": "7da64d19-9820-48aa-8441-e2ccb06b8ce9"}, "e5ad205a-d7d9-4677-a938-4e359bd90252": {"doc_hash": "354897368a6471033b363e22a3b4ff4213f55a2d3db1c0e274c429bfadb17093", "ref_doc_id": "7da64d19-9820-48aa-8441-e2ccb06b8ce9"}, "7da64d19-9820-48aa-8441-e2ccb06b8ce9": {"doc_hash": "2c3a226f8916d7a005d65d6ad14de41f863d6d2b25b4e19959cd668661a7d30d"}, "70c59119-9b2c-4b76-a164-1d15e4ad7461": {"doc_hash": "5810ffac113df264c3461293f4bbd7f00a74ff691ac0a30eab047fd6976e6619", "ref_doc_id": "4fca33ad-03c7-4571-933f-f243d82f642e"}, "06ac039f-1fd2-42ae-91cb-7831fd3863d5": {"doc_hash": "f543072885c86e55c568911bb9da7fe62417bde8db7f7557a3436ab89bec38f1", "ref_doc_id": "4fca33ad-03c7-4571-933f-f243d82f642e"}, "4fca33ad-03c7-4571-933f-f243d82f642e": {"doc_hash": "3327c9aee121990884d595d27385bbbb9b52ccf09c76dba65ca115ac2597c2da"}, "39002530-e9b8-47f1-9ef6-fa312ff23918": {"doc_hash": "69a6a178c5613d16d6803598b5641405691f8bcdde40e02c577ced89726f7863", "ref_doc_id": "60ce781f-c227-40fa-8add-0374d3e8dfef"}, "de4d9b38-c611-4614-8307-06714ee1beb3": {"doc_hash": "06255de3751ecc8dabbc9edc07086803ac16b20b75cc90e68c2cb88658c3d9ec", "ref_doc_id": "60ce781f-c227-40fa-8add-0374d3e8dfef"}, "60ce781f-c227-40fa-8add-0374d3e8dfef": {"doc_hash": "066b4f4d136ef257eede3314b3eb98f84696fbfa309004e9a1e36104a0c02ec5"}, "0c554dc0-3ad5-4556-a231-5f76767a6869": {"doc_hash": "929649ca52f115749a2094e0bbd5afed474ddad7e09f25ba776f27cba14cabe6", "ref_doc_id": "29465572-6bcb-490a-8af9-d6d79917cde3"}, "29465572-6bcb-490a-8af9-d6d79917cde3": {"doc_hash": "929649ca52f115749a2094e0bbd5afed474ddad7e09f25ba776f27cba14cabe6"}, "dc725536-14a3-4b38-82d0-95eaec66409f": {"doc_hash": "78d90f1e97364b935795b4a0632821efb68a9273bdf5fa419faa7b65797ee268", "ref_doc_id": "2b7a8932-3ab9-4ba4-946d-17f5729b94b0"}, "2b7a8932-3ab9-4ba4-946d-17f5729b94b0": {"doc_hash": "78d90f1e97364b935795b4a0632821efb68a9273bdf5fa419faa7b65797ee268"}, "efdd9c3e-89b0-42ce-9185-571d1c60a8d2": {"doc_hash": "ce62b159babc9716df98ec57fc550fa99e27b1f4a091f9d7bdb8a2fe59766656", "ref_doc_id": "eb5c5878-36d5-42ce-854a-fef648a0e168"}, "f6292d36-20b1-4746-b2a3-bab3e21617e3": {"doc_hash": "10dbea296dfc4aedafaeaef33e4497e0ce3aef9480fd2906dba95ea5f2e607c1", "ref_doc_id": "eb5c5878-36d5-42ce-854a-fef648a0e168"}, "eb5c5878-36d5-42ce-854a-fef648a0e168": {"doc_hash": "6a434fdc5fb79eee17b3631ee8a046d47ac63f5877c999848d565e73b5fe89dc"}, "b32012c9-1778-4670-988f-92c1f7a4465e": {"doc_hash": "3b5c31b12e9c7e8ebb19acb1b15f4bf93d9de01bb6012e13ff9a4ddbd85a39cc", "ref_doc_id": "b6dacd1f-c879-4094-8104-af5bb6a8a617"}, "8df83a13-0a05-4b0b-b917-3171755047f4": {"doc_hash": "3b9d3eb90b3d1a950b3121b19f8f1d670ed7ebafda93ff94072865dd2ea23e72", "ref_doc_id": "b6dacd1f-c879-4094-8104-af5bb6a8a617"}, "b6dacd1f-c879-4094-8104-af5bb6a8a617": {"doc_hash": "69c88b502844387c57117787e2c03e830715348bc9b78299f4231fabbcd69403"}, "be49a2e6-bb87-4eef-a52a-0c93b6657773": {"doc_hash": "ba1800a7ff7b23ff360d24267e6635371fe1305f18974afbc29edcb096c4ed4b", "ref_doc_id": "c13b4076-cc52-47a7-bd23-a1d0dc1d4063"}, "82f70931-ab48-48aa-958b-920604ccb8b1": {"doc_hash": "a869c891a48f5075364e2d7425d30346a00b8a91c2415a16d48b18e4286a66c2", "ref_doc_id": "c13b4076-cc52-47a7-bd23-a1d0dc1d4063"}, "c13b4076-cc52-47a7-bd23-a1d0dc1d4063": {"doc_hash": "5253e80c62fd3379c49958f0c59f4485b1036c17f95b4624607e8ba8cf7ab57e"}, "f692efce-0785-41e5-892e-75fb73689f91": {"doc_hash": "b139b8ee8b78237014b5a91b0dbe07e0f0c409a0d646eb7e5cc3bbf0776d09d1", "ref_doc_id": "a9bc6d35-9cf1-4a89-a115-1a50f10065b8"}, "434b23ab-77e6-47fd-a4fc-1eec1cb0e872": {"doc_hash": "fb84590334e4885d9155d0d72a6f8e63e70a353740c7d17502a07eb14a95533d", "ref_doc_id": "a9bc6d35-9cf1-4a89-a115-1a50f10065b8"}, "a9bc6d35-9cf1-4a89-a115-1a50f10065b8": {"doc_hash": "90a0640d46c3d0321f8cee257a04fbf906ed3ac121a720a2b7cc5e4252915a42"}, "c03d1ed0-f1d3-4448-ab7e-d7aa1b41ff74": {"doc_hash": "485a7a9552b17aceccf9a9436c9f5e3e5f59f77e0903196a8d748b8c123a8dad", "ref_doc_id": "067fc83b-a08c-4586-b425-408b2ff6bc02"}, "067fc83b-a08c-4586-b425-408b2ff6bc02": {"doc_hash": "485a7a9552b17aceccf9a9436c9f5e3e5f59f77e0903196a8d748b8c123a8dad"}, "47e216b8-49ae-4bc1-8bb1-64548d98934d": {"doc_hash": "2ea76c65e77ff05e8fc4fc3be9e39a42a8a359315462fc1440bc7e3282dcf20d", "ref_doc_id": "ddc71793-b545-46cd-aed7-28e3c9d98f64"}, "2c50109c-cdf3-445e-90c1-36f01caaacde": {"doc_hash": "577968ff401d963ef8186dd205fb9918df09ecb6b4082422df51ec5076cc6a04", "ref_doc_id": "ddc71793-b545-46cd-aed7-28e3c9d98f64"}, "ddc71793-b545-46cd-aed7-28e3c9d98f64": {"doc_hash": "7c70aa87f181b49edbe2d5b3f1fd68f4f0391c552e207029230379445cc9b0a7"}, "121686d2-ff20-41d8-a8e7-8368d5c1332e": {"doc_hash": "472c02764c743a7e70d1441070d359187857ce3d23253875ee84d4420878e0a0", "ref_doc_id": "4b149eb8-2051-4a35-9973-5ca470efb10e"}, "25304bf1-fbd1-4fbe-85c2-50d8a4ee82a0": {"doc_hash": "d6df76c688d4abce8f95f2d1f5ae77e42e0589cb49c85de2cfb29515b53d0328", "ref_doc_id": "4b149eb8-2051-4a35-9973-5ca470efb10e"}, "4b149eb8-2051-4a35-9973-5ca470efb10e": {"doc_hash": "08417eb1c2a5adb2184bd4606fc9e77b46179a66b9b914df7f9a364558e1360d"}, "ed49e192-9edc-41f9-a1ec-f7af2b58b59b": {"doc_hash": "a6d62ae72b96c2e259b4d8636b8839cc4f4a7115731853e3a3f12403d2342cbb", "ref_doc_id": "92b1ee21-17e6-4687-8ffc-f17e358aa6cd"}, "bb9c35b4-c46f-4d11-9b6c-f0ede6ef325f": {"doc_hash": "b1350559a319a0b87f0bb0fe4e956dadc90f94dbd721ada213a081ce816b3d23", "ref_doc_id": "92b1ee21-17e6-4687-8ffc-f17e358aa6cd"}, "92b1ee21-17e6-4687-8ffc-f17e358aa6cd": {"doc_hash": "e10863ff9f4b30b52be4f6a0bfc81e32af8f8895ebd4867b62eb2b6a3e3a9d38"}, "22e4424b-0128-40f1-8b22-c1cb2018cf41": {"doc_hash": "e41aa9cf19ec3219d5373e2ea8d50c2ed9b879c070172ec0cedee3743c2ddba4", "ref_doc_id": "630a02af-a6f0-49a1-a0da-f7777b4b2bb4"}, "adfb16e5-f86a-4a2e-9ead-db4a6a416081": {"doc_hash": "5381b76dfb89f19a3ce2fb0cf6d4cf678eaddc4683427c88de5c853ebd55bccb", "ref_doc_id": "630a02af-a6f0-49a1-a0da-f7777b4b2bb4"}, "630a02af-a6f0-49a1-a0da-f7777b4b2bb4": {"doc_hash": "5df1adb8a66431ba3ee36f846cf2795a2852f5dcfeb981f05c2291d69230901c"}, "826169ad-f8bc-4ee3-ab6d-d2d574db0010": {"doc_hash": "0cdb893aefd9dd74f11a4f58bf4a3db4ddaf594ffa0895cd7882dd8466b7763c", "ref_doc_id": "6e19f99c-6744-45f6-9d5c-f080de5b3a82"}, "4882514a-722e-40c0-9745-118cfe076dfd": {"doc_hash": "6622d91d739c2bbc102be82e357a28382ee33ec7eaa84b79101018a6dc4db8b9", "ref_doc_id": "6e19f99c-6744-45f6-9d5c-f080de5b3a82"}, "6e19f99c-6744-45f6-9d5c-f080de5b3a82": {"doc_hash": "0dd5e07c87fc9fb34d251b7775a38ed8c91d8befde8d4feafedaca435bbdd616"}, "757d5b9b-4a0d-4cda-8fc3-8283fabd0bd7": {"doc_hash": "bc47310dc92ab03b13c5bfb912da0a1182a6d6654285883c5f4bb58afb198b90", "ref_doc_id": "18c787bc-611d-4a8a-b3a9-4d1452dcf72c"}, "18c787bc-611d-4a8a-b3a9-4d1452dcf72c": {"doc_hash": "bc47310dc92ab03b13c5bfb912da0a1182a6d6654285883c5f4bb58afb198b90"}, "5b0419e5-3c0b-4216-a6cb-1b9ef28c539e": {"doc_hash": "582287204be52fd76e14f7938ddd8be82ef338827981c151e1cfd40a62805fce", "ref_doc_id": "4d2f88f7-6990-40b9-8260-d98b8f09503a"}, "4d2f88f7-6990-40b9-8260-d98b8f09503a": {"doc_hash": "582287204be52fd76e14f7938ddd8be82ef338827981c151e1cfd40a62805fce"}, "2b9e3f48-e0fb-4762-87c1-a5cf7ab12953": {"doc_hash": "a6b049aaa00ba86d22de59c0239611f4a564efaa0ccd47af9586f34ca59b42fc", "ref_doc_id": "0106f731-6b04-4c5e-a4f7-19bbab2e6ec3"}, "0106f731-6b04-4c5e-a4f7-19bbab2e6ec3": {"doc_hash": "a6b049aaa00ba86d22de59c0239611f4a564efaa0ccd47af9586f34ca59b42fc"}, "bfaa64d9-281e-44f8-9f18-702155ca8164": {"doc_hash": "e79dcf6bbbb75556eceb70782bd6c28daf50de2785a6bee565ff34b4ac844d3e", "ref_doc_id": "36373ec7-c605-4a10-a7d0-dabfebf39ab4"}, "36373ec7-c605-4a10-a7d0-dabfebf39ab4": {"doc_hash": "e79dcf6bbbb75556eceb70782bd6c28daf50de2785a6bee565ff34b4ac844d3e"}, "21ff6bf5-5dd6-4bcd-a6df-736cc4cd6e47": {"doc_hash": "76d6c6d0dfd5f77f8343406489a9f70e244b299a421caaa44abcb18167b508de", "ref_doc_id": "6c625e08-5918-4915-940e-948812542cef"}, "6c625e08-5918-4915-940e-948812542cef": {"doc_hash": "76d6c6d0dfd5f77f8343406489a9f70e244b299a421caaa44abcb18167b508de"}, "1edfa7f3-fcd8-4559-881d-f975eb7a1e9d": {"doc_hash": "97251f0cf1773ed7c3d14ed74be8b6ca2fa0be6ec7811862b7fa66d37c9fd3b5", "ref_doc_id": "3ff55ed0-48b8-4bc6-b125-b08b3580a6dd"}, "3ff55ed0-48b8-4bc6-b125-b08b3580a6dd": {"doc_hash": "97251f0cf1773ed7c3d14ed74be8b6ca2fa0be6ec7811862b7fa66d37c9fd3b5"}, "6236f5ac-55c1-4ba8-a414-b988e20d5d41": {"doc_hash": "88bc1a51a293b66eddb1c8ba6102a67e125970d617f64b7490e9007911ea6174", "ref_doc_id": "77487a0f-ce2d-4c6f-8fb6-673b5caf6fe5"}, "77487a0f-ce2d-4c6f-8fb6-673b5caf6fe5": {"doc_hash": "88bc1a51a293b66eddb1c8ba6102a67e125970d617f64b7490e9007911ea6174"}, "d5a5cc34-8790-412b-8b86-62a828d920f4": {"doc_hash": "e50ff5d990448762a4d45fa16e019dba41570b8db874cc76d0135efec590450e", "ref_doc_id": "dfd4f146-1f22-4ac2-95d5-2f61fb56a7a8"}, "dfd4f146-1f22-4ac2-95d5-2f61fb56a7a8": {"doc_hash": "e50ff5d990448762a4d45fa16e019dba41570b8db874cc76d0135efec590450e"}, "cfc013de-1f3f-4243-83ed-b0e81ced7233": {"doc_hash": "3e3470b38729858ea0cff73c0ca8ffe00cf68394f3ab58205bb1f01e85250ae8", "ref_doc_id": "0f24a07b-c9be-4e42-bdcb-7e30954a8ffe"}, "0f24a07b-c9be-4e42-bdcb-7e30954a8ffe": {"doc_hash": "3e3470b38729858ea0cff73c0ca8ffe00cf68394f3ab58205bb1f01e85250ae8"}, "eaba8dc1-ebfb-4afd-a8df-d30d08b022bc": {"doc_hash": "7f822f62e0b8107a8960beaa955e86a54c36b027223b9fea2697b169240edcf9", "ref_doc_id": "9f44cff3-3389-4669-bd9c-57aecb6d0a02"}, "9f44cff3-3389-4669-bd9c-57aecb6d0a02": {"doc_hash": "7f822f62e0b8107a8960beaa955e86a54c36b027223b9fea2697b169240edcf9"}, "42d6eb62-4b84-4f9f-961f-10853248134b": {"doc_hash": "edeedf567308da555388854185c7053ac9734b427ac2d42c2897e086e9292d4d", "ref_doc_id": "a4784f93-1e40-40bc-b704-7d1897ddd5b2"}, "a4784f93-1e40-40bc-b704-7d1897ddd5b2": {"doc_hash": "edeedf567308da555388854185c7053ac9734b427ac2d42c2897e086e9292d4d"}, "cdb16ce0-e4d3-4eae-97cc-938e65c3a125": {"doc_hash": "b69d6b4b5527379c01dd43fbf88065fbf47462fffbbea16f73d1dfc08b32697b", "ref_doc_id": "977f62db-5ae3-4c30-9894-bc70f36b4aab"}, "552a83dd-66c5-4f09-b911-7003dd371440": {"doc_hash": "7c6774536a4ea4a8d3bb400799a69ab8f97ad790b439b2cfe9ff5f2a899db5f9", "ref_doc_id": "977f62db-5ae3-4c30-9894-bc70f36b4aab"}, "977f62db-5ae3-4c30-9894-bc70f36b4aab": {"doc_hash": "e4b6db3ef6f8e00cf87ef68aed8d347b8ff4b5ec168c8ab6cbced87d5d8b3310"}, "79f87622-c272-43db-9a42-9ab3546f3f81": {"doc_hash": "08c35fe7bd6a2db3e691e1fe255143a0b4351be093db97def2a38d12b23b4919", "ref_doc_id": "a86406b1-82af-402a-abc4-13bd1c437960"}, "fb7fe99a-99e2-49d0-8898-12795e1f9d64": {"doc_hash": "91004b832e421903dfef71954da6a10fc94786e8033bd1ee90cf32924ac57a79", "ref_doc_id": "a86406b1-82af-402a-abc4-13bd1c437960"}, "a86406b1-82af-402a-abc4-13bd1c437960": {"doc_hash": "eedbf569cb60f750df4966a059b24896b19610fdf85285b54480272215aa15b7"}, "a024aba3-0db0-47aa-b4a0-e46b6ef6d96e": {"doc_hash": "31bdaf630739fc713060ac36435151d0d9b3f16b757e5f4edeb5d7e8a3d17bf2", "ref_doc_id": "8c1c7397-49b4-487d-a322-cffaa0657796"}, "8c1c7397-49b4-487d-a322-cffaa0657796": {"doc_hash": "31bdaf630739fc713060ac36435151d0d9b3f16b757e5f4edeb5d7e8a3d17bf2"}, "5305d153-1ffa-4832-8953-5e288e691129": {"doc_hash": "504aa2a7b58b98bbec004b5095d9abfa6bd88ffca089f76b1f4111d734d45e5a", "ref_doc_id": "f5a172d7-42d1-43b0-9427-ffc3d4ba378d"}, "f5a172d7-42d1-43b0-9427-ffc3d4ba378d": {"doc_hash": "504aa2a7b58b98bbec004b5095d9abfa6bd88ffca089f76b1f4111d734d45e5a"}, "2d5307ba-c64e-45d1-bdb0-d2816238928f": {"doc_hash": "4d11cd9b714149cd7472f937ade24fb6832491161f1656db443e4f33147b21fe", "ref_doc_id": "f5ecf6f5-63a7-4960-a80d-cfb4b53530ce"}, "f5ecf6f5-63a7-4960-a80d-cfb4b53530ce": {"doc_hash": "4d11cd9b714149cd7472f937ade24fb6832491161f1656db443e4f33147b21fe"}, "ae1c0c83-f39a-4d9c-8263-504ee6d2fba1": {"doc_hash": "59da6ed26f60318d1fb3ba44b3accfe9145e4464f6af6ce0b841a9aa6ccaa2ea", "ref_doc_id": "0e1b2da5-cf8c-4eb5-813e-b9dc1c6347f7"}, "0e1b2da5-cf8c-4eb5-813e-b9dc1c6347f7": {"doc_hash": "59da6ed26f60318d1fb3ba44b3accfe9145e4464f6af6ce0b841a9aa6ccaa2ea"}, "2eb48f09-db99-44e6-b38b-24bb34dadfd7": {"doc_hash": "e7763cc2d90b535a953cba381c9c24e5f08aa032b6a1d66a871db64d27abaeb5", "ref_doc_id": "364fe5d6-468d-43f6-b7ca-5e5d7f053344"}, "364fe5d6-468d-43f6-b7ca-5e5d7f053344": {"doc_hash": "e7763cc2d90b535a953cba381c9c24e5f08aa032b6a1d66a871db64d27abaeb5"}, "6edcdb2d-a66d-4607-9f2c-62d3e3c92e53": {"doc_hash": "42ba2e1ca1a2c1496e0c061dcd2b434521600eda11741e74de8e47e65aee939a", "ref_doc_id": "9a27cd38-180c-4a9e-bdca-d5aead22ac1c"}, "9a27cd38-180c-4a9e-bdca-d5aead22ac1c": {"doc_hash": "42ba2e1ca1a2c1496e0c061dcd2b434521600eda11741e74de8e47e65aee939a"}, "e1155e61-e1f7-4284-a093-cbeb41075056": {"doc_hash": "b29062110faa4df455293b31b2ff6a6865524769986f51f45d182ee445e2ac15", "ref_doc_id": "971c53b4-edc5-45a3-8b57-5d85e1db9016"}, "971c53b4-edc5-45a3-8b57-5d85e1db9016": {"doc_hash": "b29062110faa4df455293b31b2ff6a6865524769986f51f45d182ee445e2ac15"}, "c100cfec-79f2-4ec3-81bc-cea2c15c8c04": {"doc_hash": "df0b99b893ef25191001150b0d5da3b2c4139e5c05ff1dab7f77b9a2dd5786cd", "ref_doc_id": "70e098a7-cb12-49e9-90f0-6f318b1cec1e"}, "70e098a7-cb12-49e9-90f0-6f318b1cec1e": {"doc_hash": "df0b99b893ef25191001150b0d5da3b2c4139e5c05ff1dab7f77b9a2dd5786cd"}, "bb4d7243-b0df-418d-bc36-d6f0082837ae": {"doc_hash": "91797361092b5a0e1068a455df20138541b2f9b4c2e717c2fec7c177e3c616b3", "ref_doc_id": "69dd8665-df26-427f-8e7e-6ecd450afeed"}, "69dd8665-df26-427f-8e7e-6ecd450afeed": {"doc_hash": "91797361092b5a0e1068a455df20138541b2f9b4c2e717c2fec7c177e3c616b3"}, "392bf67f-87c0-4314-be10-85214fbdf33f": {"doc_hash": "fdb56d3bfe27718c3e625c800b1aefe32c00417ad3a45e00dad636bcdfba85c5", "ref_doc_id": "015df7ce-1350-4eb1-be49-202f048f1d8c"}, "015df7ce-1350-4eb1-be49-202f048f1d8c": {"doc_hash": "fdb56d3bfe27718c3e625c800b1aefe32c00417ad3a45e00dad636bcdfba85c5"}, "2e651499-18f1-4274-b19f-84d2890907a0": {"doc_hash": "4059c24c574493f5a3e905b5cfb7d0e716e30302f2e63dd2ed02166ec0b4ea47", "ref_doc_id": "f91b6639-de85-4e31-a5af-2f4e1fe81c63"}, "f91b6639-de85-4e31-a5af-2f4e1fe81c63": {"doc_hash": "4059c24c574493f5a3e905b5cfb7d0e716e30302f2e63dd2ed02166ec0b4ea47"}, "6db20671-1b6d-46af-bf8d-8cb72265d3ef": {"doc_hash": "15866950868fc0a0148239aec31c4cbdf385004d349d12a3830140af20a7c6a2", "ref_doc_id": "6a80cee8-0c5c-413c-80c4-41ce5fda37c5"}, "6a80cee8-0c5c-413c-80c4-41ce5fda37c5": {"doc_hash": "15866950868fc0a0148239aec31c4cbdf385004d349d12a3830140af20a7c6a2"}, "0b5e25da-73d5-4cd7-bad2-5ae3861eaeae": {"doc_hash": "787f9ea4fadc1f8c10acca49be3a28b0a62a17ac0e23626d8f894d18922c3127", "ref_doc_id": "a0598a94-87e7-4371-b1bc-57cb43d7f170"}, "a0598a94-87e7-4371-b1bc-57cb43d7f170": {"doc_hash": "787f9ea4fadc1f8c10acca49be3a28b0a62a17ac0e23626d8f894d18922c3127"}, "8f00621d-ed16-4187-ae96-18c225662b4f": {"doc_hash": "109cc4d227749c62ad6df9f0fd5b1f0e3546346a360666cb8492e74e2397c3db", "ref_doc_id": "bc9f1828-9234-4f3e-a26a-5e3c15eecbb0"}, "bc9f1828-9234-4f3e-a26a-5e3c15eecbb0": {"doc_hash": "109cc4d227749c62ad6df9f0fd5b1f0e3546346a360666cb8492e74e2397c3db"}, "512269f4-6819-4397-85db-73fd4dc2c89e": {"doc_hash": "9c8dccee9c42dcdfdc06e849dd54156b55d9ef893462c8bf977880e739da7919", "ref_doc_id": "b71ba31a-08d2-42b9-987b-f673b7df3fd1"}, "b71ba31a-08d2-42b9-987b-f673b7df3fd1": {"doc_hash": "9c8dccee9c42dcdfdc06e849dd54156b55d9ef893462c8bf977880e739da7919"}, "267f7280-d569-43ee-9df0-d219764f6d73": {"doc_hash": "20dac8ff89c0815771bcad7ec26efb901b7eeae16ac126618b127049ddb59e8c", "ref_doc_id": "29354eef-f596-4838-ab17-22df58635013"}, "29354eef-f596-4838-ab17-22df58635013": {"doc_hash": "20dac8ff89c0815771bcad7ec26efb901b7eeae16ac126618b127049ddb59e8c"}, "916dca71-aa48-4099-9dbd-d856490fa3b5": {"doc_hash": "52e989e4a15e6970c684619eac3060379f0d43c0081e198f30194d96c6b3c50b", "ref_doc_id": "f4c2ed1a-eb63-44e4-9521-49234f677153"}, "f4c2ed1a-eb63-44e4-9521-49234f677153": {"doc_hash": "52e989e4a15e6970c684619eac3060379f0d43c0081e198f30194d96c6b3c50b"}, "958a3879-74c7-4334-a515-cd24fe969dad": {"doc_hash": "d01ad1eaa7877726cd4da94f8a2d69f6ea9441800ee5f1470b3df587bac24474", "ref_doc_id": "cfd7b93e-cd0f-412b-b8cc-aa99b789983b"}, "cfd7b93e-cd0f-412b-b8cc-aa99b789983b": {"doc_hash": "d01ad1eaa7877726cd4da94f8a2d69f6ea9441800ee5f1470b3df587bac24474"}, "5ef5fe84-b6ae-4bac-b997-179506dd931c": {"doc_hash": "d035efd7768237980ada8b3eb4bc4f05c9c9f0188588c3c1909a1151ad0039ea", "ref_doc_id": "342d9846-0f4c-49be-8709-a782cfef0dc7"}, "342d9846-0f4c-49be-8709-a782cfef0dc7": {"doc_hash": "d035efd7768237980ada8b3eb4bc4f05c9c9f0188588c3c1909a1151ad0039ea"}, "62f85a7f-b2ed-4ad0-8c46-28912c5d2bcc": {"doc_hash": "3d7859eeca72e861722b847c4497431b76e56e85c1b74a8d765cab86c6c489b8", "ref_doc_id": "036f2df1-83bd-4fbf-ac24-574f990eac25"}, "036f2df1-83bd-4fbf-ac24-574f990eac25": {"doc_hash": "3d7859eeca72e861722b847c4497431b76e56e85c1b74a8d765cab86c6c489b8"}, "0328018c-58a3-4764-86e4-e82f0ba70e9c": {"doc_hash": "0d1679e3f71f30b8024445f7dfbafbab6e3ccc615038c3b2e8a475fb711ba38d", "ref_doc_id": "06193f4e-08e3-4391-bee1-b5c3267519fe"}, "06193f4e-08e3-4391-bee1-b5c3267519fe": {"doc_hash": "0d1679e3f71f30b8024445f7dfbafbab6e3ccc615038c3b2e8a475fb711ba38d"}, "f5021a88-0bee-4a68-bad4-e0b5d9853244": {"doc_hash": "c5fae72937f5a856698b8273c5000a855a3c57b6cd1ac7a04975060f7c0d762f", "ref_doc_id": "c5c59789-93f5-4713-b9f9-15f156b3474a"}, "c5c59789-93f5-4713-b9f9-15f156b3474a": {"doc_hash": "c5fae72937f5a856698b8273c5000a855a3c57b6cd1ac7a04975060f7c0d762f"}, "d6240849-f445-43d9-b426-56f05f6fe309": {"doc_hash": "706c7ef90060a8284f43becc888fd668ca5d1442fa9fefadabc56e95693725c1", "ref_doc_id": "4b65baf1-aeae-4d42-be33-a2ad8710486b"}, "4b65baf1-aeae-4d42-be33-a2ad8710486b": {"doc_hash": "706c7ef90060a8284f43becc888fd668ca5d1442fa9fefadabc56e95693725c1"}, "8ebd50a2-ebff-42a1-9235-38c7d09891cc": {"doc_hash": "466305e816ff32146932e03fffecc958f797998a24943e4b13d94685d7b3b82e", "ref_doc_id": "4953d18c-2de0-4893-a83a-d257a78a1906"}, "4953d18c-2de0-4893-a83a-d257a78a1906": {"doc_hash": "466305e816ff32146932e03fffecc958f797998a24943e4b13d94685d7b3b82e"}, "091ba85c-fac6-4b60-9034-aadfde27e120": {"doc_hash": "385a11f4f3400e4537513b4d5d179ec096a5bfbac1bbdb3c6540185db43cb29b", "ref_doc_id": "27ccebda-b20a-4a9b-8082-7d324e60e6b2"}, "27ccebda-b20a-4a9b-8082-7d324e60e6b2": {"doc_hash": "385a11f4f3400e4537513b4d5d179ec096a5bfbac1bbdb3c6540185db43cb29b"}, "d5db3e43-6cbc-4789-938b-9e7c8f89ae61": {"doc_hash": "b6dae6fb54c5ad13f4995d1a1264706cbbee9cd64be42a2b84379f6040758666", "ref_doc_id": "1e47060c-0d14-4de2-9b80-a1a54f4c0239"}, "1e47060c-0d14-4de2-9b80-a1a54f4c0239": {"doc_hash": "b6dae6fb54c5ad13f4995d1a1264706cbbee9cd64be42a2b84379f6040758666"}, "5bb13d0a-0c0e-4d41-8637-759b7b4b84fe": {"doc_hash": "196e18b382da3d196a322a33adcbe784d258f7b775b7f3b0e9925f128d1b6d56", "ref_doc_id": "d6fcc8e6-9c1e-4d53-b521-96e0516b3fc0"}, "d6fcc8e6-9c1e-4d53-b521-96e0516b3fc0": {"doc_hash": "196e18b382da3d196a322a33adcbe784d258f7b775b7f3b0e9925f128d1b6d56"}, "d98277da-972a-47da-a4db-dbb14fd4a2a0": {"doc_hash": "4dfedc8d100742bfacd8f9bf72bb81a6856d234d4c7155fad0b0ebb18d67d6c5", "ref_doc_id": "a0aa37b6-4652-40cb-b4a1-4b1265a7d405"}, "a0aa37b6-4652-40cb-b4a1-4b1265a7d405": {"doc_hash": "4dfedc8d100742bfacd8f9bf72bb81a6856d234d4c7155fad0b0ebb18d67d6c5"}, "4da2745e-0b0d-4c74-b0e6-061629287d5c": {"doc_hash": "a2956b13324132ee3f0ab02ddc06176b77f7a2be943c52aa0bf454b9e40fc31c", "ref_doc_id": "1aeae644-84ac-4441-bfa7-5454889b28a2"}, "1aeae644-84ac-4441-bfa7-5454889b28a2": {"doc_hash": "a2956b13324132ee3f0ab02ddc06176b77f7a2be943c52aa0bf454b9e40fc31c"}, "2644b374-3d3e-4b54-abcf-7eda9786abaa": {"doc_hash": "b5db2bcfedd0900a09df32b374e85109e0aea989bfcc846bde61f4756093585b", "ref_doc_id": "ff3c6086-ca55-4930-9659-6fed26414247"}, "ff3c6086-ca55-4930-9659-6fed26414247": {"doc_hash": "b5db2bcfedd0900a09df32b374e85109e0aea989bfcc846bde61f4756093585b"}, "d83c8768-a112-45c0-9791-99ac24e81c4f": {"doc_hash": "9a5ab0186743377bac8aa73b00f416626dca63cb8d3cea0b5334668be3908155", "ref_doc_id": "2e371e51-35fd-45ff-b21a-3bbd71734641"}, "2e371e51-35fd-45ff-b21a-3bbd71734641": {"doc_hash": "9a5ab0186743377bac8aa73b00f416626dca63cb8d3cea0b5334668be3908155"}, "e4a91b6b-8e83-4992-bb97-2b2a39ac06e1": {"doc_hash": "2539f92834c7942864d582202f23d5ca4e8018e57a8c0bdcf13ea51acd835707", "ref_doc_id": "95cadd8f-bad4-4c54-9f56-c41bc2c99ead"}, "95cadd8f-bad4-4c54-9f56-c41bc2c99ead": {"doc_hash": "2539f92834c7942864d582202f23d5ca4e8018e57a8c0bdcf13ea51acd835707"}, "d66c1a9d-d697-455c-b117-bfb424cf0a22": {"doc_hash": "895b6d36af84bcd07f9a92426127f5b0d9d6efe7e5b42d01d557bc6ed74adb1d", "ref_doc_id": "83cbe7cc-473b-45aa-8ddb-448d94bbed6b"}, "83cbe7cc-473b-45aa-8ddb-448d94bbed6b": {"doc_hash": "895b6d36af84bcd07f9a92426127f5b0d9d6efe7e5b42d01d557bc6ed74adb1d"}, "251ecb84-4941-471d-80f7-99878bfa12a5": {"doc_hash": "dc0e2be539fae65ec89e7fe5635107870008213957d821a6dd46756bb6313bfe", "ref_doc_id": "66d34dbe-b05f-408a-b513-dc4e05e944f1"}, "66d34dbe-b05f-408a-b513-dc4e05e944f1": {"doc_hash": "dc0e2be539fae65ec89e7fe5635107870008213957d821a6dd46756bb6313bfe"}, "746f5273-241e-4b90-b12f-d313726edb61": {"doc_hash": "62f845e98e1e78db8259bc2370e5469d1dd5a829e81efa241f33e12ddc51acdb", "ref_doc_id": "890696bb-913d-48b7-8bf0-178c85f587dd"}, "890696bb-913d-48b7-8bf0-178c85f587dd": {"doc_hash": "62f845e98e1e78db8259bc2370e5469d1dd5a829e81efa241f33e12ddc51acdb"}, "b5d85c14-d5a7-4a60-897e-0ffabbc0a6b0": {"doc_hash": "be0f2347f52745d58683f4e87a5c9fa2afe5bd52e458601b0f597ca732cc8760", "ref_doc_id": "4ca97086-52c7-4623-a6ee-6ab22c932d9d"}, "4ca97086-52c7-4623-a6ee-6ab22c932d9d": {"doc_hash": "be0f2347f52745d58683f4e87a5c9fa2afe5bd52e458601b0f597ca732cc8760"}, "83edb6e7-4a1f-4573-91d7-3a8a0c5f55c0": {"doc_hash": "62c74fe437fadeda3a6829baf1dd1741c60674c000941c856921b860e15ed8e7", "ref_doc_id": "58d6ed7f-4172-4844-9aec-42f5dc0b3a72"}, "58d6ed7f-4172-4844-9aec-42f5dc0b3a72": {"doc_hash": "62c74fe437fadeda3a6829baf1dd1741c60674c000941c856921b860e15ed8e7"}, "648e18b0-4c6d-40a4-ab38-5aa0a801b990": {"doc_hash": "ac92dfd69cbb2cf753e5313c6eb90224931b69e73213a623ee1858fbe1324fb3", "ref_doc_id": "bc6bfe10-9059-4bb8-9344-6e0a9a44453a"}, "bc6bfe10-9059-4bb8-9344-6e0a9a44453a": {"doc_hash": "ac92dfd69cbb2cf753e5313c6eb90224931b69e73213a623ee1858fbe1324fb3"}, "d9fc8522-7228-4fc9-9f0f-b5b778065903": {"doc_hash": "81e9e2e9096c308e4762b8daae3c750b96a26b7f09bd2d5c6b1de599ad8c525f", "ref_doc_id": "89145a0d-9d8b-419e-99e3-1f2f994c90af"}, "89145a0d-9d8b-419e-99e3-1f2f994c90af": {"doc_hash": "81e9e2e9096c308e4762b8daae3c750b96a26b7f09bd2d5c6b1de599ad8c525f"}, "59de3b7c-d2a5-48c9-aa5b-6307d72db195": {"doc_hash": "78c4af992aa1f40f89dae2b04034abbf2686ea3135da8617f93db3cad8bcdc30", "ref_doc_id": "e01f050f-8ce6-400b-8e5a-bf121354d4b3"}, "e01f050f-8ce6-400b-8e5a-bf121354d4b3": {"doc_hash": "78c4af992aa1f40f89dae2b04034abbf2686ea3135da8617f93db3cad8bcdc30"}, "6409eccd-8f11-4996-aa96-428f4b1d9111": {"doc_hash": "af6f2ef48328ed946751f55fb2ce327d87818cde3ba686173f79aed7e4679012", "ref_doc_id": "b7ea9f0d-6b30-4c7a-af36-1261e652e802"}, "b7ea9f0d-6b30-4c7a-af36-1261e652e802": {"doc_hash": "af6f2ef48328ed946751f55fb2ce327d87818cde3ba686173f79aed7e4679012"}, "591a3c26-67e7-4e26-b48e-3bc315127b37": {"doc_hash": "b94e29dab2cdccd1b6a721c384d16acc3528223c04c8825dbff7bb51123503ac", "ref_doc_id": "bdbfcd10-ecab-48cd-a241-66da56e47009"}, "bdbfcd10-ecab-48cd-a241-66da56e47009": {"doc_hash": "b94e29dab2cdccd1b6a721c384d16acc3528223c04c8825dbff7bb51123503ac"}, "987d3b75-e830-40c8-9953-6f069806614f": {"doc_hash": "5e030e68cd01cc38b5a17187999a3cda7d085accfa1d8493acb7139ed4691b33", "ref_doc_id": "81d2085b-7ec5-4ff4-814c-14dac6356574"}, "81d2085b-7ec5-4ff4-814c-14dac6356574": {"doc_hash": "5e030e68cd01cc38b5a17187999a3cda7d085accfa1d8493acb7139ed4691b33"}, "67c885c3-e792-4e3e-bfdf-1645c2152a6c": {"doc_hash": "c7d68845b9f438d9cce6c4c5490a3bc78618b21274810f4fc4d548538f360ca6", "ref_doc_id": "f6f6c489-68de-42ea-b61b-22d7e077719f"}, "f6f6c489-68de-42ea-b61b-22d7e077719f": {"doc_hash": "c7d68845b9f438d9cce6c4c5490a3bc78618b21274810f4fc4d548538f360ca6"}, "963d2eb3-cb6c-4f0e-86a2-7b478db1a8d8": {"doc_hash": "739d62da9925a279a297381b852f1cb031ac72f6e5ae9185ccef7f3c08cf57a7", "ref_doc_id": "d41a8915-3c59-461a-a299-39e74e2efbf6"}, "d41a8915-3c59-461a-a299-39e74e2efbf6": {"doc_hash": "739d62da9925a279a297381b852f1cb031ac72f6e5ae9185ccef7f3c08cf57a7"}, "335f40cb-086a-4dae-9e21-34b5db01163b": {"doc_hash": "efceac1127f834997b7fdb97905917c42b512259e93180b9a47d5d3eed0aa0d4", "ref_doc_id": "de5232a8-8d6e-403a-be1d-a3cf0c1f054f"}, "de5232a8-8d6e-403a-be1d-a3cf0c1f054f": {"doc_hash": "efceac1127f834997b7fdb97905917c42b512259e93180b9a47d5d3eed0aa0d4"}, "658e9d6d-c953-4d1a-a60d-03f50e9934c9": {"doc_hash": "068610eba4a9d411639cdedc68053d28939e0a715631ead65ae9e35f5ecc8d6e", "ref_doc_id": "2a0533b6-6b80-4f19-a67f-29ada59f36c3"}, "2a0533b6-6b80-4f19-a67f-29ada59f36c3": {"doc_hash": "068610eba4a9d411639cdedc68053d28939e0a715631ead65ae9e35f5ecc8d6e"}, "0755e2b9-e6a7-4ead-9d77-421982f2d2aa": {"doc_hash": "f7d00c63f75cc03d23f515d8dce5942531df6736d7319abd31378940c82ac834", "ref_doc_id": "63a056cc-7a11-4afa-848b-a33aa74fe1ae"}, "63a056cc-7a11-4afa-848b-a33aa74fe1ae": {"doc_hash": "f7d00c63f75cc03d23f515d8dce5942531df6736d7319abd31378940c82ac834"}, "737b0b7f-7022-45c8-82eb-6b2f3f1fdf0b": {"doc_hash": "f179afef9545dc705f8f3bdfc38a53dea96f8243324b8902042b57965fd43cb2", "ref_doc_id": "8480a70b-c272-4202-9914-49f28cc03380"}, "8480a70b-c272-4202-9914-49f28cc03380": {"doc_hash": "f179afef9545dc705f8f3bdfc38a53dea96f8243324b8902042b57965fd43cb2"}, "9791766d-d5ed-460a-8c28-23fac18b7adc": {"doc_hash": "2cf03efc8743ba5bf786c6f486c5dc28afa7f49c67c0de706d3111d580358fd9", "ref_doc_id": "a378b593-77ab-42f0-974f-daeb5f4b0100"}, "a378b593-77ab-42f0-974f-daeb5f4b0100": {"doc_hash": "2cf03efc8743ba5bf786c6f486c5dc28afa7f49c67c0de706d3111d580358fd9"}, "d76ed900-7cfd-4104-865f-434f521fe7de": {"doc_hash": "e0bb3978573301260ee42d432db2ea5182a9b9e79f353cdcae6f5f7ccf822562", "ref_doc_id": "1465d6ff-9d30-497e-a50e-f8244d6da58b"}, "1465d6ff-9d30-497e-a50e-f8244d6da58b": {"doc_hash": "e0bb3978573301260ee42d432db2ea5182a9b9e79f353cdcae6f5f7ccf822562"}, "4e0948a7-6439-4e59-a1b4-60c72ef8b72d": {"doc_hash": "1b425b7182c842864853d5cdb3ddad1f847df12bb0605313b1fe64bd73cfeae4", "ref_doc_id": "a8548aac-f163-4ade-9fdb-695f29dfcd7d"}, "a8548aac-f163-4ade-9fdb-695f29dfcd7d": {"doc_hash": "1b425b7182c842864853d5cdb3ddad1f847df12bb0605313b1fe64bd73cfeae4"}, "d26b1aad-57d3-4d07-9ffb-6180f7eadf28": {"doc_hash": "fd1fd9fbdfb2b9fcfd6503dba0f5875b718dc51ea27f638d8af3984aefdbf8d8", "ref_doc_id": "b8e71907-ed07-4ad2-900d-c952ee1e15f1"}, "b8e71907-ed07-4ad2-900d-c952ee1e15f1": {"doc_hash": "f1534c2c5f7569d32600c87d47bb3e8c1b455d68b7e410f99256e3985730f73a"}, "5be204de-4c9a-4ab6-8e65-a6445f1df6f7": {"doc_hash": "691c1ea3f5af94304220a525b425804862164853c932ace5049f3f15b07d595d", "ref_doc_id": "71ff7763-70c1-4ba0-b3e0-381ab5f7e326"}, "71ff7763-70c1-4ba0-b3e0-381ab5f7e326": {"doc_hash": "691c1ea3f5af94304220a525b425804862164853c932ace5049f3f15b07d595d"}, "3594404b-fa6d-4464-a267-444e42d6e91a": {"doc_hash": "8d57d4fc711f373b06a3c129a852b6dcfeaa7cd8a8e948c6b37ad80531ee0925", "ref_doc_id": "e2b60e37-7467-42ba-b8ed-65360f08e9d7"}, "e2b60e37-7467-42ba-b8ed-65360f08e9d7": {"doc_hash": "ba6ae654f18113711c2a15cd578ea3e663493597f642e8c0418a201a93c2ec66"}, "20c65302-a4de-461c-8d84-7ae13a230db1": {"doc_hash": "d1914395d14da97ef6ef97d635e6d06092cef0b1ccf70a01709beac4379909a5", "ref_doc_id": "9dc86724-6e17-4cd4-80a0-58f5532c16d9"}, "9dc86724-6e17-4cd4-80a0-58f5532c16d9": {"doc_hash": "d1914395d14da97ef6ef97d635e6d06092cef0b1ccf70a01709beac4379909a5"}, "87775247-affe-4a3c-81d2-8decc8b52bcd": {"doc_hash": "70df256c50a0044642bfa023130a39823350f72a8d67dcf3bbce315207e5dc45", "ref_doc_id": "22521c83-657b-493c-9771-0769af46ea67"}, "22521c83-657b-493c-9771-0769af46ea67": {"doc_hash": "70df256c50a0044642bfa023130a39823350f72a8d67dcf3bbce315207e5dc45"}, "bcfb5cb1-b538-4ba9-ae12-ec64661d77b2": {"doc_hash": "6f147158daa87e20a45b143bc84835075445dfaa2016f580a1eff05d0e3e4882", "ref_doc_id": "aac21f64-7b12-4106-92e6-5c1befea94b5"}, "aac21f64-7b12-4106-92e6-5c1befea94b5": {"doc_hash": "6f147158daa87e20a45b143bc84835075445dfaa2016f580a1eff05d0e3e4882"}, "707eaeeb-ba1e-413f-8771-dd770b4fe935": {"doc_hash": "856fe58890486b9c0b106071f17ab681e36261e6e7492f18c1b51908b188e059", "ref_doc_id": "c1614896-fd33-4eec-86eb-a93b610e695b"}, "c1614896-fd33-4eec-86eb-a93b610e695b": {"doc_hash": "856fe58890486b9c0b106071f17ab681e36261e6e7492f18c1b51908b188e059"}, "16add095-16fb-402d-a2bb-ef3d959e75a4": {"doc_hash": "56ffeacbd6487d9d45a055364fafc2c48662ec6a2f11db8bf94d021eaba6a6a8", "ref_doc_id": "9ed0ca2a-38bd-44e2-90f2-646efc7ef16f"}, "9ed0ca2a-38bd-44e2-90f2-646efc7ef16f": {"doc_hash": "56ffeacbd6487d9d45a055364fafc2c48662ec6a2f11db8bf94d021eaba6a6a8"}, "70a41518-a768-45eb-8b46-c2a2604ab947": {"doc_hash": "ad9692643df8f6b1b05441e61761bc8aa44caa313918a0592765c85f96d54761", "ref_doc_id": "9052fea2-9663-4c44-bf9b-e10215740360"}, "9052fea2-9663-4c44-bf9b-e10215740360": {"doc_hash": "ad9692643df8f6b1b05441e61761bc8aa44caa313918a0592765c85f96d54761"}, "0a89dcb6-ee6e-481d-9821-5c1223e960be": {"doc_hash": "4235b914964349e6eba81e706e7e69804c43f0c0a69fe2568b662af101642373", "ref_doc_id": "53319711-0487-4af7-a8de-4ea21f7578ec"}, "53319711-0487-4af7-a8de-4ea21f7578ec": {"doc_hash": "4235b914964349e6eba81e706e7e69804c43f0c0a69fe2568b662af101642373"}, "2223dc2a-477f-4d2a-8b2c-3d65cc836096": {"doc_hash": "e8fba60af84822cad20e588dea2a69a3b8140f76484a7608e25a610d8bdd37bd", "ref_doc_id": "58a34927-bc19-4122-99e8-8c6aac4e1340"}, "58a34927-bc19-4122-99e8-8c6aac4e1340": {"doc_hash": "e8fba60af84822cad20e588dea2a69a3b8140f76484a7608e25a610d8bdd37bd"}, "9dd82802-042c-4cde-9fc9-1559daa32726": {"doc_hash": "83c6335000e4c54fb2e10c2963bb7675081cf8b2ef5c988a105061aeeef180ff", "ref_doc_id": "1215ada6-70ee-44a0-9294-a9577923c618"}, "1215ada6-70ee-44a0-9294-a9577923c618": {"doc_hash": "9b60beaad04b20c70fe326fce04eb2a9c27e3fceaa4aff7b1efea9b4f0b2085b"}, "04d6b4ca-275e-47d4-b6aa-c4d15d4352f7": {"doc_hash": "a8acb55ce3a9f029b298a08f07ee260cfe576276bfd192052ef2f45e15fc61ce", "ref_doc_id": "8983ea67-a0cb-4495-aa8b-91d3c049d545"}, "8983ea67-a0cb-4495-aa8b-91d3c049d545": {"doc_hash": "a8acb55ce3a9f029b298a08f07ee260cfe576276bfd192052ef2f45e15fc61ce"}, "2ae01f91-64ec-4665-b86f-f3943f2d80e5": {"doc_hash": "eeff6734de988edcdf8a6ce099677d6a7a3f16444c3779da793d496d6e677bf4", "ref_doc_id": "24bbb848-2d8d-4747-9078-8cf167723313"}, "24bbb848-2d8d-4747-9078-8cf167723313": {"doc_hash": "eeff6734de988edcdf8a6ce099677d6a7a3f16444c3779da793d496d6e677bf4"}, "8ebfdcd1-7ba9-49e4-a940-a413c71e6d4d": {"doc_hash": "c834f65c3403536be30cab3b69fc8e61ab51c7fda2cdd80431ca4fef4aa1145a", "ref_doc_id": "9cd57a24-3832-40ee-adce-40f60d196789"}, "9cd57a24-3832-40ee-adce-40f60d196789": {"doc_hash": "c834f65c3403536be30cab3b69fc8e61ab51c7fda2cdd80431ca4fef4aa1145a"}, "1ef52f19-eeb7-46c8-b3f8-30e64dd29116": {"doc_hash": "0c0abf066589d52e5a14f341a6000e47056838c078bfa1eb663ad0d5885767fa", "ref_doc_id": "616ca7cb-6011-4c91-adc7-825db5716800"}, "616ca7cb-6011-4c91-adc7-825db5716800": {"doc_hash": "0c0abf066589d52e5a14f341a6000e47056838c078bfa1eb663ad0d5885767fa"}, "9a3beea0-c8c8-4b22-bd9a-7cd8d9952251": {"doc_hash": "20f488aafddcbb44684438ccf208797abcb9e1341a94ded989784edd84e99482", "ref_doc_id": "fc293cc2-4188-49ea-8db7-c4c03957d035"}, "fc293cc2-4188-49ea-8db7-c4c03957d035": {"doc_hash": "20f488aafddcbb44684438ccf208797abcb9e1341a94ded989784edd84e99482"}, "f8f61c07-ad1b-4d12-b989-79a86bf01626": {"doc_hash": "b576003e1dc49cd73a4d73b0411371f40f18c4223d306447c78973811918103a", "ref_doc_id": "e71d4d19-5fbf-43f0-b7aa-58b12ff21e7f"}, "e71d4d19-5fbf-43f0-b7aa-58b12ff21e7f": {"doc_hash": "b576003e1dc49cd73a4d73b0411371f40f18c4223d306447c78973811918103a"}, "31ac444b-25c4-4fd6-9d30-26fd1ddf40b0": {"doc_hash": "a66a575a7372aa4c38354dc1d04ba8d1fe4ca70ac31a2506e20104c42923ee06", "ref_doc_id": "f27b4b8f-63b3-4014-a96e-981309692954"}, "f27b4b8f-63b3-4014-a96e-981309692954": {"doc_hash": "a66a575a7372aa4c38354dc1d04ba8d1fe4ca70ac31a2506e20104c42923ee06"}, "3c5fddc5-2ba2-4983-a20c-ddd02e6fc511": {"doc_hash": "98d1d3531186dfd6c7052f922e88145998983b008b5ff1ae3dd5fdfc4667aed0", "ref_doc_id": "539e6964-7a4a-4993-a8ae-925fae6721c8"}, "539e6964-7a4a-4993-a8ae-925fae6721c8": {"doc_hash": "98d1d3531186dfd6c7052f922e88145998983b008b5ff1ae3dd5fdfc4667aed0"}, "31ed4aac-1f33-4182-b246-17e1fb8e010f": {"doc_hash": "536d0f4e5c0f9248d0fd911c22279b8e435ed179723be82bdca0be0c4b9008df", "ref_doc_id": "622a6480-5f66-4e0b-af7c-a23d1f34274b"}, "622a6480-5f66-4e0b-af7c-a23d1f34274b": {"doc_hash": "536d0f4e5c0f9248d0fd911c22279b8e435ed179723be82bdca0be0c4b9008df"}, "e482d595-4d7d-48e0-9928-e741f044985b": {"doc_hash": "494871d2b3f375d291aa7cfe0c3e6867fd32f9d21f9fb4739d22205daaf51c56", "ref_doc_id": "744cc8f4-262a-41a5-93ce-da03dc77eff2"}, "744cc8f4-262a-41a5-93ce-da03dc77eff2": {"doc_hash": "494871d2b3f375d291aa7cfe0c3e6867fd32f9d21f9fb4739d22205daaf51c56"}, "84022dfd-21cb-416b-9c0a-664704213651": {"doc_hash": "17b09acb0281ce3cf3f9e9baf42205516886491b434972e3241436f0acb2791a", "ref_doc_id": "aa3c1681-c090-44a9-93c8-d54877d1f90d"}, "aa3c1681-c090-44a9-93c8-d54877d1f90d": {"doc_hash": "17b09acb0281ce3cf3f9e9baf42205516886491b434972e3241436f0acb2791a"}, "bbe3eb96-8bcd-432c-b760-dd12bf47c8e5": {"doc_hash": "6c645da618b9e514869df0b9606d6ce674835009f6d929773fdcaa752c385118", "ref_doc_id": "ed023010-941c-46bc-ade2-b034dc4ac6d4"}, "ed023010-941c-46bc-ade2-b034dc4ac6d4": {"doc_hash": "6c645da618b9e514869df0b9606d6ce674835009f6d929773fdcaa752c385118"}, "d082078b-b919-4868-9e79-c0609a175c27": {"doc_hash": "1cbac1fc234395041eaf326ab8aeae2b61fdfc66db6d021c05d799d7588cfb7a", "ref_doc_id": "be67dabc-5f36-43d8-a4b5-bd21080b6919"}, "be67dabc-5f36-43d8-a4b5-bd21080b6919": {"doc_hash": "1cbac1fc234395041eaf326ab8aeae2b61fdfc66db6d021c05d799d7588cfb7a"}, "5b77c31e-4be9-459a-9a67-371935c11422": {"doc_hash": "9d839ef35f041d42eea60feaa97dd0b4a986fb8c8fe9f1eab79ff95e32c4936f", "ref_doc_id": "0f09bbff-fe27-4862-acf9-3034a08791d6"}, "0f09bbff-fe27-4862-acf9-3034a08791d6": {"doc_hash": "9d839ef35f041d42eea60feaa97dd0b4a986fb8c8fe9f1eab79ff95e32c4936f"}, "37f21a04-918c-4e62-81b6-5a65ddfdf6ae": {"doc_hash": "0f788be1969e83dbf0fc47fe42f326aa99b016a58c4cf4aaac23957b7bc3c95b", "ref_doc_id": "204dcbc6-3086-436d-ab1e-9e879a1476b8"}, "204dcbc6-3086-436d-ab1e-9e879a1476b8": {"doc_hash": "0f788be1969e83dbf0fc47fe42f326aa99b016a58c4cf4aaac23957b7bc3c95b"}, "7cfe8f32-a81b-4bca-a518-bf4d0678d828": {"doc_hash": "a95b923ab458b78456d125d4d96366724f39417a9310d9cc9b4b18b806f3ddb6", "ref_doc_id": "31cc4bcf-e5c2-456e-8c93-d36f571def8a"}, "31cc4bcf-e5c2-456e-8c93-d36f571def8a": {"doc_hash": "a95b923ab458b78456d125d4d96366724f39417a9310d9cc9b4b18b806f3ddb6"}}, "docstore/ref_doc_info": {"470aea9e-eee4-4af0-8a39-60b8b670815c": {"node_ids": ["0b4382ff-e79b-47c3-84d5-167c60e1053a"], "metadata": {"page_label": "1", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_704a56a9-9789-4413-9f94-73d6eda34e2b"}}, "559ad40b-e79b-47cb-82f0-052cd2898e26": {"node_ids": ["7e43b9b6-413a-44f0-bd29-d84376d7b73b"], "metadata": {"page_label": "2", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bb943d75-8df1-49b3-976f-ccdb5f54cd04"}}, "bc53427c-1dc4-4c9f-bfa4-f0d38a0043ef": {"node_ids": ["c0b87291-6364-437a-8497-9c194e92c071"], "metadata": {"page_label": "3", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0e226579-7dde-4f51-adb3-27afe150a8df"}}, "f70ecc57-7f31-4023-b1ce-8e853d34e49f": {"node_ids": ["4c776d2e-0f89-4772-9183-963b6b147d22"], "metadata": {"page_label": "4", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5d4704c4-606f-4608-a475-cfe215f9aa27"}}, "36c33ffa-c012-4413-bb86-9fac6a6d7796": {"node_ids": ["ad582ac4-aa80-4733-8ea7-f44adb1d7da3"], "metadata": {"page_label": "5", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0e1bb44f-303c-465f-8f47-80d5e276c7d2"}}, "946a0243-320b-4693-980b-8f2487746a62": {"node_ids": ["8ebcabc4-cd46-493c-863b-42dd322f897b"], "metadata": {"page_label": "6", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bccce577-e3a8-4300-b8c0-3c96be96b047"}}, "d3adebe1-0b0e-4af3-99d9-acd66ee11ceb": {"node_ids": ["77fd6bf5-07f0-4d57-b737-58e66fdd1501"], "metadata": {"page_label": "7", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_369aed3a-419f-4f0c-98ee-952ef37f2af5"}}, "498a0a43-3e14-4aad-8bd9-1fa4da3532c9": {"node_ids": ["61e16bc0-bc65-4de0-a789-040f618f55b3"], "metadata": {"page_label": "8", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bf2f1a27-7264-4ee8-b23c-a4fd4c23b3c0"}}, "9fd7324b-270e-417d-af59-c5120c5ec543": {"node_ids": ["d77ebf21-4520-4290-a7ab-1e26f432afc3"], "metadata": {"page_label": "9", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f198431f-8c1c-4c38-8671-f4ff4a5c373b"}}, "8f24fdaf-2516-4867-b76f-fd94516ced8e": {"node_ids": ["bc3f5579-5a90-4b4a-94da-6e525fdab5ba"], "metadata": {"page_label": "10", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_91123771-4b87-4a1e-84c8-1c65d21c2e8b"}}, "d08a5b73-885a-4e44-82de-25d5bf880e7b": {"node_ids": ["55281e51-de8e-4f60-adac-9db5fd0abe0b", "58bd27ca-5842-4807-943a-869aa89e462f"], "metadata": {"page_label": "11", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_22cb5d7e-4021-4bf6-af42-fc38f49f5e9d"}}, "3af6ca8e-7e4e-47db-bdfe-6741e98c3914": {"node_ids": ["d9e816d6-31f4-4317-8ead-45f7492d2385", "4feaa313-6415-4226-ae75-7e71941e5bd2"], "metadata": {"page_label": "12", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2087f73c-a1f1-42f8-8f28-7993757a85d7"}}, "8a2697e0-fed1-497f-a593-185353a9ddea": {"node_ids": ["8a0d5d11-faa7-4ac6-813b-15fdc8145bd6"], "metadata": {"page_label": "13", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ef2b7488-4de2-4520-82a1-216f82467c41"}}, "8f5a4a0b-6113-41ce-9dbf-bc9fc9314320": {"node_ids": ["7dcd8ff1-d1af-493e-b40f-4bf5cc3e859a"], "metadata": {"page_label": "14", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f024fd4d-d59b-444b-b845-58f096f73a24"}}, "87285a91-9ba5-40a7-9dfa-84198b5909aa": {"node_ids": ["3a7c4884-ddab-45aa-a83d-76f52219974d"], "metadata": {"page_label": "15", "file_name": "Attention Is All You Need.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Attention Is All You Need.pdf", "file_type": "application/pdf", "file_size": 2215244, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e82d7109-c339-4db6-80e7-68a52eee8df5"}}, "85a0df51-845b-4fdb-a435-da3523d5bfb1": {"node_ids": ["933b6e1d-70c8-49ab-a6f4-2e1a5998251c", "7e6ca033-72d3-40a5-b249-98bb915ed504"], "metadata": {"page_label": "1", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4f483482-e129-4cf7-b2df-2712db8ecb61"}}, "d31c81d4-dfb9-4f5e-81c0-a5c43d4dfe8e": {"node_ids": ["6d979b58-50c8-430a-9495-0a300f5611c0"], "metadata": {"page_label": "2", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_23c8cb65-a679-49dc-99ff-597ebbfd7e25"}}, "46526b9f-1f3a-4e70-b3fa-20180f9d4314": {"node_ids": ["434584a4-d097-4253-8a26-2c2e7a527ee7", "8129a709-e3f8-4da2-ab8f-53a3433a0503"], "metadata": {"page_label": "3", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4d48b1f3-b680-4fbe-9e58-9caae469e37f"}}, "39bc83ab-eb42-43c5-96d1-5a85037c19a0": {"node_ids": ["86b540ae-99af-4bef-8904-98b4dbc29483", "7532052f-ef7a-461a-9cac-b72e5d2f8eb0"], "metadata": {"page_label": "4", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_eb9f5072-4f29-40e8-b2b3-8126f019b6b6"}}, "02a04001-61fb-47f8-b44c-26bc83dba291": {"node_ids": ["5e1e065a-6143-4f12-94df-1fe1b836a80e", "92b5701e-3b44-4d50-8979-f61a2c937454"], "metadata": {"page_label": "5", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_713c6749-25af-4601-bcb6-6ef0ea87ea93"}}, "592c25bb-04c1-4e15-8772-d7b481524bfc": {"node_ids": ["d7f4b22c-88e2-4368-a647-11b90447bcef", "bbaee324-9db7-4409-954b-90fd96e4c8b9", "30c9f378-e2ed-4dd9-9709-d28beb9e3505"], "metadata": {"page_label": "6", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_530d34b9-3785-4177-94c4-b5f775c5209c"}}, "def5b079-93bf-469b-899a-36e94a9d8207": {"node_ids": ["07889096-ec34-4975-a907-a7cbfccb9999", "f96973d9-40a4-4ca1-baf3-7e5d9625ae63"], "metadata": {"page_label": "7", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_83b1d926-55ed-42fa-92d4-a386a984ab8f"}}, "01522fc9-219c-4bf1-96a9-4c2fad52e98c": {"node_ids": ["56416986-896f-4655-b157-b3ea3dce833f", "dd0a30c7-587a-40d2-bf70-2176b4271e34"], "metadata": {"page_label": "8", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_312f51a1-85e3-40ed-b277-11ef73d4ee98"}}, "49cbdf01-4090-4204-95c4-01ad5603b94e": {"node_ids": ["0a6b4a6b-e898-4b30-8f51-50002e8942b6", "c636e520-5899-4ea3-98ba-8bea265995fb"], "metadata": {"page_label": "9", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f1fe13ec-61cc-4522-93f2-77344bfd33fe"}}, "2a860910-1b8b-4660-b2cb-417ae28697e4": {"node_ids": ["602e12b0-0f54-4601-9905-f72c7e1942ac", "a13418ca-c784-4a3f-9873-22957bbc0efc", "a10d4ab2-ee04-4885-bfc0-2476d924d4b2"], "metadata": {"page_label": "10", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2c92508-4c9e-4d5f-a02f-0d6717d46dd3"}}, "30d293d0-e0d5-44ab-bbce-e81fba386379": {"node_ids": ["94922eb9-7e11-4ee1-9704-9dd91cb56b4c", "89a52eae-0cb8-4f6b-8d57-f6a52c682ba9", "38d14119-7377-45a1-9ab1-fadc969337a5"], "metadata": {"page_label": "11", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_35d02f6e-1ddd-4b12-9d96-7a4d7e90f415"}}, "034d8c0c-def8-400e-b2da-ccbfa9936911": {"node_ids": ["b90671e3-f228-46b4-b1e5-e78f40e19b97"], "metadata": {"page_label": "12", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b697a7d4-9425-4a31-9b90-44f2c14dc042"}}, "ed1514d2-a15f-4122-8e0e-18429e7ee461": {"node_ids": ["369ce22b-8016-4316-b431-055171a4a389"], "metadata": {"page_label": "13", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_844827f1-90e0-4034-b39b-741fd74c2153"}}, "0c9f1ee8-4654-48f2-a6d7-040f73acd552": {"node_ids": ["c4c9f6af-096a-4d0f-949d-c3bf905bde11", "29ea649f-2b24-433a-86f7-b66f1374a98f"], "metadata": {"page_label": "14", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_17ae37d3-5809-4621-9ca5-33b66de39264"}}, "15c4482b-227d-46c2-b83c-acdda1dee2c5": {"node_ids": ["60bc9dac-c6c2-4156-b6fa-393bb15dc70c", "e1d8b38e-104b-4e8f-bd88-21f293331864"], "metadata": {"page_label": "15", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0d1ecfcc-d9fe-4e43-83a3-27d3f598cf4a"}}, "5072742f-e98c-4a85-8707-d5b5efa8db16": {"node_ids": ["d2d15fd9-cc29-4a55-81c2-13838a163477"], "metadata": {"page_label": "16", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_95833137-665b-462a-8ba2-9517ca638607"}}, "d1287402-82be-46d5-81b6-14fd6871ca95": {"node_ids": ["856351f4-a520-4d5b-9691-b8a5d5b876c7", "dad94d54-e5ce-468a-a670-f6965bbe5af8"], "metadata": {"page_label": "17", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8489d48f-4790-4137-9bf0-9fbcc65ba4f6"}}, "95c6886c-d5ec-404d-a1a8-432f29273609": {"node_ids": ["5060bb58-5c0c-462a-afe5-4df6dd1f0ffa"], "metadata": {"page_label": "18", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_506fa68b-48d6-49ef-8c65-a097b5a9112e"}}, "92378fd6-ae0e-4a53-b8dd-bc475f9eb590": {"node_ids": ["f1f6c62f-2b96-430f-9fc5-f0cbca4fb67f"], "metadata": {"page_label": "19", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cf737876-6cbd-4eec-8026-ccdbf08849b9"}}, "eccb7f88-4aaa-406a-9219-89680f6831ef": {"node_ids": ["17db4a73-9f33-48b3-9a6b-13dbddad6343"], "metadata": {"page_label": "20", "file_name": "CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CLEAR Character Unlearning in Textual and Visual Modalities.pdf", "file_type": "application/pdf", "file_size": 8734883, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1870c0de-2114-484d-b309-3f9a27d61788"}}, "f08efdad-6495-4710-8127-30cfb86600ab": {"node_ids": ["3a6ac91e-0da2-4a54-9c3a-00a52d34588a", "c44d2e4a-5e73-4951-8b16-b4153a0365d3"], "metadata": {"page_label": "1", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_36b44a4e-1470-42f5-bafc-a797191042a4"}}, "14465d6c-5f3b-4863-991b-684c25c5bf1a": {"node_ids": ["8ecf8252-87f9-4cbb-b69e-bea552152f30", "6a6eb347-0e69-48f6-af84-daaac08f6355"], "metadata": {"page_label": "2", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c7fbef1e-462b-45b6-b18f-6edd12174e7e"}}, "501ec93a-c994-4e7e-860c-318c90c44142": {"node_ids": ["841d08f0-3a27-4218-929e-ff14240f2f3a", "c2e46d11-89a0-4122-8ff4-25a3e15be4ef"], "metadata": {"page_label": "3", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f3437940-cf5e-469d-ac6a-8a6812a51f45"}}, "71fea522-d983-4f0e-9557-e0b26bc36823": {"node_ids": ["647d350b-56c9-412a-8b91-a1e8526e3184", "c1a1a760-5b90-4ed1-9368-a68c1ca8e2e3"], "metadata": {"page_label": "4", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_49eaed8f-fb6c-4545-bc16-0010a5faa05b"}}, "802300c2-4daf-4fe8-99e1-1924e2cd511e": {"node_ids": ["03dfb7fa-b0a9-4cfe-9f82-b38378099f10", "32d7c58d-efcc-405b-a8c1-061ae3a03116"], "metadata": {"page_label": "5", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5ed2a3b2-93c9-4c73-826f-35f604f912bd"}}, "98cca184-30b6-4198-9722-c21e4c60125a": {"node_ids": ["a22b1072-7159-444f-aa83-a7cfc529e368", "e6514127-9a28-4232-9458-8dd6c5e918d6"], "metadata": {"page_label": "6", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e1d29221-41a7-4850-95e6-c18f88f18c7c"}}, "eeb59a00-0960-4c74-9148-c2c6f1429a62": {"node_ids": ["77c53922-b0ca-4b30-a1f1-90ce837551bb", "543ca04c-c292-460b-8e5d-fa488f5f358a"], "metadata": {"page_label": "7", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_355d5897-dc02-45a7-9b0b-8754074bf014"}}, "50d9a972-2a4f-4073-8d2c-6a24ab48a4e3": {"node_ids": ["518f595f-2fa7-477e-9361-7f9bb066070c", "b1739833-1579-488f-a85b-74e93ad67776"], "metadata": {"page_label": "8", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2ee6ab69-6fa6-4bfa-a9e2-ca12187b59d8"}}, "abb572ed-1551-4ae3-8943-a12d8835c3e7": {"node_ids": ["d8eb5ba9-60c7-4089-897f-b5697fc91149", "b5278636-8d50-4393-8c5c-c2a819ce110f"], "metadata": {"page_label": "9", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e89cb6f2-be2a-45ea-b6b7-aa8df08bdf3e"}}, "ff89670f-ba51-4423-8cdc-7b213c639795": {"node_ids": ["601ada8f-3e76-4699-aeb5-f69bf3f29639", "fc5f97cb-9e8b-4782-b573-e24bbf51c116", "c15e477b-e5f1-4e92-896f-c60e049075a0"], "metadata": {"page_label": "10", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6ea10c62-d592-4005-bf78-b298a5fc2d7f"}}, "5dd2c0e5-292b-40ba-acf6-8d1dea9b268d": {"node_ids": ["d38b250e-dbd7-4dfd-85b6-248e84170994", "ba60fc36-16b0-4dc3-9dbf-0308eb6cfefb"], "metadata": {"page_label": "11", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1c11f410-3add-4a73-aa17-5d8dc7c40415"}}, "00e7b1f7-432c-4848-977b-9ec3b4a3a407": {"node_ids": ["d073bf17-10f9-4aa2-9788-da597bd500b7", "e1c1ccb1-01ef-4d17-83a3-13ecf2efcd2d"], "metadata": {"page_label": "12", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_062aefa0-48bc-45aa-8dab-4a2a4ba7b69e"}}, "4e83275c-cf23-44da-829c-cdcd7bc51fbe": {"node_ids": ["90f9f8f8-7bd0-4e8e-89cf-8c0bf8ff889e"], "metadata": {"page_label": "13", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d922fe8c-463e-42fa-bc03-d3fe13a5c065"}}, "6859c1bb-c50b-4803-b0ce-dfa0a48f3beb": {"node_ids": ["a8f12285-2738-4995-b991-614aabed5148", "b0d805a2-0282-47b6-b69e-3076580f3657"], "metadata": {"page_label": "14", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_47557273-f1a1-4842-8364-43649c7bccd6"}}, "fc34b172-50b9-44e1-9eef-a4791db20196": {"node_ids": ["adf4a0ac-2f86-49a7-9d45-52a30efb9abc", "ca459feb-df3e-40fa-afed-22e119d52897"], "metadata": {"page_label": "15", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d196f7d8-2239-468f-8b83-db1db27bac78"}}, "b2917322-74be-47ef-ae3e-fccaa3adc5b2": {"node_ids": ["7d06fef1-fd60-4538-9c5c-857ee46acea5"], "metadata": {"page_label": "16", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3cfe55f9-a707-4896-a747-cf664c139063"}}, "779401ef-90b8-4679-9548-559274acb5f7": {"node_ids": ["3ac6a8b0-8e86-499b-85db-04a2eb0d3e30"], "metadata": {"page_label": "17", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_46ba9c07-4708-4ef7-b4d5-744ca4246d35"}}, "7a378539-d952-4a89-8ccb-4f086d42ad74": {"node_ids": ["ef08506a-120a-4d68-8e4a-ac5819ad773d"], "metadata": {"page_label": "18", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6270ad53-3763-415b-9e1f-e03e4438a1fd"}}, "9dc9dd4d-e649-4072-9a7f-60e5c67396d6": {"node_ids": ["251f6155-11fd-4e02-92ce-0bcdbf62b78e"], "metadata": {"page_label": "19", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a2610326-bb1a-4c6c-b4da-57db0b8df613"}}, "01a68519-f0f3-489d-b3b6-d4df2c6e4e82": {"node_ids": ["4f6053de-8d7c-4faa-866c-692e7ae8f441"], "metadata": {"page_label": "20", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d6c383d2-ef51-4810-a617-de58f2085ec6"}}, "60a75ca6-8db9-475f-b54c-e80872815c7b": {"node_ids": ["59cdccb8-0afe-49a7-8cb7-6f2809625247"], "metadata": {"page_label": "21", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_399f34b5-a12a-4e12-8e57-96f96d6e6877"}}, "c4a45799-713c-4fc5-afcb-82734244de4d": {"node_ids": ["4afde38d-53eb-49a3-88f5-c8ee8fbea4bc"], "metadata": {"page_label": "22", "file_name": "CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\CORAL Benchmarking Multi-turn Conversational Retrieval-Augmentation Generation.pdf", "file_type": "application/pdf", "file_size": 1921298, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0c13cd5a-84b5-400a-af0c-de68724f6f7a"}}, "604a01df-8600-40d5-93e1-254dd6851b92": {"node_ids": ["91de5452-3f8f-45b5-bbc2-676b212fdce0"], "metadata": {"page_label": "1", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ba35bf0d-affc-45a8-92ed-4387398e63b1"}}, "72e129a9-eebe-4070-adb0-68d5e9f84ffe": {"node_ids": ["8d44515f-4b60-4deb-af41-240875710fed"], "metadata": {"page_label": "2", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c2b6bef3-ba55-4e69-8481-ec41fe549d34"}}, "543724c9-7fed-46ca-b5c1-93102a3248ad": {"node_ids": ["b67e027f-83e3-45db-a159-4998ca46b3c7"], "metadata": {"page_label": "3", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_48eb1869-a0d5-4c10-afd0-02b8aba874e4"}}, "db535ebe-8647-4bc9-9a4e-c4c52e1413f1": {"node_ids": ["6494b310-f406-4ac0-bd89-75c376dbcb91"], "metadata": {"page_label": "4", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_504b162e-3172-4c46-81bf-c7cbc3733250"}}, "dbd40c26-12f9-4e09-a012-454e9fc2c044": {"node_ids": ["253dbe28-b07f-4508-8819-fcd23f9998cd"], "metadata": {"page_label": "5", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_deb22c2a-94dd-4202-a562-69cfce40ed4b"}}, "38f586c2-848f-4e10-b324-c5563b75327d": {"node_ids": ["1d818f0a-ac5d-4381-92d3-3a0aac77fe7c"], "metadata": {"page_label": "6", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_77bbcb37-9d87-4c96-856a-dd5be1cbf48f"}}, "5f5ae570-fe7a-4b5f-a47c-033cded12afe": {"node_ids": ["9937aee3-5133-4cd6-ae34-eb9f21fbcb54"], "metadata": {"page_label": "7", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d8f43aa5-66b5-41d1-aa0a-807ba07250eb"}}, "5c1060b1-f36c-4b7d-ba7a-b2a4e14cd003": {"node_ids": ["9c6eeeeb-1e66-40ba-a1bb-7edf2a48770a"], "metadata": {"page_label": "8", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3b7d1f08-3e33-471e-9aab-bec1deac5d7e"}}, "d7bab682-4ab2-43c4-b519-ea7dcb8e1b40": {"node_ids": ["07980e2e-5313-456e-a6bd-332f80dc576a"], "metadata": {"page_label": "9", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ac9212a6-8d28-4471-b837-cb7d89b7096d"}}, "3472a329-83f4-4d07-a790-65a5ffb83dbc": {"node_ids": ["8a89f9f4-9275-4677-b924-0fe7d550d1f6"], "metadata": {"page_label": "10", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_489c9a17-3d9c-42c9-b915-6ffc183a7668"}}, "4d4eb934-5107-4d9e-b758-c4262ed402e6": {"node_ids": ["09865a19-1fbb-4f0b-8348-ce9f0d7d6654"], "metadata": {"page_label": "11", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_077bb7c6-4daa-403b-9e4e-0ab7f363095d"}}, "e46848cb-ec72-4cf0-a171-79e04a845412": {"node_ids": ["2e60d2e5-4225-4435-b242-2f6488f1f9d0"], "metadata": {"page_label": "12", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1fc8d5a0-f216-4acd-af45-4915bc4374d2"}}, "1d49d86a-c943-405a-83a9-99913a035eba": {"node_ids": ["bc920deb-4a2b-4ae8-94a5-bdf82fea8c74"], "metadata": {"page_label": "13", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5a143ec4-75a5-436b-9189-6409bb854362"}}, "33338075-ddad-4455-86ae-283d5d1ed844": {"node_ids": ["89dbf294-2f08-4cef-a27e-dd378b3f0cb1"], "metadata": {"page_label": "14", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_fc8b2497-0280-4ce2-a2fd-b432646e38e7"}}, "a654cc66-ef2b-4ba5-9e8e-a67fceaf8a51": {"node_ids": ["332aa17a-47c6-4955-a213-0d28d3fbdc41"], "metadata": {"page_label": "15", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_45a96d46-a1d8-42c0-a774-259b82862c0f"}}, "b98b1920-525b-4ed9-a8dc-8e0a3093c2bd": {"node_ids": ["a5a7716e-4ef0-4343-ab17-d942a83dde25"], "metadata": {"page_label": "16", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9b35ef7e-b290-4c1f-af4f-49ede439dccf"}}, "be75985d-5ecd-4f00-b4db-262165bad0ef": {"node_ids": ["e3da756e-e157-4df5-b737-7c9e2ec6369f"], "metadata": {"page_label": "17", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_90b0f046-febf-4240-ba3c-1ee54a1b3314"}}, "4a0c00d3-a12f-421f-b8d6-c26c62a527c3": {"node_ids": ["b60c463a-4ed7-4844-b7b2-6a7f1e308574"], "metadata": {"page_label": "18", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8218342e-ee23-41d4-a6fb-73bcf53aaf30"}}, "77be2933-da9f-4bd8-b9d0-0d1f68eba7d1": {"node_ids": ["5832350e-2635-45c3-a614-27ebde354f82"], "metadata": {"page_label": "19", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6c6ebe94-0255-4241-b87e-cdbb85d9df1f"}}, "410d56d2-5c10-4cb4-b6b3-e3393b6de3f6": {"node_ids": ["b08cbc2e-d583-4ee2-b247-8104de647145"], "metadata": {"page_label": "20", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8e458490-3153-4316-9e17-81b650556069"}}, "d7b9b335-ea19-4484-84b0-105fba91184d": {"node_ids": ["cc95ac90-67a9-4ca8-ae14-951b35d188f9"], "metadata": {"page_label": "21", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_008d5017-f5d7-47f4-99c5-7f0cf68c85c0"}}, "300edfd3-f525-439b-9a94-b6f734a28b67": {"node_ids": ["277fe873-efaa-400f-b9ad-7efa7fd80937"], "metadata": {"page_label": "22", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3ba8b408-3a14-40d6-8f2e-2f1cc2a924c3"}}, "64ccfb61-5620-4c58-a944-a0563665400e": {"node_ids": ["9c3dcb2e-bbbe-4b65-8abc-c34a9babe51e"], "metadata": {"page_label": "23", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5fc3ba8d-9f5f-41ae-b2b6-b26c7358b2c7"}}, "d4dbc210-6e53-4013-b11b-7578d8abe32f": {"node_ids": ["96747efd-59e0-440a-a14b-fc6a71d032f1"], "metadata": {"page_label": "24", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_97bb9640-0175-4a14-9468-89d03ccf87e4"}}, "ae22ce76-cfaf-4e45-9665-2357f82e8068": {"node_ids": ["0eb14a51-8eb5-48f2-afd1-1b3afa4f57f0"], "metadata": {"page_label": "25", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_56493eab-0a42-44d6-8a84-484dda8d4a6e"}}, "826b1795-b7fe-4e57-8212-db8707f48c54": {"node_ids": ["6d473d84-8fd1-4d4a-b1af-9df92ec2f6b6", "1066dd4b-c724-4852-9a51-eaeadfed5730"], "metadata": {"page_label": "26", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1e996e96-1b80-442e-903e-9def50230af7"}}, "30963d4e-02f5-4aba-85e0-4eb3ce99e2cb": {"node_ids": ["34369b72-c313-4efc-a9b7-0d4555a887f7", "fb5a13a1-ba78-45b5-bab6-47548951ff3b"], "metadata": {"page_label": "27", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_be846e7c-f739-4ee3-9562-e30f45767d7f"}}, "00613d84-f8ed-44db-aefc-b6da8eddb05e": {"node_ids": ["c833d849-20fb-4755-a2f7-05d3bbb2a5dd", "60fa2e2e-467f-47af-9c8d-4ca736fda5b7"], "metadata": {"page_label": "28", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d0f6dde7-7eef-4472-9dc3-188216d8d742"}}, "d13c4898-bb9c-4b73-be61-a69bb164fcf1": {"node_ids": ["8412566c-c24e-4c79-972a-14c06fe3f274", "be58266b-8ea1-4897-85b1-535a585ec408"], "metadata": {"page_label": "29", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_237d1e48-52a6-4d19-ab23-e3e38cf1a2f8"}}, "b37aa547-b538-454c-9fc1-0ce11a484017": {"node_ids": ["28d8bef0-3738-4d8f-9fef-407049b02846", "af0ae41b-dd4d-4b62-a0ff-5ad65a451429", "2ff9a882-6bb1-456c-a38e-7a145c057965"], "metadata": {"page_label": "30", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1b5e2327-3be0-4dc2-b009-2cec6beec9ce"}}, "f718058d-2869-4b2e-b2e4-97460929cf24": {"node_ids": ["81a4ce5b-16b6-4db0-bc78-16ae83d85915", "8cfad21e-475a-4e7b-85ea-a0236d3a58a4"], "metadata": {"page_label": "31", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ef08882b-ead9-484a-b91b-d9f6b0d81405"}}, "47a8e021-eb0b-41b1-8b0f-93a2c3bd6141": {"node_ids": ["8cb8898d-7c0a-4126-a248-15230bd5bce0"], "metadata": {"page_label": "32", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_72bebe20-f132-4095-9f60-83ff149fbdf3"}}, "b9526a89-aa44-402d-be62-77035f653e63": {"node_ids": ["793f4ba8-6baf-4a12-9ede-d8b78773383c"], "metadata": {"page_label": "33", "file_name": "GPT-4o System Card.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\GPT-4o System Card.pdf", "file_type": "application/pdf", "file_size": 1440160, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_95ba9c8f-0e13-478c-91ad-4d7a966fcb55"}}, "7aa20ff5-f215-4b96-86b4-e9913d75d654": {"node_ids": ["2de54eb5-6f37-4943-8059-a0e1b4733243"], "metadata": {"page_label": "1", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6e3e6d0a-138f-483b-bd52-699c07c6907f"}}, "7da64d19-9820-48aa-8441-e2ccb06b8ce9": {"node_ids": ["118aa5f5-8279-46cb-b111-2364b433e4c5", "e5ad205a-d7d9-4677-a938-4e359bd90252"], "metadata": {"page_label": "2", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0df4ee88-0686-4754-b28b-26135544a2dc"}}, "4fca33ad-03c7-4571-933f-f243d82f642e": {"node_ids": ["70c59119-9b2c-4b76-a164-1d15e4ad7461", "06ac039f-1fd2-42ae-91cb-7831fd3863d5"], "metadata": {"page_label": "3", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_292a9d26-fefc-4b29-b5ee-e70910b9f0b8"}}, "60ce781f-c227-40fa-8add-0374d3e8dfef": {"node_ids": ["39002530-e9b8-47f1-9ef6-fa312ff23918", "de4d9b38-c611-4614-8307-06714ee1beb3"], "metadata": {"page_label": "4", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_43f04e8e-f236-40dd-b398-85808cebacf2"}}, "29465572-6bcb-490a-8af9-d6d79917cde3": {"node_ids": ["0c554dc0-3ad5-4556-a231-5f76767a6869"], "metadata": {"page_label": "5", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5ec76c8d-dbab-4425-b4f3-313b73bc3a24"}}, "2b7a8932-3ab9-4ba4-946d-17f5729b94b0": {"node_ids": ["dc725536-14a3-4b38-82d0-95eaec66409f"], "metadata": {"page_label": "6", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_78fcff2f-31e6-4fd9-af5e-4d0886839615"}}, "eb5c5878-36d5-42ce-854a-fef648a0e168": {"node_ids": ["efdd9c3e-89b0-42ce-9185-571d1c60a8d2", "f6292d36-20b1-4746-b2a3-bab3e21617e3"], "metadata": {"page_label": "7", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5c188ef4-8e9a-4fc7-b7cb-e5b3a12e1c6d"}}, "b6dacd1f-c879-4094-8104-af5bb6a8a617": {"node_ids": ["b32012c9-1778-4670-988f-92c1f7a4465e", "8df83a13-0a05-4b0b-b917-3171755047f4"], "metadata": {"page_label": "8", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f7c17fd2-66d5-417e-9889-44436ec336f4"}}, "c13b4076-cc52-47a7-bd23-a1d0dc1d4063": {"node_ids": ["be49a2e6-bb87-4eef-a52a-0c93b6657773", "82f70931-ab48-48aa-958b-920604ccb8b1"], "metadata": {"page_label": "9", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_011a168e-11f4-4fed-996f-adb1e74d33f3"}}, "a9bc6d35-9cf1-4a89-a115-1a50f10065b8": {"node_ids": ["f692efce-0785-41e5-892e-75fb73689f91", "434b23ab-77e6-47fd-a4fc-1eec1cb0e872"], "metadata": {"page_label": "10", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_04a27b59-1661-40e5-8c0a-b680e90980f9"}}, "067fc83b-a08c-4586-b425-408b2ff6bc02": {"node_ids": ["c03d1ed0-f1d3-4448-ab7e-d7aa1b41ff74"], "metadata": {"page_label": "11", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_895a1a37-8f3b-4288-8806-4c51f5534845"}}, "ddc71793-b545-46cd-aed7-28e3c9d98f64": {"node_ids": ["47e216b8-49ae-4bc1-8bb1-64548d98934d", "2c50109c-cdf3-445e-90c1-36f01caaacde"], "metadata": {"page_label": "12", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7c16e237-d09c-46c2-b1cf-65f41e21aa05"}}, "4b149eb8-2051-4a35-9973-5ca470efb10e": {"node_ids": ["121686d2-ff20-41d8-a8e7-8368d5c1332e", "25304bf1-fbd1-4fbe-85c2-50d8a4ee82a0"], "metadata": {"page_label": "13", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_7ac903b2-160c-4ced-9355-44f8fa637185"}}, "92b1ee21-17e6-4687-8ffc-f17e358aa6cd": {"node_ids": ["ed49e192-9edc-41f9-a1ec-f7af2b58b59b", "bb9c35b4-c46f-4d11-9b6c-f0ede6ef325f"], "metadata": {"page_label": "14", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1f24763a-48e8-4b9b-9756-1d694c22046b"}}, "630a02af-a6f0-49a1-a0da-f7777b4b2bb4": {"node_ids": ["22e4424b-0128-40f1-8b22-c1cb2018cf41", "adfb16e5-f86a-4a2e-9ead-db4a6a416081"], "metadata": {"page_label": "15", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cb98ae13-225c-4887-9101-935418a567a4"}}, "6e19f99c-6744-45f6-9d5c-f080de5b3a82": {"node_ids": ["826169ad-f8bc-4ee3-ab6d-d2d574db0010", "4882514a-722e-40c0-9745-118cfe076dfd"], "metadata": {"page_label": "16", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_08b3359b-00ae-4df0-8c00-126ff9532792"}}, "18c787bc-611d-4a8a-b3a9-4d1452dcf72c": {"node_ids": ["757d5b9b-4a0d-4cda-8fc3-8283fabd0bd7"], "metadata": {"page_label": "17", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_38362c39-bdf4-4948-ab14-462ec6a98adc"}}, "4d2f88f7-6990-40b9-8260-d98b8f09503a": {"node_ids": ["5b0419e5-3c0b-4216-a6cb-1b9ef28c539e"], "metadata": {"page_label": "18", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b0be4544-5112-4e8c-a741-8df32153f426"}}, "0106f731-6b04-4c5e-a4f7-19bbab2e6ec3": {"node_ids": ["2b9e3f48-e0fb-4762-87c1-a5cf7ab12953"], "metadata": {"page_label": "19", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_34aa04a8-ed51-4db7-a685-6c358b461301"}}, "36373ec7-c605-4a10-a7d0-dabfebf39ab4": {"node_ids": ["bfaa64d9-281e-44f8-9f18-702155ca8164"], "metadata": {"page_label": "20", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5fd52b63-21b4-4c12-bc15-16e182c272d9"}}, "6c625e08-5918-4915-940e-948812542cef": {"node_ids": ["21ff6bf5-5dd6-4bcd-a6df-736cc4cd6e47"], "metadata": {"page_label": "21", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_74707909-3502-4b21-85e9-331cd2c353eb"}}, "3ff55ed0-48b8-4bc6-b125-b08b3580a6dd": {"node_ids": ["1edfa7f3-fcd8-4559-881d-f975eb7a1e9d"], "metadata": {"page_label": "22", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2e2895f8-d7f5-46a7-93e0-c30c2aff6561"}}, "77487a0f-ce2d-4c6f-8fb6-673b5caf6fe5": {"node_ids": ["6236f5ac-55c1-4ba8-a414-b988e20d5d41"], "metadata": {"page_label": "23", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2784a910-4e1c-4d91-b364-8c8f5c6436b6"}}, "dfd4f146-1f22-4ac2-95d5-2f61fb56a7a8": {"node_ids": ["d5a5cc34-8790-412b-8b86-62a828d920f4"], "metadata": {"page_label": "24", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_93d40fdc-7958-4bd6-ad75-150540c5f764"}}, "0f24a07b-c9be-4e42-bdcb-7e30954a8ffe": {"node_ids": ["cfc013de-1f3f-4243-83ed-b0e81ced7233"], "metadata": {"page_label": "25", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6b6d53c9-b783-4232-8785-66048f0d066a"}}, "9f44cff3-3389-4669-bd9c-57aecb6d0a02": {"node_ids": ["eaba8dc1-ebfb-4afd-a8df-d30d08b022bc"], "metadata": {"page_label": "26", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_29ce5bb6-230e-46ef-a291-f599ed151581"}}, "a4784f93-1e40-40bc-b704-7d1897ddd5b2": {"node_ids": ["42d6eb62-4b84-4f9f-961f-10853248134b"], "metadata": {"page_label": "27", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_565ae114-3def-486d-8024-a37df37a412f"}}, "977f62db-5ae3-4c30-9894-bc70f36b4aab": {"node_ids": ["cdb16ce0-e4d3-4eae-97cc-938e65c3a125", "552a83dd-66c5-4f09-b911-7003dd371440"], "metadata": {"page_label": "28", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3e94c69b-5bfd-4a92-965f-e50463130eb0"}}, "a86406b1-82af-402a-abc4-13bd1c437960": {"node_ids": ["79f87622-c272-43db-9a42-9ab3546f3f81", "fb7fe99a-99e2-49d0-8898-12795e1f9d64"], "metadata": {"page_label": "29", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ac966a96-33f9-44c4-b8cc-b9e307acdc1f"}}, "8c1c7397-49b4-487d-a322-cffaa0657796": {"node_ids": ["a024aba3-0db0-47aa-b4a0-e46b6ef6d96e"], "metadata": {"page_label": "30", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_061ad42a-6996-4ded-9e44-86ca48e8948a"}}, "f5a172d7-42d1-43b0-9427-ffc3d4ba378d": {"node_ids": ["5305d153-1ffa-4832-8953-5e288e691129"], "metadata": {"page_label": "31", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cff3b99b-506d-4827-8ee9-ced5aeac4d06"}}, "f5ecf6f5-63a7-4960-a80d-cfb4b53530ce": {"node_ids": ["2d5307ba-c64e-45d1-bdb0-d2816238928f"], "metadata": {"page_label": "32", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e8c2fa99-52ac-427e-8622-e370f2178d2d"}}, "0e1b2da5-cf8c-4eb5-813e-b9dc1c6347f7": {"node_ids": ["ae1c0c83-f39a-4d9c-8263-504ee6d2fba1"], "metadata": {"page_label": "33", "file_name": "Unpacking SDXL Turbo.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\scientific_literature\\Unpacking SDXL Turbo.pdf", "file_type": "application/pdf", "file_size": 10995270, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f6842e1d-f441-4a99-bc40-bf321c479f36"}}, "364fe5d6-468d-43f6-b7ca-5e5d7f053344": {"node_ids": ["2eb48f09-db99-44e6-b38b-24bb34dadfd7"], "metadata": {"page_label": "1", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_592e52c6-f214-423c-99a4-f8cdecc03901"}}, "9a27cd38-180c-4a9e-bdca-d5aead22ac1c": {"node_ids": ["6edcdb2d-a66d-4607-9f2c-62d3e3c92e53"], "metadata": {"page_label": "2", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_21b5ca0a-1904-40b9-b047-851932c8fa9d"}}, "971c53b4-edc5-45a3-8b57-5d85e1db9016": {"node_ids": ["e1155e61-e1f7-4284-a093-cbeb41075056"], "metadata": {"page_label": "3", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b2d64afc-af82-4489-a03a-3cbc544e2bb2"}}, "70e098a7-cb12-49e9-90f0-6f318b1cec1e": {"node_ids": ["c100cfec-79f2-4ec3-81bc-cea2c15c8c04"], "metadata": {"page_label": "4", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9407bd7c-5277-40e4-821a-8b5d14da8f74"}}, "69dd8665-df26-427f-8e7e-6ecd450afeed": {"node_ids": ["bb4d7243-b0df-418d-bc36-d6f0082837ae"], "metadata": {"page_label": "5", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_634955b9-16f9-42bc-8606-798c64281721"}}, "015df7ce-1350-4eb1-be49-202f048f1d8c": {"node_ids": ["392bf67f-87c0-4314-be10-85214fbdf33f"], "metadata": {"page_label": "6", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8bb4c4bf-7aaf-49f3-910e-115c3c465d86"}}, "f91b6639-de85-4e31-a5af-2f4e1fe81c63": {"node_ids": ["2e651499-18f1-4274-b19f-84d2890907a0"], "metadata": {"page_label": "7", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_dd33f46c-5fb3-4729-8398-d8be096ece33"}}, "6a80cee8-0c5c-413c-80c4-41ce5fda37c5": {"node_ids": ["6db20671-1b6d-46af-bf8d-8cb72265d3ef"], "metadata": {"page_label": "8", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_eb89ecd3-58bc-44d7-ada1-0c9fd72c3411"}}, "a0598a94-87e7-4371-b1bc-57cb43d7f170": {"node_ids": ["0b5e25da-73d5-4cd7-bad2-5ae3861eaeae"], "metadata": {"page_label": "9", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_40f69057-c9cb-4cfd-a7eb-95a0e5c6fc96"}}, "bc9f1828-9234-4f3e-a26a-5e3c15eecbb0": {"node_ids": ["8f00621d-ed16-4187-ae96-18c225662b4f"], "metadata": {"page_label": "10", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_4d053012-d5a1-4a0b-977b-dcedc8a5a2e0"}}, "b71ba31a-08d2-42b9-987b-f673b7df3fd1": {"node_ids": ["512269f4-6819-4397-85db-73fd4dc2c89e"], "metadata": {"page_label": "11", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0efbe367-121c-42ae-95f8-68e918d56d8c"}}, "29354eef-f596-4838-ab17-22df58635013": {"node_ids": ["267f7280-d569-43ee-9df0-d219764f6d73"], "metadata": {"page_label": "12", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_583b77f7-7f29-49b9-b0c0-7b391142c7c0"}}, "f4c2ed1a-eb63-44e4-9521-49234f677153": {"node_ids": ["916dca71-aa48-4099-9dbd-d856490fa3b5"], "metadata": {"page_label": "13", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b18ed6b3-360a-4adc-b31c-1773ec62b9b4"}}, "cfd7b93e-cd0f-412b-b8cc-aa99b789983b": {"node_ids": ["958a3879-74c7-4334-a515-cd24fe969dad"], "metadata": {"page_label": "14", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_133a3d20-201c-4a89-9bc4-63799e182343"}}, "342d9846-0f4c-49be-8709-a782cfef0dc7": {"node_ids": ["5ef5fe84-b6ae-4bac-b997-179506dd931c"], "metadata": {"page_label": "15", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_02b19295-6f75-46b6-90a9-3485108a8073"}}, "036f2df1-83bd-4fbf-ac24-574f990eac25": {"node_ids": ["62f85a7f-b2ed-4ad0-8c46-28912c5d2bcc"], "metadata": {"page_label": "16", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ecb9bdbd-d0e9-4961-b38e-660962731312"}}, "06193f4e-08e3-4391-bee1-b5c3267519fe": {"node_ids": ["0328018c-58a3-4764-86e4-e82f0ba70e9c"], "metadata": {"page_label": "17", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_adaa99b0-1bce-4792-85e0-bd125163fe35"}}, "c5c59789-93f5-4713-b9f9-15f156b3474a": {"node_ids": ["f5021a88-0bee-4a68-bad4-e0b5d9853244"], "metadata": {"page_label": "18", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5538190e-4cdc-418f-9564-7414fae0e98a"}}, "4b65baf1-aeae-4d42-be33-a2ad8710486b": {"node_ids": ["d6240849-f445-43d9-b426-56f05f6fe309"], "metadata": {"page_label": "19", "file_name": "Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Building Postcards for \u201cAirbnb\u201d Scale _ by Leo Wong _ The Airbnb Tech Blog _ Medium.pdf", "file_type": "application/pdf", "file_size": 6778509, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_de7c5241-ed5b-47dc-9bb1-a9f30f89c5cd"}}, "4953d18c-2de0-4893-a83a-d257a78a1906": {"node_ids": ["8ebd50a2-ebff-42a1-9235-38c7d09891cc"], "metadata": {"page_label": "1", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f722e853-cac0-4259-8d5c-0c5e4517284e"}}, "27ccebda-b20a-4a9b-8082-7d324e60e6b2": {"node_ids": ["091ba85c-fac6-4b60-9034-aadfde27e120"], "metadata": {"page_label": "2", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_70ddcae2-59df-4e97-8a53-fd13bd0865ed"}}, "1e47060c-0d14-4de2-9b80-a1a54f4c0239": {"node_ids": ["d5db3e43-6cbc-4789-938b-9e7c8f89ae61"], "metadata": {"page_label": "3", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9ece0034-ace3-4fe0-8449-32a8a60afbdc"}}, "d6fcc8e6-9c1e-4d53-b521-96e0516b3fc0": {"node_ids": ["5bb13d0a-0c0e-4d41-8637-759b7b4b84fe"], "metadata": {"page_label": "4", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a2fc8a19-a7f1-4ece-a6de-991f6df6922b"}}, "a0aa37b6-4652-40cb-b4a1-4b1265a7d405": {"node_ids": ["d98277da-972a-47da-a4db-dbb14fd4a2a0"], "metadata": {"page_label": "5", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_571f41af-d4ec-4e2f-9f38-4255538d32dd"}}, "1aeae644-84ac-4441-bfa7-5454889b28a2": {"node_ids": ["4da2745e-0b0d-4c74-b0e6-061629287d5c"], "metadata": {"page_label": "6", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_c76ec4e2-de2e-48f7-bae7-501559af5016"}}, "ff3c6086-ca55-4930-9659-6fed26414247": {"node_ids": ["2644b374-3d3e-4b54-abcf-7eda9786abaa"], "metadata": {"page_label": "7", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d9f139cf-88ea-41e8-a414-428506e128af"}}, "2e371e51-35fd-45ff-b21a-3bbd71734641": {"node_ids": ["d83c8768-a112-45c0-9791-99ac24e81c4f"], "metadata": {"page_label": "8", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a7234322-a32a-4e75-94e3-b79fe17c6485"}}, "95cadd8f-bad4-4c54-9f56-c41bc2c99ead": {"node_ids": ["e4a91b6b-8e83-4992-bb97-2b2a39ac06e1"], "metadata": {"page_label": "9", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_cfd8e9d9-8ceb-4d4f-b57b-7ce4a07e5a1a"}}, "83cbe7cc-473b-45aa-8ddb-448d94bbed6b": {"node_ids": ["d66c1a9d-d697-455c-b117-bfb424cf0a22"], "metadata": {"page_label": "10", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b2495c04-624f-41b2-bf9d-3f8fd61539e7"}}, "66d34dbe-b05f-408a-b513-dc4e05e944f1": {"node_ids": ["251ecb84-4941-471d-80f7-99878bfa12a5"], "metadata": {"page_label": "11", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_00530a43-bda3-493a-ab79-c8e92f98a843"}}, "890696bb-913d-48b7-8bf0-178c85f587dd": {"node_ids": ["746f5273-241e-4b90-b12f-d313726edb61"], "metadata": {"page_label": "12", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0e4fede4-a73b-4345-9b5e-8be248095c37"}}, "4ca97086-52c7-4623-a6ee-6ab22c932d9d": {"node_ids": ["b5d85c14-d5a7-4a60-897e-0ffabbc0a6b0"], "metadata": {"page_label": "13", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_df583342-d740-42d9-bfde-75819edc431e"}}, "58d6ed7f-4172-4844-9aec-42f5dc0b3a72": {"node_ids": ["83edb6e7-4a1f-4573-91d7-3a8a0c5f55c0"], "metadata": {"page_label": "14", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_de3aaa68-9e5e-4bdb-93ec-a441aa2da71f"}}, "bc6bfe10-9059-4bb8-9344-6e0a9a44453a": {"node_ids": ["648e18b0-4c6d-40a4-ab38-5aa0a801b990"], "metadata": {"page_label": "15", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8c97c3e2-daa1-4846-82b0-42e8619c0e1b"}}, "89145a0d-9d8b-419e-99e3-1f2f994c90af": {"node_ids": ["d9fc8522-7228-4fc9-9f0f-b5b778065903"], "metadata": {"page_label": "16", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_d3973557-686e-4493-88de-c0d99a2bb828"}}, "e01f050f-8ce6-400b-8e5a-bf121354d4b3": {"node_ids": ["59de3b7c-d2a5-48c9-aa5b-6307d72db195"], "metadata": {"page_label": "17", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_ce53cd18-30f6-4d15-99cb-afcf665f5ce0"}}, "b7ea9f0d-6b30-4c7a-af36-1261e652e802": {"node_ids": ["6409eccd-8f11-4996-aa96-428f4b1d9111"], "metadata": {"page_label": "18", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_416446a8-c076-4685-b3ee-1efc9df19e8b"}}, "bdbfcd10-ecab-48cd-a241-66da56e47009": {"node_ids": ["591a3c26-67e7-4e26-b48e-3bc315127b37"], "metadata": {"page_label": "19", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_9794d139-2209-48b8-8bfe-d55ca8f50989"}}, "81d2085b-7ec5-4ff4-814c-14dac6356574": {"node_ids": ["987d3b75-e830-40c8-9953-6f069806614f"], "metadata": {"page_label": "20", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_34eb14c5-dfea-4b96-a378-d488a7d4fd1e"}}, "f6f6c489-68de-42ea-b61b-22d7e077719f": {"node_ids": ["67c885c3-e792-4e3e-bfdf-1645c2152a6c"], "metadata": {"page_label": "21", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_18b73c9b-3ea9-4881-a21e-9dc830392865"}}, "d41a8915-3c59-461a-a299-39e74e2efbf6": {"node_ids": ["963d2eb3-cb6c-4f0e-86a2-7b478db1a8d8"], "metadata": {"page_label": "22", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_0b73ceef-2cae-49d7-bf3c-d505dcb51b7b"}}, "de5232a8-8d6e-403a-be1d-a3cf0c1f054f": {"node_ids": ["335f40cb-086a-4dae-9e21-34b5db01163b"], "metadata": {"page_label": "23", "file_name": "Data Mesh Principles and Logical Architecture.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Data Mesh Principles and Logical Architecture.pdf", "file_type": "application/pdf", "file_size": 6193989, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_822e7338-e179-4853-acd1-1ddd91643d44"}}, "2a0533b6-6b80-4f19-a67f-29ada59f36c3": {"node_ids": ["658e9d6d-c953-4d1a-a60d-03f50e9934c9"], "metadata": {"page_label": "1", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_64d0fe8b-624e-4942-b8f1-a6d3527c3878"}}, "63a056cc-7a11-4afa-848b-a33aa74fe1ae": {"node_ids": ["0755e2b9-e6a7-4ead-9d77-421982f2d2aa"], "metadata": {"page_label": "2", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_5810ab49-72ef-400d-a746-554a0072d35e"}}, "8480a70b-c272-4202-9914-49f28cc03380": {"node_ids": ["737b0b7f-7022-45c8-82eb-6b2f3f1fdf0b"], "metadata": {"page_label": "3", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_80d4d653-bc2d-4992-8c0a-f223ce6e7739"}}, "a378b593-77ab-42f0-974f-daeb5f4b0100": {"node_ids": ["9791766d-d5ed-460a-8c28-23fac18b7adc"], "metadata": {"page_label": "4", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_2ed34d4e-c64a-40a1-9da0-d6829ef25aa1"}}, "1465d6ff-9d30-497e-a50e-f8244d6da58b": {"node_ids": ["d76ed900-7cfd-4104-865f-434f521fe7de"], "metadata": {"page_label": "5", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_60fd93d3-0c5f-49b5-87c0-8d0f02a4919a"}}, "a8548aac-f163-4ade-9fdb-695f29dfcd7d": {"node_ids": ["4e0948a7-6439-4e59-a1b4-60c72ef8b72d"], "metadata": {"page_label": "6", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_a2d19996-c490-4462-97fe-204bb928b1a2"}}, "b8e71907-ed07-4ad2-900d-c952ee1e15f1": {"node_ids": ["d26b1aad-57d3-4d07-9ffb-6180f7eadf28"], "metadata": {"page_label": "7", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_572bed3c-87eb-4b07-9bf3-0c5c68d74030"}}, "71ff7763-70c1-4ba0-b3e0-381ab5f7e326": {"node_ids": ["5be204de-4c9a-4ab6-8e65-a6445f1df6f7"], "metadata": {"page_label": "8", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_04e80f6b-392d-4b70-b6a6-e3ace2f7f961"}}, "e2b60e37-7467-42ba-b8ed-65360f08e9d7": {"node_ids": ["3594404b-fa6d-4464-a267-444e42d6e91a"], "metadata": {"page_label": "9", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_b784c0a5-c084-444f-b631-719313332d4c"}}, "9dc86724-6e17-4cd4-80a0-58f5532c16d9": {"node_ids": ["20c65302-a4de-461c-8d84-7ae13a230db1"], "metadata": {"page_label": "10", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_6d829122-2128-47a7-9107-b54ce984c04f"}}, "22521c83-657b-493c-9771-0769af46ea67": {"node_ids": ["87775247-affe-4a3c-81d2-8decc8b52bcd"], "metadata": {"page_label": "11", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1dac3ce6-4be6-46cf-ac80-d125191130a2"}}, "aac21f64-7b12-4106-92e6-5c1befea94b5": {"node_ids": ["bcfb5cb1-b538-4ba9-ae12-ec64661d77b2"], "metadata": {"page_label": "12", "file_name": "Governing data products using fitness functions.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Governing data products using fitness functions.pdf", "file_type": "application/pdf", "file_size": 1592014, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_09c94c05-e2f0-42d0-9703-dbe3d0bdb3ee"}}, "c1614896-fd33-4eec-86eb-a93b610e695b": {"node_ids": ["707eaeeb-ba1e-413f-8771-dd770b4fe935"], "metadata": {"page_label": "1", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bb0f1965-4dff-4ed9-8169-419e1e970fa6"}}, "9ed0ca2a-38bd-44e2-90f2-646efc7ef16f": {"node_ids": ["16add095-16fb-402d-a2bb-ef3d959e75a4"], "metadata": {"page_label": "2", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_73c38132-d5c7-4cd8-9b35-36bb2ded2528"}}, "9052fea2-9663-4c44-bf9b-e10215740360": {"node_ids": ["70a41518-a768-45eb-8b46-c2a2604ab947"], "metadata": {"page_label": "3", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_08a380b7-a6fd-4c1f-b3bd-1ab0a1842eda"}}, "53319711-0487-4af7-a8de-4ea21f7578ec": {"node_ids": ["0a89dcb6-ee6e-481d-9821-5c1223e960be"], "metadata": {"page_label": "4", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_03d109dc-9256-4ae8-ae42-472c7e268a4a"}}, "58a34927-bc19-4122-99e8-8c6aac4e1340": {"node_ids": ["2223dc2a-477f-4d2a-8b2c-3d65cc836096"], "metadata": {"page_label": "5", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_3b537167-9d1b-4303-9579-8cabf75c0b55"}}, "1215ada6-70ee-44a0-9294-a9577923c618": {"node_ids": ["9dd82802-042c-4cde-9fc9-1559daa32726"], "metadata": {"page_label": "6", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_85809132-9272-44df-aef5-546589826383"}}, "8983ea67-a0cb-4495-aa8b-91d3c049d545": {"node_ids": ["04d6b4ca-275e-47d4-b6aa-c4d15d4352f7"], "metadata": {"page_label": "7", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_78e23aca-bc0a-4201-808a-045aa997b174"}}, "24bbb848-2d8d-4747-9078-8cf167723313": {"node_ids": ["2ae01f91-64ec-4665-b86f-f3943f2d80e5"], "metadata": {"page_label": "8", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f8329204-0c62-4c2f-9418-d4e662df1ee8"}}, "9cd57a24-3832-40ee-adce-40f60d196789": {"node_ids": ["8ebfdcd1-7ba9-49e4-a940-a413c71e6d4d"], "metadata": {"page_label": "9", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f1d75350-2514-43a9-9a5b-31bcd529cda4"}}, "616ca7cb-6011-4c91-adc7-825db5716800": {"node_ids": ["1ef52f19-eeb7-46c8-b3f8-30e64dd29116"], "metadata": {"page_label": "10", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_49786212-2190-4aa4-9b9b-cc767e21842b"}}, "fc293cc2-4188-49ea-8db7-c4c03957d035": {"node_ids": ["9a3beea0-c8c8-4b22-bd9a-7cd8d9952251"], "metadata": {"page_label": "11", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_61a85fb6-ea26-4e14-bf39-e12d1794b0eb"}}, "e71d4d19-5fbf-43f0-b7aa-58b12ff21e7f": {"node_ids": ["f8f61c07-ad1b-4d12-b989-79a86bf01626"], "metadata": {"page_label": "12", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_06d9d901-1493-48a3-b852-b621a4f56cd1"}}, "f27b4b8f-63b3-4014-a96e-981309692954": {"node_ids": ["31ac444b-25c4-4fd6-9d30-26fd1ddf40b0"], "metadata": {"page_label": "13", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_305e2b09-598c-46e5-8819-38f46f88c12d"}}, "539e6964-7a4a-4993-a8ae-925fae6721c8": {"node_ids": ["3c5fddc5-2ba2-4983-a20c-ddd02e6fc511"], "metadata": {"page_label": "14", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_097eae81-8bed-4c34-84ab-aca49f7c2a26"}}, "622a6480-5f66-4e0b-af7c-a23d1f34274b": {"node_ids": ["31ed4aac-1f33-4182-b246-17e1fb8e010f"], "metadata": {"page_label": "15", "file_name": "Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Sandcastle_ data_AI apps for everyone _ by Daniel Miller _ The Airbnb Tech Blog _ Sep, 2024 _ Medium.pdf", "file_type": "application/pdf", "file_size": 4308753, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_dda18760-a2d3-42e6-aee8-d5716cc743e9"}}, "744cc8f4-262a-41a5-93ce-da03dc77eff2": {"node_ids": ["e482d595-4d7d-48e0-9928-e741f044985b"], "metadata": {"page_label": "1", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_1d36443f-3fb4-48c3-b2bd-6be72f061afa"}}, "aa3c1681-c090-44a9-93c8-d54877d1f90d": {"node_ids": ["84022dfd-21cb-416b-9c0a-664704213651"], "metadata": {"page_label": "2", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_8949e47c-fe8e-4607-adf4-c106afe78209"}}, "ed023010-941c-46bc-ade2-b034dc4ac6d4": {"node_ids": ["bbe3eb96-8bcd-432c-b760-dd12bf47c8e5"], "metadata": {"page_label": "3", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_e6bfef7a-651a-4535-8d99-e1752e00f2c9"}}, "be67dabc-5f36-43d8-a4b5-bd21080b6919": {"node_ids": ["d082078b-b919-4868-9e79-c0609a175c27"], "metadata": {"page_label": "4", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f17f2784-6943-44c7-b73c-d2434097b176"}}, "0f09bbff-fe27-4862-acf9-3034a08791d6": {"node_ids": ["5b77c31e-4be9-459a-9a67-371935c11422"], "metadata": {"page_label": "5", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_95256bab-38be-4e1c-933b-71dca3105152"}}, "204dcbc6-3086-436d-ab1e-9e879a1476b8": {"node_ids": ["37f21a04-918c-4e62-81b6-5a65ddfdf6ae"], "metadata": {"page_label": "6", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_f87d2026-2682-4d11-9dfb-6bf85028787e"}}, "31cc4bcf-e5c2-456e-8c93-d36f571def8a": {"node_ids": ["7cfe8f32-a81b-4bca-a518-bf4d0678d828"], "metadata": {"page_label": "7", "file_name": "Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_path": "E:\\neu\\LLM\\RAG-2\\data\\technical_blogs\\Things You Should Never Do, Part I \u2013 Joel on Software.pdf", "file_type": "application/pdf", "file_size": 1278831, "creation_date": "2024-11-02", "last_modified_date": "2024-11-02", "filename": "doc_bafc8714-c1ab-48d5-b0a9-40af0a9a2eac"}}}}