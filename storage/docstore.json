{"docstore/data": {"9abe8186-d3ee-4b5f-941a-910011263a8e": {"__data__": {"id_": "9abe8186-d3ee-4b5f-941a-910011263a8e", "embedding": null, "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:49:17.244779"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c1e06d0c-6c8f-4bd0-bfbd-26df2eb621a6", "node_type": "4", "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:49:17.244779"}, "hash": "a90485033003ea0e83b00c9667bb95f40a2142ed40147278498502bfbf4683bb", "class_name": "RelatedNodeInfo"}}, "text": "Lite TradingView Cloud Web Application Project - Google Docs\nJavaScript isn't enabled in your browser, so this file can't be opened. Enable and reload.\nThis browser version is no longer supported. Please upgrade to a supported browser.\nLite TradingView Cloud Web Application Project\nShare\nSign in\nFile\nEdit\nView\nTools\nHelp\nAccessibility\nDebug", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "89b77865-6983-4efa-a38a-a882cb25ef2a": {"__data__": {"id_": "89b77865-6983-4efa-a38a-a882cb25ef2a", "embedding": null, "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:55:03.158939", "added_at": "2024-10-31T15:55:03.160934"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "41e4c824-9e12-4cf0-9dee-24eeeb2c49d9", "node_type": "4", "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:55:03.158939", "added_at": "2024-10-31T15:55:03.160934"}, "hash": "bc2f8e48c709c90a5554b2619838ba414adfc9e66359a27be1994fca47c98ca3", "class_name": "RelatedNodeInfo"}}, "text": "Lite TradingView Cloud Web Application Project - Google Docs\nJavaScript isn't enabled in your browser, so this file can't be opened. Enable and reload.\nThis browser version is no longer supported. Please upgrade to a supported browser.\nLite TradingView Cloud Web Application Project\nShare\nSign in\nFile\nEdit\nView\nTools\nHelp\nAccessibility\nDebug", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b1b8a73f-49c5-4543-af78-54b2c32d8c1c": {"__data__": {"id_": "b1b8a73f-49c5-4543-af78-54b2c32d8c1c", "embedding": null, "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:55:33.762583", "added_at": "2024-10-31T15:55:33.764543"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5664aad3-4ca2-4875-ab83-efecfcf3be13", "node_type": "4", "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:55:33.762583", "added_at": "2024-10-31T15:55:33.764543"}, "hash": "eb9852952b9fb7e4c32c05ed26cfe5f47d97feb970d8fa2cb31e41e97292b824", "class_name": "RelatedNodeInfo"}}, "text": "Lite TradingView Cloud Web Application Project - Google Docs\nJavaScript isn't enabled in your browser, so this file can't be opened. Enable and reload.\nThis browser version is no longer supported. Please upgrade to a supported browser.\nLite TradingView Cloud Web Application Project\nShare\nSign in\nFile\nEdit\nView\nTools\nHelp\nAccessibility\nDebug", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c8b3a2f3-8bf1-4d10-8ce7-0a44ba29b894": {"__data__": {"id_": "c8b3a2f3-8bf1-4d10-8ce7-0a44ba29b894", "embedding": null, "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:55:46.835013", "added_at": "2024-10-31T15:55:46.837002"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bb299390-71a3-4727-b567-e022973f5854", "node_type": "4", "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:55:46.835013", "added_at": "2024-10-31T15:55:46.837002"}, "hash": "ff22e09b7b97551eb7f79d3018884f76c4da25e918103fe1cb64dae3570d4d0c", "class_name": "RelatedNodeInfo"}}, "text": "Lite TradingView Cloud Web Application Project - Google Docs\nJavaScript isn't enabled in your browser, so this file can't be opened. Enable and reload.\nThis browser version is no longer supported. Please upgrade to a supported browser.\nLite TradingView Cloud Web Application Project\nShare\nSign in\nFile\nEdit\nView\nTools\nHelp\nAccessibility\nDebug", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 342, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a9290983-7b2c-4198-bb2e-88286b2f80a0": {"__data__": {"id_": "a9290983-7b2c-4198-bb2e-88286b2f80a0", "embedding": null, "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:58:02.840733", "added_at": "2024-10-31T15:58:02.843691"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7dbff074-b246-48c2-b559-69dceb4e36bf", "node_type": "4", "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:58:02.840733", "added_at": "2024-10-31T15:58:02.843691"}, "hash": "e7af174672feeb1cf30fb978c6c2cfce440fb58cf4d4592b1cdecc634d7cb693", "class_name": "RelatedNodeInfo"}}, "text": "Lite TradingView Cloud Web Application Project - Google Docs\nJavaScript isn't enabled in your browser, so this file can't be opened. Enable and reload.\nThis browser version is no longer supported. Please upgrade to a supported browser.\nLite TradingView Cloud Web Application Project\nShare\nSign in\nFile\nEdit\nView\nInsert\nFormat\nTools\nExtensions\nHelp\nAccessibility\nDebug\nUnsaved changes to Drive\nImage options\nImage options\nReplace image\nTable options\nAccessibility", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 462, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e0b9120d-4aee-42c1-bbf6-e197e0cbf225": {"__data__": {"id_": "e0b9120d-4aee-42c1-bbf6-e197e0cbf225", "embedding": null, "metadata": {"title": "https://zhuanlan.zhihu.com/p/651507945", "url": "https://zhuanlan.zhihu.com/p/651507945", "timestamp": "2024-10-31T15:58:29.235781", "added_at": "2024-10-31T15:58:29.237777"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ac60c74b-abed-4e3b-ae26-0ee99e4eef05", "node_type": "4", "metadata": {"title": "https://zhuanlan.zhihu.com/p/651507945", "url": "https://zhuanlan.zhihu.com/p/651507945", "timestamp": "2024-10-31T15:58:29.235781", "added_at": "2024-10-31T15:58:29.237777"}, "hash": "10cb34ab6f0955549ccad200acc7efbc375c2bcab482d4cbffa9dba1f7900c15", "class_name": "RelatedNodeInfo"}}, "text": "\u00e7\u009f\u00a5\u00e4\u00b9\u008e\u00ef\u00bc\u008c\u00e8\u00ae\u00a9\u00e6\u00af\u008f\u00e4\u00b8\u0080\u00e6\u00ac\u00a1\u00e7\u0082\u00b9\u00e5\u0087\u00bb\u00e9\u0083\u00bd\u00e5\u0085\u0085\u00e6\u00bb\u00a1\u00e6\u0084\u008f\u00e4\u00b9\u0089 \u00e2\u0080\u0094\u00e2\u0080\u0094 \u00e6\u00ac\u00a2\u00e8\u00bf\u008e\u00e6\u009d\u00a5\u00e5\u0088\u00b0\u00e7\u009f\u00a5\u00e4\u00b9\u008e\u00ef\u00bc\u008c\u00e5\u008f\u0091\u00e7\u008e\u00b0\u00e9\u0097\u00ae\u00e9\u00a2\u0098\u00e8\u0083\u008c\u00e5\u0090\u008e\u00e7\u009a\u0084\u00e4\u00b8\u0096\u00e7\u0095\u008c\u00e3\u0080\u0082", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 101, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "cab3cdf6-a214-423a-bc9f-d6209782c1bc": {"__data__": {"id_": "cab3cdf6-a214-423a-bc9f-d6209782c1bc", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:29.202531", "added_at": "2024-10-31T15:59:29.204525"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5dd615d6-3b80-489a-a37f-6f54a5b89965", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:29.202531", "added_at": "2024-10-31T15:59:29.204525"}, "hash": "59ad76f532305a6d875a41033cd4f9f0f9ffb24c2a03013dd779a444323339ec", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "1fd30894-f6b7-41ae-9487-c0a2473f3f0e", "node_type": "1", "metadata": {}, "hash": "c7414eadf6fcf0baf17f18997adef5e5ea3e6399237a47c140a271588716cb7b", "class_name": "RelatedNodeInfo"}}, "text": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nSkip to main content\nWe gratefully acknowledge support from the Simons Foundation,\nmember institutions\n, and all contributors.\nDonate\n>\ncs\n>\narXiv:2410.23242\nHelp\n|\nAdvanced Search\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nopen search\nGO\nopen navigation menu\nquick links\nLogin\nHelp Pages\nAbout\nComputer Science > Artificial Intelligence\narXiv:2410.23242\n(cs)\n[Submitted on 30 Oct 2024]\nTitle:\nA little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nAuthors:\nMatteo G. Mecattaf\n,\nBen Slater\n,\nMarko Te\u0161i\u0107\n,\nJonathan Prunty\n,\nKonstantinos Voudouris\n,\nLucy G. Cheke\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nAbstract:\nAs general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1fd30894-f6b7-41ae-9487-c0a2473f3f0e": {"__data__": {"id_": "1fd30894-f6b7-41ae-9487-c0a2473f3f0e", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:29.202531", "added_at": "2024-10-31T15:59:29.204525"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5dd615d6-3b80-489a-a37f-6f54a5b89965", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:29.202531", "added_at": "2024-10-31T15:59:29.204525"}, "hash": "59ad76f532305a6d875a41033cd4f9f0f9ffb24c2a03013dd779a444323339ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "cab3cdf6-a214-423a-bc9f-d6209782c1bc", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:29.202531", "added_at": "2024-10-31T15:59:29.204525"}, "hash": "960bfe9ace1dd48a017489df9a91c2cc4b4daa6265bd585a50fe00e6e9c1f4f5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "944e96f8-b1c4-48ad-b473-fdc60a387fb1", "node_type": "1", "metadata": {}, "hash": "a29f392886c26828f5419c111b8e8537df1448ad1f9d786c14a25a2e556c6704", "class_name": "RelatedNodeInfo"}}, "text": "We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.\nComments:\n25 pages, 4 figures\nSubjects:\nArtificial Intelligence (cs.AI)\nCite as:\narXiv:2410.23242\n[cs.AI]\n(or\narXiv:2410.23242v1\n[cs.AI]\nfor this version)\nhttps://doi.org/10.48550/arXiv.2410.23242\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Ben Slater [\nview email\n]\n[v1]\nWed, 30 Oct 2024 17:28:28 UTC (17,714 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.AI\n<\u00a0prev\n|\nnext\u00a0>\nnew\n|\nrecent\n|\n2024-10\nChange to browse by:\ncs\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\na\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer\n(\nWhat is the Explorer?\n)\nConnected Papers Toggle\nConnected Papers\n(\nWhat is Connected Papers?\n)\nLitmaps Toggle\nLitmaps\n(\nWhat is Litmaps?\n)\nscite.ai Toggle\nscite Smart Citations\n(\nWhat are Smart Citations?\n)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv\n(\nWhat is alphaXiv?\n)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers\n(\nWhat is CatalyzeX?\n)\nDagsHub Toggle\nDagsHub\n(\nWhat is DagsHub?\n)\nGotitPub Toggle\nGotit.pub\n(\nWhat is GotitPub?\n)\nHuggingface Toggle\nHugging Face\n(\nWhat is Huggingface?\n)\nLinks to Code Toggle\nPapers with Code\n(\nWhat is Papers with Code?\n)\nScienceCast Toggle\nScienceCast\n(\nWhat is ScienceCast?\n)\nDemos\nDemos\nReplicate Toggle\nReplicate\n(\nWhat is Replicate?\n)\nSpaces Toggle\nHugging Face Spaces\n(\nWhat is Spaces?\n)\nSpaces Toggle\nTXYZ.AI\n(\nWhat is TXYZ.AI?\n)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 5759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "944e96f8-b1c4-48ad-b473-fdc60a387fb1": {"__data__": {"id_": "944e96f8-b1c4-48ad-b473-fdc60a387fb1", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:29.202531", "added_at": "2024-10-31T15:59:29.204525"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5dd615d6-3b80-489a-a37f-6f54a5b89965", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:29.202531", "added_at": "2024-10-31T15:59:29.204525"}, "hash": "59ad76f532305a6d875a41033cd4f9f0f9ffb24c2a03013dd779a444323339ec", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1fd30894-f6b7-41ae-9487-c0a2473f3f0e", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:29.202531", "added_at": "2024-10-31T15:59:29.204525"}, "hash": "f5ba30f05d1f823f42393cc2c17f5790d9323f41fefc478d5758ef262a1e7d9b", "class_name": "RelatedNodeInfo"}}, "text": "Related Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)\nAbout\nHelp\ncontact arXiv\nClick here to contact arXiv\nContact\nsubscribe to arXiv mailings\nClick here to subscribe\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\narXiv Operational Status\nGet status notifications via\nemail\nor\nslack", "mimetype": "text/plain", "start_char_idx": 4934, "end_char_idx": 6005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f522e3ef-7a76-4a57-8d1e-e73ee37b5711": {"__data__": {"id_": "f522e3ef-7a76-4a57-8d1e-e73ee37b5711", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:55.804257", "added_at": "2024-10-31T15:59:55.807249"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c9a7e77-aceb-49f8-bcf7-1937f125dede", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:55.804257", "added_at": "2024-10-31T15:59:55.807249"}, "hash": "69e944ca474dd8dda07dac9f7833449f25f0ccfeff3b76fe919fa2bc6f812dea", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7bae6454-9fea-4b7e-84f9-6824aa0e7d4c", "node_type": "1", "metadata": {}, "hash": "c7414eadf6fcf0baf17f18997adef5e5ea3e6399237a47c140a271588716cb7b", "class_name": "RelatedNodeInfo"}}, "text": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nSkip to main content\nWe gratefully acknowledge support from the Simons Foundation,\nmember institutions\n, and all contributors.\nDonate\n>\ncs\n>\narXiv:2410.23242\nHelp\n|\nAdvanced Search\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nopen search\nGO\nopen navigation menu\nquick links\nLogin\nHelp Pages\nAbout\nComputer Science > Artificial Intelligence\narXiv:2410.23242\n(cs)\n[Submitted on 30 Oct 2024]\nTitle:\nA little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nAuthors:\nMatteo G. Mecattaf\n,\nBen Slater\n,\nMarko Te\u0161i\u0107\n,\nJonathan Prunty\n,\nKonstantinos Voudouris\n,\nLucy G. Cheke\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nAbstract:\nAs general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7bae6454-9fea-4b7e-84f9-6824aa0e7d4c": {"__data__": {"id_": "7bae6454-9fea-4b7e-84f9-6824aa0e7d4c", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:55.804257", "added_at": "2024-10-31T15:59:55.807249"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c9a7e77-aceb-49f8-bcf7-1937f125dede", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:55.804257", "added_at": "2024-10-31T15:59:55.807249"}, "hash": "69e944ca474dd8dda07dac9f7833449f25f0ccfeff3b76fe919fa2bc6f812dea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "f522e3ef-7a76-4a57-8d1e-e73ee37b5711", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:55.804257", "added_at": "2024-10-31T15:59:55.807249"}, "hash": "67d6036c213e5fe9d1c11f0e92e9922a906211d35facc4322800584702b28f11", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "35fa1fa1-0baa-41fa-b557-e43b7bcdac22", "node_type": "1", "metadata": {}, "hash": "a29f392886c26828f5419c111b8e8537df1448ad1f9d786c14a25a2e556c6704", "class_name": "RelatedNodeInfo"}}, "text": "We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.\nComments:\n25 pages, 4 figures\nSubjects:\nArtificial Intelligence (cs.AI)\nCite as:\narXiv:2410.23242\n[cs.AI]\n(or\narXiv:2410.23242v1\n[cs.AI]\nfor this version)\nhttps://doi.org/10.48550/arXiv.2410.23242\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Ben Slater [\nview email\n]\n[v1]\nWed, 30 Oct 2024 17:28:28 UTC (17,714 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.AI\n<\u00a0prev\n|\nnext\u00a0>\nnew\n|\nrecent\n|\n2024-10\nChange to browse by:\ncs\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\na\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer\n(\nWhat is the Explorer?\n)\nConnected Papers Toggle\nConnected Papers\n(\nWhat is Connected Papers?\n)\nLitmaps Toggle\nLitmaps\n(\nWhat is Litmaps?\n)\nscite.ai Toggle\nscite Smart Citations\n(\nWhat are Smart Citations?\n)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv\n(\nWhat is alphaXiv?\n)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers\n(\nWhat is CatalyzeX?\n)\nDagsHub Toggle\nDagsHub\n(\nWhat is DagsHub?\n)\nGotitPub Toggle\nGotit.pub\n(\nWhat is GotitPub?\n)\nHuggingface Toggle\nHugging Face\n(\nWhat is Huggingface?\n)\nLinks to Code Toggle\nPapers with Code\n(\nWhat is Papers with Code?\n)\nScienceCast Toggle\nScienceCast\n(\nWhat is ScienceCast?\n)\nDemos\nDemos\nReplicate Toggle\nReplicate\n(\nWhat is Replicate?\n)\nSpaces Toggle\nHugging Face Spaces\n(\nWhat is Spaces?\n)\nSpaces Toggle\nTXYZ.AI\n(\nWhat is TXYZ.AI?\n)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 5759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "35fa1fa1-0baa-41fa-b557-e43b7bcdac22": {"__data__": {"id_": "35fa1fa1-0baa-41fa-b557-e43b7bcdac22", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:55.804257", "added_at": "2024-10-31T15:59:55.807249"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8c9a7e77-aceb-49f8-bcf7-1937f125dede", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:55.804257", "added_at": "2024-10-31T15:59:55.807249"}, "hash": "69e944ca474dd8dda07dac9f7833449f25f0ccfeff3b76fe919fa2bc6f812dea", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "7bae6454-9fea-4b7e-84f9-6824aa0e7d4c", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:55.804257", "added_at": "2024-10-31T15:59:55.807249"}, "hash": "787eda77efaf5130e60dd2f8b3745916b25c82daeb2945507502ee0655df3509", "class_name": "RelatedNodeInfo"}}, "text": "Related Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)\nAbout\nHelp\ncontact arXiv\nClick here to contact arXiv\nContact\nsubscribe to arXiv mailings\nClick here to subscribe\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\narXiv Operational Status\nGet status notifications via\nemail\nor\nslack", "mimetype": "text/plain", "start_char_idx": 4934, "end_char_idx": 6005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "96a4dc85-2e06-4208-adc4-df4a79cf859d": {"__data__": {"id_": "96a4dc85-2e06-4208-adc4-df4a79cf859d", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:00:21.628725", "added_at": "2024-10-31T16:00:21.630719"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5be37bb-881a-4e89-89ce-ba59ad94e5f8", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:00:21.628725", "added_at": "2024-10-31T16:00:21.630719"}, "hash": "28d1bebc52c0aefe9d491ab2728bcbb5c4ca4c2270917c8fb120c9d2712b1ba5", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "34a6fe28-63af-47de-9e80-048536082c3e", "node_type": "1", "metadata": {}, "hash": "c7414eadf6fcf0baf17f18997adef5e5ea3e6399237a47c140a271588716cb7b", "class_name": "RelatedNodeInfo"}}, "text": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nSkip to main content\nWe gratefully acknowledge support from the Simons Foundation,\nmember institutions\n, and all contributors.\nDonate\n>\ncs\n>\narXiv:2410.23242\nHelp\n|\nAdvanced Search\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nopen search\nGO\nopen navigation menu\nquick links\nLogin\nHelp Pages\nAbout\nComputer Science > Artificial Intelligence\narXiv:2410.23242\n(cs)\n[Submitted on 30 Oct 2024]\nTitle:\nA little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nAuthors:\nMatteo G. Mecattaf\n,\nBen Slater\n,\nMarko Te\u0161i\u0107\n,\nJonathan Prunty\n,\nKonstantinos Voudouris\n,\nLucy G. Cheke\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nAbstract:\nAs general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "34a6fe28-63af-47de-9e80-048536082c3e": {"__data__": {"id_": "34a6fe28-63af-47de-9e80-048536082c3e", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:00:21.628725", "added_at": "2024-10-31T16:00:21.630719"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5be37bb-881a-4e89-89ce-ba59ad94e5f8", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:00:21.628725", "added_at": "2024-10-31T16:00:21.630719"}, "hash": "28d1bebc52c0aefe9d491ab2728bcbb5c4ca4c2270917c8fb120c9d2712b1ba5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "96a4dc85-2e06-4208-adc4-df4a79cf859d", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:00:21.628725", "added_at": "2024-10-31T16:00:21.630719"}, "hash": "bbe4f6e2148cdb008cf610d4d5fff1b1630eeab4afa37d1d049f2da7c9f290e3", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "7663abd3-e54c-419e-b82a-aea60e7204bc", "node_type": "1", "metadata": {}, "hash": "a29f392886c26828f5419c111b8e8537df1448ad1f9d786c14a25a2e556c6704", "class_name": "RelatedNodeInfo"}}, "text": "We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.\nComments:\n25 pages, 4 figures\nSubjects:\nArtificial Intelligence (cs.AI)\nCite as:\narXiv:2410.23242\n[cs.AI]\n(or\narXiv:2410.23242v1\n[cs.AI]\nfor this version)\nhttps://doi.org/10.48550/arXiv.2410.23242\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Ben Slater [\nview email\n]\n[v1]\nWed, 30 Oct 2024 17:28:28 UTC (17,714 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.AI\n<\u00a0prev\n|\nnext\u00a0>\nnew\n|\nrecent\n|\n2024-10\nChange to browse by:\ncs\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\na\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer\n(\nWhat is the Explorer?\n)\nConnected Papers Toggle\nConnected Papers\n(\nWhat is Connected Papers?\n)\nLitmaps Toggle\nLitmaps\n(\nWhat is Litmaps?\n)\nscite.ai Toggle\nscite Smart Citations\n(\nWhat are Smart Citations?\n)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv\n(\nWhat is alphaXiv?\n)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers\n(\nWhat is CatalyzeX?\n)\nDagsHub Toggle\nDagsHub\n(\nWhat is DagsHub?\n)\nGotitPub Toggle\nGotit.pub\n(\nWhat is GotitPub?\n)\nHuggingface Toggle\nHugging Face\n(\nWhat is Huggingface?\n)\nLinks to Code Toggle\nPapers with Code\n(\nWhat is Papers with Code?\n)\nScienceCast Toggle\nScienceCast\n(\nWhat is ScienceCast?\n)\nDemos\nDemos\nReplicate Toggle\nReplicate\n(\nWhat is Replicate?\n)\nSpaces Toggle\nHugging Face Spaces\n(\nWhat is Spaces?\n)\nSpaces Toggle\nTXYZ.AI\n(\nWhat is TXYZ.AI?\n)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 5759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "7663abd3-e54c-419e-b82a-aea60e7204bc": {"__data__": {"id_": "7663abd3-e54c-419e-b82a-aea60e7204bc", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:00:21.628725", "added_at": "2024-10-31T16:00:21.630719"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b5be37bb-881a-4e89-89ce-ba59ad94e5f8", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:00:21.628725", "added_at": "2024-10-31T16:00:21.630719"}, "hash": "28d1bebc52c0aefe9d491ab2728bcbb5c4ca4c2270917c8fb120c9d2712b1ba5", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "34a6fe28-63af-47de-9e80-048536082c3e", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:00:21.628725", "added_at": "2024-10-31T16:00:21.630719"}, "hash": "25d0508cf8f9e7ef2defbf75657f4d46062955c5f507a6deb5bd09681db6509b", "class_name": "RelatedNodeInfo"}}, "text": "Related Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)\nAbout\nHelp\ncontact arXiv\nClick here to contact arXiv\nContact\nsubscribe to arXiv mailings\nClick here to subscribe\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\narXiv Operational Status\nGet status notifications via\nemail\nor\nslack", "mimetype": "text/plain", "start_char_idx": 4934, "end_char_idx": 6005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "c488e5f4-2fba-4629-8973-0151a2b7135e": {"__data__": {"id_": "c488e5f4-2fba-4629-8973-0151a2b7135e", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:11.557451", "added_at": "2024-10-31T16:04:11.559447"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2e677cf-e19a-4fae-8638-8a20998cc745", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:11.557451", "added_at": "2024-10-31T16:04:11.559447"}, "hash": "dc3f83ca1a9c37b68c6daf85324d3f4227a6b6e030ada45fff3c23373afe683c", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "e6ba8e06-1662-4cb9-8354-eb274b97ff6d", "node_type": "1", "metadata": {}, "hash": "c7414eadf6fcf0baf17f18997adef5e5ea3e6399237a47c140a271588716cb7b", "class_name": "RelatedNodeInfo"}}, "text": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nSkip to main content\nWe gratefully acknowledge support from the Simons Foundation,\nmember institutions\n, and all contributors.\nDonate\n>\ncs\n>\narXiv:2410.23242\nHelp\n|\nAdvanced Search\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nopen search\nGO\nopen navigation menu\nquick links\nLogin\nHelp Pages\nAbout\nComputer Science > Artificial Intelligence\narXiv:2410.23242\n(cs)\n[Submitted on 30 Oct 2024]\nTitle:\nA little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nAuthors:\nMatteo G. Mecattaf\n,\nBen Slater\n,\nMarko Te\u0161i\u0107\n,\nJonathan Prunty\n,\nKonstantinos Voudouris\n,\nLucy G. Cheke\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nAbstract:\nAs general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "e6ba8e06-1662-4cb9-8354-eb274b97ff6d": {"__data__": {"id_": "e6ba8e06-1662-4cb9-8354-eb274b97ff6d", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:11.557451", "added_at": "2024-10-31T16:04:11.559447"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2e677cf-e19a-4fae-8638-8a20998cc745", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:11.557451", "added_at": "2024-10-31T16:04:11.559447"}, "hash": "dc3f83ca1a9c37b68c6daf85324d3f4227a6b6e030ada45fff3c23373afe683c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "c488e5f4-2fba-4629-8973-0151a2b7135e", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:11.557451", "added_at": "2024-10-31T16:04:11.559447"}, "hash": "8a1d31bb66fe7ab2214d5e3136ef90d5934ff9fa8ed1ff53eca68763334e7201", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "83f0c2dd-6b81-4825-9e7f-e0156f2a6834", "node_type": "1", "metadata": {}, "hash": "a29f392886c26828f5419c111b8e8537df1448ad1f9d786c14a25a2e556c6704", "class_name": "RelatedNodeInfo"}}, "text": "We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.\nComments:\n25 pages, 4 figures\nSubjects:\nArtificial Intelligence (cs.AI)\nCite as:\narXiv:2410.23242\n[cs.AI]\n(or\narXiv:2410.23242v1\n[cs.AI]\nfor this version)\nhttps://doi.org/10.48550/arXiv.2410.23242\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Ben Slater [\nview email\n]\n[v1]\nWed, 30 Oct 2024 17:28:28 UTC (17,714 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.AI\n<\u00a0prev\n|\nnext\u00a0>\nnew\n|\nrecent\n|\n2024-10\nChange to browse by:\ncs\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\na\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer\n(\nWhat is the Explorer?\n)\nConnected Papers Toggle\nConnected Papers\n(\nWhat is Connected Papers?\n)\nLitmaps Toggle\nLitmaps\n(\nWhat is Litmaps?\n)\nscite.ai Toggle\nscite Smart Citations\n(\nWhat are Smart Citations?\n)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv\n(\nWhat is alphaXiv?\n)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers\n(\nWhat is CatalyzeX?\n)\nDagsHub Toggle\nDagsHub\n(\nWhat is DagsHub?\n)\nGotitPub Toggle\nGotit.pub\n(\nWhat is GotitPub?\n)\nHuggingface Toggle\nHugging Face\n(\nWhat is Huggingface?\n)\nLinks to Code Toggle\nPapers with Code\n(\nWhat is Papers with Code?\n)\nScienceCast Toggle\nScienceCast\n(\nWhat is ScienceCast?\n)\nDemos\nDemos\nReplicate Toggle\nReplicate\n(\nWhat is Replicate?\n)\nSpaces Toggle\nHugging Face Spaces\n(\nWhat is Spaces?\n)\nSpaces Toggle\nTXYZ.AI\n(\nWhat is TXYZ.AI?\n)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 5759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "83f0c2dd-6b81-4825-9e7f-e0156f2a6834": {"__data__": {"id_": "83f0c2dd-6b81-4825-9e7f-e0156f2a6834", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:11.557451", "added_at": "2024-10-31T16:04:11.559447"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c2e677cf-e19a-4fae-8638-8a20998cc745", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:11.557451", "added_at": "2024-10-31T16:04:11.559447"}, "hash": "dc3f83ca1a9c37b68c6daf85324d3f4227a6b6e030ada45fff3c23373afe683c", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "e6ba8e06-1662-4cb9-8354-eb274b97ff6d", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:11.557451", "added_at": "2024-10-31T16:04:11.559447"}, "hash": "6906832f0caffcd84cc1aa2e0399937803b17ebb13db5a98a2aef133c7c10273", "class_name": "RelatedNodeInfo"}}, "text": "Related Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)\nAbout\nHelp\ncontact arXiv\nClick here to contact arXiv\nContact\nsubscribe to arXiv mailings\nClick here to subscribe\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\narXiv Operational Status\nGet status notifications via\nemail\nor\nslack", "mimetype": "text/plain", "start_char_idx": 4934, "end_char_idx": 6005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "1cc8d102-be31-4fe2-b82b-ab8a485886db": {"__data__": {"id_": "1cc8d102-be31-4fe2-b82b-ab8a485886db", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:51.931991", "added_at": "2024-10-31T16:04:51.933984"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f1998f6-c5e6-4079-ae42-69a6696165f3", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:51.931991", "added_at": "2024-10-31T16:04:51.933984"}, "hash": "fc04b1583c936fd0231e0ba7cd39b40df5ef24c4b4b850bcc3c0b7fb24d39282", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "ba38bf66-0f3b-4460-a4fc-aa269f27a514", "node_type": "1", "metadata": {}, "hash": "c7414eadf6fcf0baf17f18997adef5e5ea3e6399237a47c140a271588716cb7b", "class_name": "RelatedNodeInfo"}}, "text": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nSkip to main content\nWe gratefully acknowledge support from the Simons Foundation,\nmember institutions\n, and all contributors.\nDonate\n>\ncs\n>\narXiv:2410.23242\nHelp\n|\nAdvanced Search\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nopen search\nGO\nopen navigation menu\nquick links\nLogin\nHelp Pages\nAbout\nComputer Science > Artificial Intelligence\narXiv:2410.23242\n(cs)\n[Submitted on 30 Oct 2024]\nTitle:\nA little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nAuthors:\nMatteo G. Mecattaf\n,\nBen Slater\n,\nMarko Te\u0161i\u0107\n,\nJonathan Prunty\n,\nKonstantinos Voudouris\n,\nLucy G. Cheke\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nAbstract:\nAs general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "ba38bf66-0f3b-4460-a4fc-aa269f27a514": {"__data__": {"id_": "ba38bf66-0f3b-4460-a4fc-aa269f27a514", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:51.931991", "added_at": "2024-10-31T16:04:51.933984"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f1998f6-c5e6-4079-ae42-69a6696165f3", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:51.931991", "added_at": "2024-10-31T16:04:51.933984"}, "hash": "fc04b1583c936fd0231e0ba7cd39b40df5ef24c4b4b850bcc3c0b7fb24d39282", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "1cc8d102-be31-4fe2-b82b-ab8a485886db", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:51.931991", "added_at": "2024-10-31T16:04:51.933984"}, "hash": "fab9f254c90e893b1ae70cbd369bb2fdb7bcd1b1e12ce9be70486e82bd4972d4", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "f2b57ebf-13fc-4d45-92c9-9a8d80e4de67", "node_type": "1", "metadata": {}, "hash": "a29f392886c26828f5419c111b8e8537df1448ad1f9d786c14a25a2e556c6704", "class_name": "RelatedNodeInfo"}}, "text": "We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.\nComments:\n25 pages, 4 figures\nSubjects:\nArtificial Intelligence (cs.AI)\nCite as:\narXiv:2410.23242\n[cs.AI]\n(or\narXiv:2410.23242v1\n[cs.AI]\nfor this version)\nhttps://doi.org/10.48550/arXiv.2410.23242\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Ben Slater [\nview email\n]\n[v1]\nWed, 30 Oct 2024 17:28:28 UTC (17,714 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.AI\n<\u00a0prev\n|\nnext\u00a0>\nnew\n|\nrecent\n|\n2024-10\nChange to browse by:\ncs\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\na\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer\n(\nWhat is the Explorer?\n)\nConnected Papers Toggle\nConnected Papers\n(\nWhat is Connected Papers?\n)\nLitmaps Toggle\nLitmaps\n(\nWhat is Litmaps?\n)\nscite.ai Toggle\nscite Smart Citations\n(\nWhat are Smart Citations?\n)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv\n(\nWhat is alphaXiv?\n)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers\n(\nWhat is CatalyzeX?\n)\nDagsHub Toggle\nDagsHub\n(\nWhat is DagsHub?\n)\nGotitPub Toggle\nGotit.pub\n(\nWhat is GotitPub?\n)\nHuggingface Toggle\nHugging Face\n(\nWhat is Huggingface?\n)\nLinks to Code Toggle\nPapers with Code\n(\nWhat is Papers with Code?\n)\nScienceCast Toggle\nScienceCast\n(\nWhat is ScienceCast?\n)\nDemos\nDemos\nReplicate Toggle\nReplicate\n(\nWhat is Replicate?\n)\nSpaces Toggle\nHugging Face Spaces\n(\nWhat is Spaces?\n)\nSpaces Toggle\nTXYZ.AI\n(\nWhat is TXYZ.AI?\n)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 5759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "f2b57ebf-13fc-4d45-92c9-9a8d80e4de67": {"__data__": {"id_": "f2b57ebf-13fc-4d45-92c9-9a8d80e4de67", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:51.931991", "added_at": "2024-10-31T16:04:51.933984"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f1998f6-c5e6-4079-ae42-69a6696165f3", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:51.931991", "added_at": "2024-10-31T16:04:51.933984"}, "hash": "fc04b1583c936fd0231e0ba7cd39b40df5ef24c4b4b850bcc3c0b7fb24d39282", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "ba38bf66-0f3b-4460-a4fc-aa269f27a514", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:51.931991", "added_at": "2024-10-31T16:04:51.933984"}, "hash": "c2fdda2592ca3c3a2fe9e9392e5f12d8f0f6515a4412127b6e0a01980019a387", "class_name": "RelatedNodeInfo"}}, "text": "Related Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)\nAbout\nHelp\ncontact arXiv\nClick here to contact arXiv\nContact\nsubscribe to arXiv mailings\nClick here to subscribe\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\narXiv Operational Status\nGet status notifications via\nemail\nor\nslack", "mimetype": "text/plain", "start_char_idx": 4934, "end_char_idx": 6005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "77acc3c9-010b-42b4-8058-501f6af7ed5e": {"__data__": {"id_": "77acc3c9-010b-42b4-8058-501f6af7ed5e", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:20.712155", "added_at": "2024-10-31T16:05:20.714149"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46ea4f6c-a5dc-4540-af10-5b97becb5b0b", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:20.712155", "added_at": "2024-10-31T16:05:20.714149"}, "hash": "f649a0df836496002cdcf0f5dd1fbf96275f8f9d1b06ca574d4d1ffceb5bd394", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "d550416b-a737-4394-af23-19a7030926bf", "node_type": "1", "metadata": {}, "hash": "c7414eadf6fcf0baf17f18997adef5e5ea3e6399237a47c140a271588716cb7b", "class_name": "RelatedNodeInfo"}}, "text": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nSkip to main content\nWe gratefully acknowledge support from the Simons Foundation,\nmember institutions\n, and all contributors.\nDonate\n>\ncs\n>\narXiv:2410.23242\nHelp\n|\nAdvanced Search\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nopen search\nGO\nopen navigation menu\nquick links\nLogin\nHelp Pages\nAbout\nComputer Science > Artificial Intelligence\narXiv:2410.23242\n(cs)\n[Submitted on 30 Oct 2024]\nTitle:\nA little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nAuthors:\nMatteo G. Mecattaf\n,\nBen Slater\n,\nMarko Te\u0161i\u0107\n,\nJonathan Prunty\n,\nKonstantinos Voudouris\n,\nLucy G. Cheke\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nAbstract:\nAs general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "d550416b-a737-4394-af23-19a7030926bf": {"__data__": {"id_": "d550416b-a737-4394-af23-19a7030926bf", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:20.712155", "added_at": "2024-10-31T16:05:20.714149"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46ea4f6c-a5dc-4540-af10-5b97becb5b0b", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:20.712155", "added_at": "2024-10-31T16:05:20.714149"}, "hash": "f649a0df836496002cdcf0f5dd1fbf96275f8f9d1b06ca574d4d1ffceb5bd394", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "77acc3c9-010b-42b4-8058-501f6af7ed5e", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:20.712155", "added_at": "2024-10-31T16:05:20.714149"}, "hash": "160f2aa5c1255a9da9fdd57e8e427095ba319d81efb83611e642754fc65d295a", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "97726f7c-7ec1-4634-a537-31a89ef4f13a", "node_type": "1", "metadata": {}, "hash": "a29f392886c26828f5419c111b8e8537df1448ad1f9d786c14a25a2e556c6704", "class_name": "RelatedNodeInfo"}}, "text": "We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.\nComments:\n25 pages, 4 figures\nSubjects:\nArtificial Intelligence (cs.AI)\nCite as:\narXiv:2410.23242\n[cs.AI]\n(or\narXiv:2410.23242v1\n[cs.AI]\nfor this version)\nhttps://doi.org/10.48550/arXiv.2410.23242\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Ben Slater [\nview email\n]\n[v1]\nWed, 30 Oct 2024 17:28:28 UTC (17,714 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.AI\n<\u00a0prev\n|\nnext\u00a0>\nnew\n|\nrecent\n|\n2024-10\nChange to browse by:\ncs\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\na\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer\n(\nWhat is the Explorer?\n)\nConnected Papers Toggle\nConnected Papers\n(\nWhat is Connected Papers?\n)\nLitmaps Toggle\nLitmaps\n(\nWhat is Litmaps?\n)\nscite.ai Toggle\nscite Smart Citations\n(\nWhat are Smart Citations?\n)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv\n(\nWhat is alphaXiv?\n)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers\n(\nWhat is CatalyzeX?\n)\nDagsHub Toggle\nDagsHub\n(\nWhat is DagsHub?\n)\nGotitPub Toggle\nGotit.pub\n(\nWhat is GotitPub?\n)\nHuggingface Toggle\nHugging Face\n(\nWhat is Huggingface?\n)\nLinks to Code Toggle\nPapers with Code\n(\nWhat is Papers with Code?\n)\nScienceCast Toggle\nScienceCast\n(\nWhat is ScienceCast?\n)\nDemos\nDemos\nReplicate Toggle\nReplicate\n(\nWhat is Replicate?\n)\nSpaces Toggle\nHugging Face Spaces\n(\nWhat is Spaces?\n)\nSpaces Toggle\nTXYZ.AI\n(\nWhat is TXYZ.AI?\n)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 5759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "97726f7c-7ec1-4634-a537-31a89ef4f13a": {"__data__": {"id_": "97726f7c-7ec1-4634-a537-31a89ef4f13a", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:20.712155", "added_at": "2024-10-31T16:05:20.714149"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "46ea4f6c-a5dc-4540-af10-5b97becb5b0b", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:20.712155", "added_at": "2024-10-31T16:05:20.714149"}, "hash": "f649a0df836496002cdcf0f5dd1fbf96275f8f9d1b06ca574d4d1ffceb5bd394", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "d550416b-a737-4394-af23-19a7030926bf", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:20.712155", "added_at": "2024-10-31T16:05:20.714149"}, "hash": "4f6ca4ef7a836158d2cc4808145365df2181ffa9a85b1eeb9e10c67dd6b6e7b6", "class_name": "RelatedNodeInfo"}}, "text": "Related Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)\nAbout\nHelp\ncontact arXiv\nClick here to contact arXiv\nContact\nsubscribe to arXiv mailings\nClick here to subscribe\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\narXiv Operational Status\nGet status notifications via\nemail\nor\nslack", "mimetype": "text/plain", "start_char_idx": 4934, "end_char_idx": 6005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "8050e939-1125-42cd-bd95-4b3e377487fd": {"__data__": {"id_": "8050e939-1125-42cd-bd95-4b3e377487fd", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:38.561531", "added_at": "2024-10-31T16:05:38.564523"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "517284ca-f4a1-47b9-84cd-c5260fc9d8c4", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:38.561531", "added_at": "2024-10-31T16:05:38.564523"}, "hash": "72bb7d0eaac97f9b144e0a1ba0c6d7cfe2da0a385a3f586acefcc76718cf0aca", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "a55b8e24-c615-46c0-acb5-6f11a4716590", "node_type": "1", "metadata": {}, "hash": "c7414eadf6fcf0baf17f18997adef5e5ea3e6399237a47c140a271588716cb7b", "class_name": "RelatedNodeInfo"}}, "text": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nSkip to main content\nWe gratefully acknowledge support from the Simons Foundation,\nmember institutions\n, and all contributors.\nDonate\n>\ncs\n>\narXiv:2410.23242\nHelp\n|\nAdvanced Search\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nopen search\nGO\nopen navigation menu\nquick links\nLogin\nHelp Pages\nAbout\nComputer Science > Artificial Intelligence\narXiv:2410.23242\n(cs)\n[Submitted on 30 Oct 2024]\nTitle:\nA little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nAuthors:\nMatteo G. Mecattaf\n,\nBen Slater\n,\nMarko Te\u0161i\u0107\n,\nJonathan Prunty\n,\nKonstantinos Voudouris\n,\nLucy G. Cheke\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nAbstract:\nAs general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "a55b8e24-c615-46c0-acb5-6f11a4716590": {"__data__": {"id_": "a55b8e24-c615-46c0-acb5-6f11a4716590", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:38.561531", "added_at": "2024-10-31T16:05:38.564523"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "517284ca-f4a1-47b9-84cd-c5260fc9d8c4", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:38.561531", "added_at": "2024-10-31T16:05:38.564523"}, "hash": "72bb7d0eaac97f9b144e0a1ba0c6d7cfe2da0a385a3f586acefcc76718cf0aca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "8050e939-1125-42cd-bd95-4b3e377487fd", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:38.561531", "added_at": "2024-10-31T16:05:38.564523"}, "hash": "9256e4e553fc4aa4818496c2b032f6dfb1568ba64038a2f9719aa07b9c25f960", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "5ca4dc59-d00b-4d6d-9276-8d54014d351a", "node_type": "1", "metadata": {}, "hash": "a29f392886c26828f5419c111b8e8537df1448ad1f9d786c14a25a2e556c6704", "class_name": "RelatedNodeInfo"}}, "text": "We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.\nComments:\n25 pages, 4 figures\nSubjects:\nArtificial Intelligence (cs.AI)\nCite as:\narXiv:2410.23242\n[cs.AI]\n(or\narXiv:2410.23242v1\n[cs.AI]\nfor this version)\nhttps://doi.org/10.48550/arXiv.2410.23242\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Ben Slater [\nview email\n]\n[v1]\nWed, 30 Oct 2024 17:28:28 UTC (17,714 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.AI\n<\u00a0prev\n|\nnext\u00a0>\nnew\n|\nrecent\n|\n2024-10\nChange to browse by:\ncs\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\na\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer\n(\nWhat is the Explorer?\n)\nConnected Papers Toggle\nConnected Papers\n(\nWhat is Connected Papers?\n)\nLitmaps Toggle\nLitmaps\n(\nWhat is Litmaps?\n)\nscite.ai Toggle\nscite Smart Citations\n(\nWhat are Smart Citations?\n)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv\n(\nWhat is alphaXiv?\n)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers\n(\nWhat is CatalyzeX?\n)\nDagsHub Toggle\nDagsHub\n(\nWhat is DagsHub?\n)\nGotitPub Toggle\nGotit.pub\n(\nWhat is GotitPub?\n)\nHuggingface Toggle\nHugging Face\n(\nWhat is Huggingface?\n)\nLinks to Code Toggle\nPapers with Code\n(\nWhat is Papers with Code?\n)\nScienceCast Toggle\nScienceCast\n(\nWhat is ScienceCast?\n)\nDemos\nDemos\nReplicate Toggle\nReplicate\n(\nWhat is Replicate?\n)\nSpaces Toggle\nHugging Face Spaces\n(\nWhat is Spaces?\n)\nSpaces Toggle\nTXYZ.AI\n(\nWhat is TXYZ.AI?\n)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 5759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5ca4dc59-d00b-4d6d-9276-8d54014d351a": {"__data__": {"id_": "5ca4dc59-d00b-4d6d-9276-8d54014d351a", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:38.561531", "added_at": "2024-10-31T16:05:38.564523"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "517284ca-f4a1-47b9-84cd-c5260fc9d8c4", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:38.561531", "added_at": "2024-10-31T16:05:38.564523"}, "hash": "72bb7d0eaac97f9b144e0a1ba0c6d7cfe2da0a385a3f586acefcc76718cf0aca", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "a55b8e24-c615-46c0-acb5-6f11a4716590", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:38.561531", "added_at": "2024-10-31T16:05:38.564523"}, "hash": "5b50eb6ce5529a3fef986b4100834bbb4062e5fb4c53078bb8f57d41715846bb", "class_name": "RelatedNodeInfo"}}, "text": "Related Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)\nAbout\nHelp\ncontact arXiv\nClick here to contact arXiv\nContact\nsubscribe to arXiv mailings\nClick here to subscribe\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\narXiv Operational Status\nGet status notifications via\nemail\nor\nslack", "mimetype": "text/plain", "start_char_idx": 4934, "end_char_idx": 6005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "5e274bf4-4365-4b88-8de7-104c3376a8f8": {"__data__": {"id_": "5e274bf4-4365-4b88-8de7-104c3376a8f8", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:08:48.680590", "added_at": "2024-10-31T16:08:48.682584"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5adcf81d-04d2-42d0-9bcb-eebfcf56a669", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:08:48.680590", "added_at": "2024-10-31T16:08:48.682584"}, "hash": "2fc2598e52f082ad81a2ac18e06506a61e1a62d5c522fff7794e42c06c1bab47", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "4b6d0ac2-8d99-4601-bb80-f4ef955a7914", "node_type": "1", "metadata": {}, "hash": "c7414eadf6fcf0baf17f18997adef5e5ea3e6399237a47c140a271588716cb7b", "class_name": "RelatedNodeInfo"}}, "text": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nSkip to main content\nWe gratefully acknowledge support from the Simons Foundation,\nmember institutions\n, and all contributors.\nDonate\n>\ncs\n>\narXiv:2410.23242\nHelp\n|\nAdvanced Search\nAll fields\nTitle\nAuthor\nAbstract\nComments\nJournal reference\nACM classification\nMSC classification\nReport number\narXiv identifier\nDOI\nORCID\narXiv author ID\nHelp pages\nFull text\nSearch\nopen search\nGO\nopen navigation menu\nquick links\nLogin\nHelp Pages\nAbout\nComputer Science > Artificial Intelligence\narXiv:2410.23242\n(cs)\n[Submitted on 30 Oct 2024]\nTitle:\nA little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment\nAuthors:\nMatteo G. Mecattaf\n,\nBen Slater\n,\nMarko Te\u0161i\u0107\n,\nJonathan Prunty\n,\nKonstantinos Voudouris\n,\nLucy G. Cheke\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nAbstract:\nAs general-purpose tools, Large Language Models (LLMs) must often reason about everyday physical environments. In a question-and-answer capacity, understanding the interactions of physical objects may be necessary to give appropriate responses. Moreover, LLMs are increasingly used as reasoning engines in agentic systems, designing and controlling their action sequences. The vast majority of research has tackled this issue using static benchmarks, comprised of text or image-based questions about the physical world. However, these benchmarks do not capture the complexity and nuance of real-life physical processes. Here we advocate for a second, relatively unexplored, approach: 'embodying' the LLMs by granting them control of an agent within a 3D environment. We present the first embodied and cognitively meaningful evaluation of physical common-sense reasoning in LLMs. Our framework allows direct comparison of LLMs with other embodied agents, such as those based on Deep Reinforcement Learning, and human and non-human animals. We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 3083, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "4b6d0ac2-8d99-4601-bb80-f4ef955a7914": {"__data__": {"id_": "4b6d0ac2-8d99-4601-bb80-f4ef955a7914", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:08:48.680590", "added_at": "2024-10-31T16:08:48.682584"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5adcf81d-04d2-42d0-9bcb-eebfcf56a669", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:08:48.680590", "added_at": "2024-10-31T16:08:48.682584"}, "hash": "2fc2598e52f082ad81a2ac18e06506a61e1a62d5c522fff7794e42c06c1bab47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "5e274bf4-4365-4b88-8de7-104c3376a8f8", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:08:48.680590", "added_at": "2024-10-31T16:08:48.682584"}, "hash": "205f40654d5210f3daf97021db41877de0c684087bed1596dc769dc1216a500d", "class_name": "RelatedNodeInfo"}, "3": {"node_id": "80266dca-d108-4456-ab8a-1ce508de3d48", "node_type": "1", "metadata": {}, "hash": "a29f392886c26828f5419c111b8e8537df1448ad1f9d786c14a25a2e556c6704", "class_name": "RelatedNodeInfo"}}, "text": "We employ the Animal-AI (AAI) environment, a simulated 3D virtual laboratory, to study physical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a suite of experiments that replicate laboratory studies with non-human animals, to study physical reasoning capabilities including distance estimation, tracking out-of-sight objects, and tool use. We demonstrate that state-of-the-art multi-modal models with no finetuning can complete this style of task, allowing meaningful comparison to the entrants of the 2019 Animal-AI Olympics competition and to human children. Our results show that LLMs are currently outperformed by human children on these tasks. We argue that this approach allows the study of physical reasoning using ecologically valid experiments drawn directly from cognitive science, improving the predictability and reliability of LLMs.\nComments:\n25 pages, 4 figures\nSubjects:\nArtificial Intelligence (cs.AI)\nCite as:\narXiv:2410.23242\n[cs.AI]\n(or\narXiv:2410.23242v1\n[cs.AI]\nfor this version)\nhttps://doi.org/10.48550/arXiv.2410.23242\nFocus to learn more\narXiv-issued DOI via DataCite\nSubmission history\nFrom: Ben Slater [\nview email\n]\n[v1]\nWed, 30 Oct 2024 17:28:28 UTC (17,714 KB)\nFull-text links:\nAccess Paper:\nView a PDF of the paper titled A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment, by Matteo G. Mecattaf and 5 other authors\nView PDF\nHTML (experimental)\nTeX Source\nOther Formats\nview license\nCurrent browse context:\ncs.AI\n<\u00a0prev\n|\nnext\u00a0>\nnew\n|\nrecent\n|\n2024-10\nChange to browse by:\ncs\nReferences & Citations\nNASA ADS\nGoogle Scholar\nSemantic Scholar\na\nexport BibTeX citation\nLoading...\nBibTeX formatted citation\n\u00d7\nloading...\nData provided by:\nBookmark\nBibliographic Tools\nBibliographic and Citation Tools\nBibliographic Explorer Toggle\nBibliographic Explorer\n(\nWhat is the Explorer?\n)\nConnected Papers Toggle\nConnected Papers\n(\nWhat is Connected Papers?\n)\nLitmaps Toggle\nLitmaps\n(\nWhat is Litmaps?\n)\nscite.ai Toggle\nscite Smart Citations\n(\nWhat are Smart Citations?\n)\nCode, Data, Media\nCode, Data and Media Associated with this Article\nalphaXiv Toggle\nalphaXiv\n(\nWhat is alphaXiv?\n)\nLinks to Code Toggle\nCatalyzeX Code Finder for Papers\n(\nWhat is CatalyzeX?\n)\nDagsHub Toggle\nDagsHub\n(\nWhat is DagsHub?\n)\nGotitPub Toggle\nGotit.pub\n(\nWhat is GotitPub?\n)\nHuggingface Toggle\nHugging Face\n(\nWhat is Huggingface?\n)\nLinks to Code Toggle\nPapers with Code\n(\nWhat is Papers with Code?\n)\nScienceCast Toggle\nScienceCast\n(\nWhat is ScienceCast?\n)\nDemos\nDemos\nReplicate Toggle\nReplicate\n(\nWhat is Replicate?\n)\nSpaces Toggle\nHugging Face Spaces\n(\nWhat is Spaces?\n)\nSpaces Toggle\nTXYZ.AI\n(\nWhat is TXYZ.AI?\n)\nRelated Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)", "mimetype": "text/plain", "start_char_idx": 2218, "end_char_idx": 5759, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "80266dca-d108-4456-ab8a-1ce508de3d48": {"__data__": {"id_": "80266dca-d108-4456-ab8a-1ce508de3d48", "embedding": null, "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:08:48.680590", "added_at": "2024-10-31T16:08:48.682584"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5adcf81d-04d2-42d0-9bcb-eebfcf56a669", "node_type": "4", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:08:48.680590", "added_at": "2024-10-31T16:08:48.682584"}, "hash": "2fc2598e52f082ad81a2ac18e06506a61e1a62d5c522fff7794e42c06c1bab47", "class_name": "RelatedNodeInfo"}, "2": {"node_id": "4b6d0ac2-8d99-4601-bb80-f4ef955a7914", "node_type": "1", "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:08:48.680590", "added_at": "2024-10-31T16:08:48.682584"}, "hash": "baaa2468038f4f9977f01401e39d3b278974ba9e609b500ed64c71b8e81d752c", "class_name": "RelatedNodeInfo"}}, "text": "Related Papers\nRecommenders and Search Tools\nLink to Influence Flower\nInfluence Flower\n(\nWhat are Influence Flowers?\n)\nCore recommender toggle\nCORE Recommender\n(\nWhat is CORE?\n)\nAuthor\nVenue\nInstitution\nTopic\nAbout arXivLabs\narXivLabs: experimental projects with community collaborators\narXivLabs is a framework that allows collaborators to develop and share new arXiv features directly on our website.\nBoth individuals and organizations that work with arXivLabs have embraced and accepted our values of openness, community, excellence, and user data privacy. arXiv is committed to these values and only works with partners that adhere to them.\nHave an idea for a project that will add value for arXiv's community?\nLearn more about arXivLabs\n.\nWhich authors of this paper are endorsers?\n|\nDisable MathJax\n(\nWhat is MathJax?\n)\nAbout\nHelp\ncontact arXiv\nClick here to contact arXiv\nContact\nsubscribe to arXiv mailings\nClick here to subscribe\nSubscribe\nCopyright\nPrivacy Policy\nWeb Accessibility Assistance\narXiv Operational Status\nGet status notifications via\nemail\nor\nslack", "mimetype": "text/plain", "start_char_idx": 4934, "end_char_idx": 6005, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"9abe8186-d3ee-4b5f-941a-910011263a8e": {"doc_hash": "a90485033003ea0e83b00c9667bb95f40a2142ed40147278498502bfbf4683bb", "ref_doc_id": "c1e06d0c-6c8f-4bd0-bfbd-26df2eb621a6"}, "c1e06d0c-6c8f-4bd0-bfbd-26df2eb621a6": {"doc_hash": "a90485033003ea0e83b00c9667bb95f40a2142ed40147278498502bfbf4683bb"}, "89b77865-6983-4efa-a38a-a882cb25ef2a": {"doc_hash": "bc2f8e48c709c90a5554b2619838ba414adfc9e66359a27be1994fca47c98ca3", "ref_doc_id": "41e4c824-9e12-4cf0-9dee-24eeeb2c49d9"}, "41e4c824-9e12-4cf0-9dee-24eeeb2c49d9": {"doc_hash": "bc2f8e48c709c90a5554b2619838ba414adfc9e66359a27be1994fca47c98ca3"}, "b1b8a73f-49c5-4543-af78-54b2c32d8c1c": {"doc_hash": "eb9852952b9fb7e4c32c05ed26cfe5f47d97feb970d8fa2cb31e41e97292b824", "ref_doc_id": "5664aad3-4ca2-4875-ab83-efecfcf3be13"}, "5664aad3-4ca2-4875-ab83-efecfcf3be13": {"doc_hash": "eb9852952b9fb7e4c32c05ed26cfe5f47d97feb970d8fa2cb31e41e97292b824"}, "c8b3a2f3-8bf1-4d10-8ce7-0a44ba29b894": {"doc_hash": "ff22e09b7b97551eb7f79d3018884f76c4da25e918103fe1cb64dae3570d4d0c", "ref_doc_id": "bb299390-71a3-4727-b567-e022973f5854"}, "bb299390-71a3-4727-b567-e022973f5854": {"doc_hash": "ff22e09b7b97551eb7f79d3018884f76c4da25e918103fe1cb64dae3570d4d0c"}, "a9290983-7b2c-4198-bb2e-88286b2f80a0": {"doc_hash": "e7af174672feeb1cf30fb978c6c2cfce440fb58cf4d4592b1cdecc634d7cb693", "ref_doc_id": "7dbff074-b246-48c2-b559-69dceb4e36bf"}, "7dbff074-b246-48c2-b559-69dceb4e36bf": {"doc_hash": "e7af174672feeb1cf30fb978c6c2cfce440fb58cf4d4592b1cdecc634d7cb693"}, "e0b9120d-4aee-42c1-bbf6-e197e0cbf225": {"doc_hash": "10cb34ab6f0955549ccad200acc7efbc375c2bcab482d4cbffa9dba1f7900c15", "ref_doc_id": "ac60c74b-abed-4e3b-ae26-0ee99e4eef05"}, "ac60c74b-abed-4e3b-ae26-0ee99e4eef05": {"doc_hash": "10cb34ab6f0955549ccad200acc7efbc375c2bcab482d4cbffa9dba1f7900c15"}, "cab3cdf6-a214-423a-bc9f-d6209782c1bc": {"doc_hash": "960bfe9ace1dd48a017489df9a91c2cc4b4daa6265bd585a50fe00e6e9c1f4f5", "ref_doc_id": "5dd615d6-3b80-489a-a37f-6f54a5b89965"}, "1fd30894-f6b7-41ae-9487-c0a2473f3f0e": {"doc_hash": "f5ba30f05d1f823f42393cc2c17f5790d9323f41fefc478d5758ef262a1e7d9b", "ref_doc_id": "5dd615d6-3b80-489a-a37f-6f54a5b89965"}, "944e96f8-b1c4-48ad-b473-fdc60a387fb1": {"doc_hash": "9df3510dd4e88efc2d6ae4d2107cf45f7ff8c96f6c52618be8848ce8fae22ef3", "ref_doc_id": "5dd615d6-3b80-489a-a37f-6f54a5b89965"}, "5dd615d6-3b80-489a-a37f-6f54a5b89965": {"doc_hash": "59ad76f532305a6d875a41033cd4f9f0f9ffb24c2a03013dd779a444323339ec"}, "f522e3ef-7a76-4a57-8d1e-e73ee37b5711": {"doc_hash": "67d6036c213e5fe9d1c11f0e92e9922a906211d35facc4322800584702b28f11", "ref_doc_id": "8c9a7e77-aceb-49f8-bcf7-1937f125dede"}, "7bae6454-9fea-4b7e-84f9-6824aa0e7d4c": {"doc_hash": "787eda77efaf5130e60dd2f8b3745916b25c82daeb2945507502ee0655df3509", "ref_doc_id": "8c9a7e77-aceb-49f8-bcf7-1937f125dede"}, "35fa1fa1-0baa-41fa-b557-e43b7bcdac22": {"doc_hash": "b4127045e58e28c04384f8326af6d37689289156f1b366422471f47013a2f1ed", "ref_doc_id": "8c9a7e77-aceb-49f8-bcf7-1937f125dede"}, "8c9a7e77-aceb-49f8-bcf7-1937f125dede": {"doc_hash": "69e944ca474dd8dda07dac9f7833449f25f0ccfeff3b76fe919fa2bc6f812dea"}, "96a4dc85-2e06-4208-adc4-df4a79cf859d": {"doc_hash": "bbe4f6e2148cdb008cf610d4d5fff1b1630eeab4afa37d1d049f2da7c9f290e3", "ref_doc_id": "b5be37bb-881a-4e89-89ce-ba59ad94e5f8"}, "34a6fe28-63af-47de-9e80-048536082c3e": {"doc_hash": "25d0508cf8f9e7ef2defbf75657f4d46062955c5f507a6deb5bd09681db6509b", "ref_doc_id": "b5be37bb-881a-4e89-89ce-ba59ad94e5f8"}, "7663abd3-e54c-419e-b82a-aea60e7204bc": {"doc_hash": "55ca5e1363415b10f2728d3e991c9810b8b9cefa81079e3742032a3e180c273d", "ref_doc_id": "b5be37bb-881a-4e89-89ce-ba59ad94e5f8"}, "b5be37bb-881a-4e89-89ce-ba59ad94e5f8": {"doc_hash": "28d1bebc52c0aefe9d491ab2728bcbb5c4ca4c2270917c8fb120c9d2712b1ba5"}, "c488e5f4-2fba-4629-8973-0151a2b7135e": {"doc_hash": "8a1d31bb66fe7ab2214d5e3136ef90d5934ff9fa8ed1ff53eca68763334e7201", "ref_doc_id": "c2e677cf-e19a-4fae-8638-8a20998cc745"}, "e6ba8e06-1662-4cb9-8354-eb274b97ff6d": {"doc_hash": "6906832f0caffcd84cc1aa2e0399937803b17ebb13db5a98a2aef133c7c10273", "ref_doc_id": "c2e677cf-e19a-4fae-8638-8a20998cc745"}, "83f0c2dd-6b81-4825-9e7f-e0156f2a6834": {"doc_hash": "9ed15be02543061272ea95b214710af59531eaa3776ac5ab74f912dc75fd0260", "ref_doc_id": "c2e677cf-e19a-4fae-8638-8a20998cc745"}, "c2e677cf-e19a-4fae-8638-8a20998cc745": {"doc_hash": "dc3f83ca1a9c37b68c6daf85324d3f4227a6b6e030ada45fff3c23373afe683c"}, "1cc8d102-be31-4fe2-b82b-ab8a485886db": {"doc_hash": "fab9f254c90e893b1ae70cbd369bb2fdb7bcd1b1e12ce9be70486e82bd4972d4", "ref_doc_id": "5f1998f6-c5e6-4079-ae42-69a6696165f3"}, "ba38bf66-0f3b-4460-a4fc-aa269f27a514": {"doc_hash": "c2fdda2592ca3c3a2fe9e9392e5f12d8f0f6515a4412127b6e0a01980019a387", "ref_doc_id": "5f1998f6-c5e6-4079-ae42-69a6696165f3"}, "f2b57ebf-13fc-4d45-92c9-9a8d80e4de67": {"doc_hash": "7b76dd867eddff82d85453b93adcc03e210527115acf38460eb25131c4777f5e", "ref_doc_id": "5f1998f6-c5e6-4079-ae42-69a6696165f3"}, "5f1998f6-c5e6-4079-ae42-69a6696165f3": {"doc_hash": "fc04b1583c936fd0231e0ba7cd39b40df5ef24c4b4b850bcc3c0b7fb24d39282"}, "77acc3c9-010b-42b4-8058-501f6af7ed5e": {"doc_hash": "160f2aa5c1255a9da9fdd57e8e427095ba319d81efb83611e642754fc65d295a", "ref_doc_id": "46ea4f6c-a5dc-4540-af10-5b97becb5b0b"}, "d550416b-a737-4394-af23-19a7030926bf": {"doc_hash": "4f6ca4ef7a836158d2cc4808145365df2181ffa9a85b1eeb9e10c67dd6b6e7b6", "ref_doc_id": "46ea4f6c-a5dc-4540-af10-5b97becb5b0b"}, "97726f7c-7ec1-4634-a537-31a89ef4f13a": {"doc_hash": "3fa1e29a50f039a06567cbc937ee64619f92c0f46d90eaa055a55aa81d8433c1", "ref_doc_id": "46ea4f6c-a5dc-4540-af10-5b97becb5b0b"}, "46ea4f6c-a5dc-4540-af10-5b97becb5b0b": {"doc_hash": "f649a0df836496002cdcf0f5dd1fbf96275f8f9d1b06ca574d4d1ffceb5bd394"}, "8050e939-1125-42cd-bd95-4b3e377487fd": {"doc_hash": "9256e4e553fc4aa4818496c2b032f6dfb1568ba64038a2f9719aa07b9c25f960", "ref_doc_id": "517284ca-f4a1-47b9-84cd-c5260fc9d8c4"}, "a55b8e24-c615-46c0-acb5-6f11a4716590": {"doc_hash": "5b50eb6ce5529a3fef986b4100834bbb4062e5fb4c53078bb8f57d41715846bb", "ref_doc_id": "517284ca-f4a1-47b9-84cd-c5260fc9d8c4"}, "5ca4dc59-d00b-4d6d-9276-8d54014d351a": {"doc_hash": "67a08b9563de4edf503134a62fc71e11dff7d6241b3b54d2887f181fda1e7a26", "ref_doc_id": "517284ca-f4a1-47b9-84cd-c5260fc9d8c4"}, "517284ca-f4a1-47b9-84cd-c5260fc9d8c4": {"doc_hash": "72bb7d0eaac97f9b144e0a1ba0c6d7cfe2da0a385a3f586acefcc76718cf0aca"}, "5e274bf4-4365-4b88-8de7-104c3376a8f8": {"doc_hash": "205f40654d5210f3daf97021db41877de0c684087bed1596dc769dc1216a500d", "ref_doc_id": "5adcf81d-04d2-42d0-9bcb-eebfcf56a669"}, "4b6d0ac2-8d99-4601-bb80-f4ef955a7914": {"doc_hash": "baaa2468038f4f9977f01401e39d3b278974ba9e609b500ed64c71b8e81d752c", "ref_doc_id": "5adcf81d-04d2-42d0-9bcb-eebfcf56a669"}, "80266dca-d108-4456-ab8a-1ce508de3d48": {"doc_hash": "4a3be773e585cadaf0d32d4fa98df6bf169835ac1a1370bb3efa54ce0ff58852", "ref_doc_id": "5adcf81d-04d2-42d0-9bcb-eebfcf56a669"}, "5adcf81d-04d2-42d0-9bcb-eebfcf56a669": {"doc_hash": "2fc2598e52f082ad81a2ac18e06506a61e1a62d5c522fff7794e42c06c1bab47"}}, "docstore/ref_doc_info": {"c1e06d0c-6c8f-4bd0-bfbd-26df2eb621a6": {"node_ids": ["9abe8186-d3ee-4b5f-941a-910011263a8e"], "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:49:17.244779"}}, "41e4c824-9e12-4cf0-9dee-24eeeb2c49d9": {"node_ids": ["89b77865-6983-4efa-a38a-a882cb25ef2a"], "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:55:03.158939", "added_at": "2024-10-31T15:55:03.160934"}}, "5664aad3-4ca2-4875-ab83-efecfcf3be13": {"node_ids": ["b1b8a73f-49c5-4543-af78-54b2c32d8c1c"], "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:55:33.762583", "added_at": "2024-10-31T15:55:33.764543"}}, "bb299390-71a3-4727-b567-e022973f5854": {"node_ids": ["c8b3a2f3-8bf1-4d10-8ce7-0a44ba29b894"], "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:55:46.835013", "added_at": "2024-10-31T15:55:46.837002"}}, "7dbff074-b246-48c2-b559-69dceb4e36bf": {"node_ids": ["a9290983-7b2c-4198-bb2e-88286b2f80a0"], "metadata": {"title": "Lite TradingView Cloud Web Application Project - Google Docs", "url": "https://docs.google.com/document/d/1PuGtKrGXIfH-okvRUygCCcES_J3wDQQ-ChFj5pHS0eg/edit?usp=sharing", "timestamp": "2024-10-31T15:58:02.840733", "added_at": "2024-10-31T15:58:02.843691"}}, "ac60c74b-abed-4e3b-ae26-0ee99e4eef05": {"node_ids": ["e0b9120d-4aee-42c1-bbf6-e197e0cbf225"], "metadata": {"title": "https://zhuanlan.zhihu.com/p/651507945", "url": "https://zhuanlan.zhihu.com/p/651507945", "timestamp": "2024-10-31T15:58:29.235781", "added_at": "2024-10-31T15:58:29.237777"}}, "5dd615d6-3b80-489a-a37f-6f54a5b89965": {"node_ids": ["cab3cdf6-a214-423a-bc9f-d6209782c1bc", "1fd30894-f6b7-41ae-9487-c0a2473f3f0e", "944e96f8-b1c4-48ad-b473-fdc60a387fb1"], "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:29.202531", "added_at": "2024-10-31T15:59:29.204525"}}, "8c9a7e77-aceb-49f8-bcf7-1937f125dede": {"node_ids": ["f522e3ef-7a76-4a57-8d1e-e73ee37b5711", "7bae6454-9fea-4b7e-84f9-6824aa0e7d4c", "35fa1fa1-0baa-41fa-b557-e43b7bcdac22"], "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T15:59:55.804257", "added_at": "2024-10-31T15:59:55.807249"}}, "b5be37bb-881a-4e89-89ce-ba59ad94e5f8": {"node_ids": ["96a4dc85-2e06-4208-adc4-df4a79cf859d", "34a6fe28-63af-47de-9e80-048536082c3e", "7663abd3-e54c-419e-b82a-aea60e7204bc"], "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:00:21.628725", "added_at": "2024-10-31T16:00:21.630719"}}, "c2e677cf-e19a-4fae-8638-8a20998cc745": {"node_ids": ["c488e5f4-2fba-4629-8973-0151a2b7135e", "e6ba8e06-1662-4cb9-8354-eb274b97ff6d", "83f0c2dd-6b81-4825-9e7f-e0156f2a6834"], "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:11.557451", "added_at": "2024-10-31T16:04:11.559447"}}, "5f1998f6-c5e6-4079-ae42-69a6696165f3": {"node_ids": ["1cc8d102-be31-4fe2-b82b-ab8a485886db", "ba38bf66-0f3b-4460-a4fc-aa269f27a514", "f2b57ebf-13fc-4d45-92c9-9a8d80e4de67"], "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:04:51.931991", "added_at": "2024-10-31T16:04:51.933984"}}, "46ea4f6c-a5dc-4540-af10-5b97becb5b0b": {"node_ids": ["77acc3c9-010b-42b4-8058-501f6af7ed5e", "d550416b-a737-4394-af23-19a7030926bf", "97726f7c-7ec1-4634-a537-31a89ef4f13a"], "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:20.712155", "added_at": "2024-10-31T16:05:20.714149"}}, "517284ca-f4a1-47b9-84cd-c5260fc9d8c4": {"node_ids": ["8050e939-1125-42cd-bd95-4b3e377487fd", "a55b8e24-c615-46c0-acb5-6f11a4716590", "5ca4dc59-d00b-4d6d-9276-8d54014d351a"], "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:05:38.561531", "added_at": "2024-10-31T16:05:38.564523"}}, "5adcf81d-04d2-42d0-9bcb-eebfcf56a669": {"node_ids": ["5e274bf4-4365-4b88-8de7-104c3376a8f8", "4b6d0ac2-8d99-4601-bb80-f4ef955a7914", "80266dca-d108-4456-ab8a-1ce508de3d48"], "metadata": {"title": "[2410.23242] A little less conversation, a little more action, please: Investigating the physical common-sense of LLMs in a 3D embodied environment", "url": "https://arxiv.org/abs/2410.23242", "timestamp": "2024-10-31T16:08:48.680590", "added_at": "2024-10-31T16:08:48.682584"}}}}