{"docstore/data": {"a4ca4d39-f920-4b3c-9b16-7f21cff0247c": {"__data__": {"id_": "a4ca4d39-f920-4b3c-9b16-7f21cff0247c", "embedding": null, "metadata": {"filename": "GPT-4o System Card.pdf", "file_type": ".pdf", "timestamp": "2024-11-03T10:54:55.362085", "added_at": "2024-11-03T10:54:55.362085"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "672903a7-7538-48d2-8f33-f29b8ef9035f", "node_type": "4", "metadata": {"filename": "GPT-4o System Card.pdf", "file_type": ".pdf", "timestamp": "2024-11-03T10:54:55.362085", "added_at": "2024-11-03T10:54:55.362085"}, "hash": "30099c83261dd6015f607bf883f4c102fd55067812e3106ae2d2f8bc609a73e8", "class_name": "RelatedNodeInfo"}}, "text": "GPT-4o System Card\nOpenAI\u2217\nAugust 8, 2024\n1 Introduction\nGPT-4o[1] is an autoregressive omni model, which accepts as input any combination of text, audio,\nimage, and video and generates any combination of text, audio, and image outputs. It\u2019s trained\nend-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by\nthe same neural network.\nGPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320\nmilliseconds, which is similar to human response time[ 2] in a conversation. It matches GPT-4\nTurbo performance on text in English and code, with significant improvement on text in non-\nEnglish languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially\nbetter at vision and audio understanding compared to existing models.\nIn line with our commitment to building AI safely and consistent with our voluntary commitments\nto the White House[ 3], we are sharing the GPT-4o System Card, which includes our Preparedness\nFramework[ 4]evaluations. InthisSystemCard, weprovideadetailedlookatGPT-4o\u2019scapabilities,\nlimitations, and safety evaluations across multiple categories, with a focus on speech-to-speech\n(voice)1while also evaluating text and image capabilities, and the measures we\u2019ve implemented\nto ensure the model is safe and aligned. We also include third party assessments on dangerous\ncapabilities, as well as discussion of potential societal impacts of GPT-4o text and vision\ncapabilities.\n2 Model data and training\nGPT-4o\u2019s text and voice capabilities were pre-trained using data up to October 2023, sourced\nfrom a wide variety of materials including:\n\u2022Select publicly available data , mostly collected from industry-standard machine learning\ndatasets and web crawls.\n\u2217Please cite this work as \u201cOpenAI (2024)\". Full authorship contribution statements appear at the end of the\ndocument.\n1Some evaluations, in particular, the majority of the Preparedness Evaluations, third party assessments and\nsome of the societal impacts focus on the text and vision capabilities of GPT-4o, depending on the risk assessed.\nThis is indicated accordingly throughout the System Card.\n1arXiv:2410.21276v1  [cs.CL]  25 Oct 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "b69c95de-0731-4be8-a3ab-4a0fa0c06fa2": {"__data__": {"id_": "b69c95de-0731-4be8-a3ab-4a0fa0c06fa2", "embedding": null, "metadata": {"filename": "GPT-4o System Card.pdf", "file_type": ".pdf", "timestamp": "2024-11-03T10:56:51.387219", "added_at": "2024-11-03T10:56:51.387219"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed8ce39b-0177-4604-b6c5-9e8d5ab9100e", "node_type": "4", "metadata": {"filename": "GPT-4o System Card.pdf", "file_type": ".pdf", "timestamp": "2024-11-03T10:56:51.387219", "added_at": "2024-11-03T10:56:51.387219"}, "hash": "2c53aa504e56d5fa67c86dc0fd3ead67275449d172ac4ccad1755cb8cedd07ed", "class_name": "RelatedNodeInfo"}}, "text": "GPT-4o System Card\nOpenAI\u2217\nAugust 8, 2024\n1 Introduction\nGPT-4o[1] is an autoregressive omni model, which accepts as input any combination of text, audio,\nimage, and video and generates any combination of text, audio, and image outputs. It\u2019s trained\nend-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by\nthe same neural network.\nGPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320\nmilliseconds, which is similar to human response time[ 2] in a conversation. It matches GPT-4\nTurbo performance on text in English and code, with significant improvement on text in non-\nEnglish languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially\nbetter at vision and audio understanding compared to existing models.\nIn line with our commitment to building AI safely and consistent with our voluntary commitments\nto the White House[ 3], we are sharing the GPT-4o System Card, which includes our Preparedness\nFramework[ 4]evaluations. InthisSystemCard, weprovideadetailedlookatGPT-4o\u2019scapabilities,\nlimitations, and safety evaluations across multiple categories, with a focus on speech-to-speech\n(voice)1while also evaluating text and image capabilities, and the measures we\u2019ve implemented\nto ensure the model is safe and aligned. We also include third party assessments on dangerous\ncapabilities, as well as discussion of potential societal impacts of GPT-4o text and vision\ncapabilities.\n2 Model data and training\nGPT-4o\u2019s text and voice capabilities were pre-trained using data up to October 2023, sourced\nfrom a wide variety of materials including:\n\u2022Select publicly available data , mostly collected from industry-standard machine learning\ndatasets and web crawls.\n\u2217Please cite this work as \u201cOpenAI (2024)\". Full authorship contribution statements appear at the end of the\ndocument.\n1Some evaluations, in particular, the majority of the Preparedness Evaluations, third party assessments and\nsome of the societal impacts focus on the text and vision capabilities of GPT-4o, depending on the risk assessed.\nThis is indicated accordingly throughout the System Card.\n1arXiv:2410.21276v1  [cs.CL]  25 Oct 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}, "dba878d5-e6c5-4e27-bd4b-f688e8d01c34": {"__data__": {"id_": "dba878d5-e6c5-4e27-bd4b-f688e8d01c34", "embedding": null, "metadata": {"filename": "GPT-4o System Card.pdf", "file_type": ".pdf", "timestamp": "2024-11-03T10:59:02.209872", "added_at": "2024-11-03T10:59:02.209872"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4f7a8113-7b73-4631-a8b4-910dc00d3b75", "node_type": "4", "metadata": {"filename": "GPT-4o System Card.pdf", "file_type": ".pdf", "timestamp": "2024-11-03T10:59:02.209872", "added_at": "2024-11-03T10:59:02.209872"}, "hash": "953566856077deb335ea9b3933bc50efd366b092944e91076221272f8b5a0148", "class_name": "RelatedNodeInfo"}}, "text": "GPT-4o System Card\nOpenAI\u2217\nAugust 8, 2024\n1 Introduction\nGPT-4o[1] is an autoregressive omni model, which accepts as input any combination of text, audio,\nimage, and video and generates any combination of text, audio, and image outputs. It\u2019s trained\nend-to-end across text, vision, and audio, meaning that all inputs and outputs are processed by\nthe same neural network.\nGPT-4o can respond to audio inputs in as little as 232 milliseconds, with an average of 320\nmilliseconds, which is similar to human response time[ 2] in a conversation. It matches GPT-4\nTurbo performance on text in English and code, with significant improvement on text in non-\nEnglish languages, while also being much faster and 50% cheaper in the API. GPT-4o is especially\nbetter at vision and audio understanding compared to existing models.\nIn line with our commitment to building AI safely and consistent with our voluntary commitments\nto the White House[ 3], we are sharing the GPT-4o System Card, which includes our Preparedness\nFramework[ 4]evaluations. InthisSystemCard, weprovideadetailedlookatGPT-4o\u2019scapabilities,\nlimitations, and safety evaluations across multiple categories, with a focus on speech-to-speech\n(voice)1while also evaluating text and image capabilities, and the measures we\u2019ve implemented\nto ensure the model is safe and aligned. We also include third party assessments on dangerous\ncapabilities, as well as discussion of potential societal impacts of GPT-4o text and vision\ncapabilities.\n2 Model data and training\nGPT-4o\u2019s text and voice capabilities were pre-trained using data up to October 2023, sourced\nfrom a wide variety of materials including:\n\u2022Select publicly available data , mostly collected from industry-standard machine learning\ndatasets and web crawls.\n\u2217Please cite this work as \u201cOpenAI (2024)\". Full authorship contribution statements appear at the end of the\ndocument.\n1Some evaluations, in particular, the majority of the Preparedness Evaluations, third party assessments and\nsome of the societal impacts focus on the text and vision capabilities of GPT-4o, depending on the risk assessed.\nThis is indicated accordingly throughout the System Card.\n1arXiv:2410.21276v1  [cs.CL]  25 Oct 2024", "mimetype": "text/plain", "start_char_idx": 0, "end_char_idx": 2206, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n", "class_name": "TextNode"}, "__type__": "1"}}, "docstore/metadata": {"a4ca4d39-f920-4b3c-9b16-7f21cff0247c": {"doc_hash": "30099c83261dd6015f607bf883f4c102fd55067812e3106ae2d2f8bc609a73e8", "ref_doc_id": "672903a7-7538-48d2-8f33-f29b8ef9035f"}, "672903a7-7538-48d2-8f33-f29b8ef9035f": {"doc_hash": "30099c83261dd6015f607bf883f4c102fd55067812e3106ae2d2f8bc609a73e8"}, "b69c95de-0731-4be8-a3ab-4a0fa0c06fa2": {"doc_hash": "2c53aa504e56d5fa67c86dc0fd3ead67275449d172ac4ccad1755cb8cedd07ed", "ref_doc_id": "ed8ce39b-0177-4604-b6c5-9e8d5ab9100e"}, "ed8ce39b-0177-4604-b6c5-9e8d5ab9100e": {"doc_hash": "2c53aa504e56d5fa67c86dc0fd3ead67275449d172ac4ccad1755cb8cedd07ed"}, "dba878d5-e6c5-4e27-bd4b-f688e8d01c34": {"doc_hash": "953566856077deb335ea9b3933bc50efd366b092944e91076221272f8b5a0148", "ref_doc_id": "4f7a8113-7b73-4631-a8b4-910dc00d3b75"}, "4f7a8113-7b73-4631-a8b4-910dc00d3b75": {"doc_hash": "953566856077deb335ea9b3933bc50efd366b092944e91076221272f8b5a0148"}}, "docstore/ref_doc_info": {"672903a7-7538-48d2-8f33-f29b8ef9035f": {"node_ids": ["a4ca4d39-f920-4b3c-9b16-7f21cff0247c"], "metadata": {"filename": "GPT-4o System Card.pdf", "file_type": ".pdf", "timestamp": "2024-11-03T10:54:55.362085", "added_at": "2024-11-03T10:54:55.362085"}}, "ed8ce39b-0177-4604-b6c5-9e8d5ab9100e": {"node_ids": ["b69c95de-0731-4be8-a3ab-4a0fa0c06fa2"], "metadata": {"filename": "GPT-4o System Card.pdf", "file_type": ".pdf", "timestamp": "2024-11-03T10:56:51.387219", "added_at": "2024-11-03T10:56:51.387219"}}, "4f7a8113-7b73-4631-a8b4-910dc00d3b75": {"node_ids": ["dba878d5-e6c5-4e27-bd4b-f688e8d01c34"], "metadata": {"filename": "GPT-4o System Card.pdf", "file_type": ".pdf", "timestamp": "2024-11-03T10:59:02.209872", "added_at": "2024-11-03T10:59:02.209872"}}}}