import os
import base64
import gc
import tempfile
import uuid
import json
import requests
import time  # ç”¨äºè·Ÿè¸ªå“åº”æ—¶é—´
import threading  # ç”¨äºè°ƒåº¦å™¨
import schedule  # ç”¨äºå®šæ—¶ä»»åŠ¡
from typing import List, Dict, Optional
from datetime import datetime
import streamlit as st

from llama_index.readers.file.docs import DocxReader
from llama_index.readers.file.docs import PDFReader
from llama_index.readers.file.docs import HWPReader  # å‡è®¾å­˜åœ¨MarkdownReader

from bs4 import BeautifulSoup
from llama_index.core import (
    VectorStoreIndex,
    SimpleDirectoryReader,
    Settings,
    Document,
    StorageContext,
    load_index_from_storage,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.prompts import PromptTemplate
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.postprocessor import SimilarityPostprocessor
from llama_index.core.response_synthesizers import get_response_synthesizer

import tiktoken  # ç”¨äºTokenè®¡æ•°

# å¸¸é‡å®šä¹‰
SUPPORTED_FILE_TYPES = [".pdf", ".txt", ".doc", ".docx", ".md"]
CACHE_DIR = "cache"
PERSIST_DIR = "storage"
DATA_DIR = "data"  # æ•°æ®ç›®å½•ï¼Œç”¨äºå­˜æ”¾åŸŸç‰¹å®šçš„æº
MODEL_NAME = "llama3.1"

class KnowledgeSource:
    def __init__(self, source_type: str, content: str, metadata: Dict):
        self.source_type = source_type
        self.content = content
        self.metadata = metadata
        self.timestamp = datetime.now()

class WebScraper:
    @staticmethod
    def scrape_url(url: str) -> Optional[str]:
        """æŠ“å–ç½‘é¡µå†…å®¹å¹¶ä¿å­˜åˆ°ä¸´æ—¶JSONæ–‡ä»¶"""
        try:
            headers = {'User-Agent': 'Mozilla/5.0'}
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼å…ƒç´ 
            for script in soup(["script", "style"]):
                script.decompose()
                
            # æå–æ–‡æœ¬å†…å®¹
            text = soup.get_text(separator='\n', strip=True)
            
            # è°ƒè¯•è¾“å‡º
            st.write(f"Scraped content length: {len(text)}")
            st.write("First 200 characters of content:", text[:200])
            
            # æå–å…ƒæ•°æ®
            metadata = {
                'title': soup.title.string if soup.title else url,
                'url': url,
                'timestamp': datetime.now().isoformat()
            }
            
            if not text.strip():
                st.warning("Warning: Scraped content is empty")
                return None
            
            # ä¿å­˜æŠ“å–å†…å®¹åˆ°ä¸´æ—¶JSONæ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', encoding='utf-8', delete=False, suffix=".json") as tmp_file:
                json_data = {
                    "source_type": "web",
                    "content": text,
                    "metadata": metadata
                }
                json.dump(json_data, tmp_file)
                temp_file_path = tmp_file.name
                st.write(f"Scraped content saved to temporary JSON file: {temp_file_path}")

            return temp_file_path  # è¿”å›JSONæ–‡ä»¶è·¯å¾„
        except Exception as e:
            st.error(f"Error scraping URL {url}: {str(e)}")
            return None

def load_from_json_file(json_file_path: str, rag_system) -> bool:
    """ä»JSONæ–‡ä»¶åŠ è½½çŸ¥è¯†æºå¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“"""
    try:
        with open(json_file_path, 'r') as file:
            data = json.load(file)
            content = data.get("content", "")
            metadata = data.get("metadata", {})
            
            if not content.strip():
                st.warning("Loaded JSON content is empty")
                return False
            
            # åˆ›å»ºKnowledgeSourceå¯¹è±¡
            knowledge_source = KnowledgeSource(
                source_type=data.get("source_type", "file"),
                content=content,
                metadata=metadata
            )
            
            # æ·»åŠ åˆ°çŸ¥è¯†åº“
            success = rag_system.add_knowledge_source(knowledge_source)
            if success:
                st.success(f"Successfully loaded and added content from {json_file_path}")
                return True
            else:
                st.error(f"Failed to add content from {json_file_path} to knowledge base")
                return False
    except Exception as e:
        st.error(f"Error loading from JSON file {json_file_path}: {str(e)}")
        return False

def process_uploaded_files(uploaded_files, rag_system):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œæ·»åŠ åˆ°çŸ¥è¯†åº“"""
    if not uploaded_files:
        return
        
    for file in uploaded_files:
        temp_dir = tempfile.mkdtemp()
        temp_path = None
        try:
            temp_path = os.path.join(temp_dir, file.name)
            st.write(f"Processing file: {file.name} in {temp_path}")  # è°ƒè¯•ä¿¡æ¯
            
            with open(temp_path, 'wb') as tmp_file:
                tmp_file.write(file.getvalue())
            
            knowledge_source = rag_system.document_processor.process_file(temp_path)
            if knowledge_source:
                if knowledge_source.content.strip():
                    success = rag_system.add_knowledge_source(knowledge_source)
                    if success:
                        st.success(f"Successfully processed and added {file.name} to the knowledge base.")
                    else:
                        st.error(f"Failed to add {file.name} to knowledge base")
                else:
                    st.warning(f"No content extracted from {file.name}")
            else:
                st.error(f"Failed to process {file.name}")
                
        except FileNotFoundError as e:
            st.error(f"File not found error while processing {file.name}: {str(e)}")
        except PermissionError as e:
            st.error(f"Permission denied while processing {file.name}: {str(e)}")
        except IOError as e:
            st.error(f"IO error while processing {file.name}: {str(e)}")
        except Exception as e:
            st.error(f"Unexpected error processing {file.name}: {str(e)}")
            import traceback
            st.error(f"Traceback: {traceback.format_exc()}")
        finally:
            try:
                if temp_path and os.path.exists(temp_path):
                    os.remove(temp_path)
                if temp_dir and os.path.exists(temp_dir):
                    os.rmdir(temp_dir)
            except Exception as e:
                st.warning(f"Could not clean up temporary files: {str(e)}")

class DocumentProcessor:
    def __init__(self):
        self.parser = SentenceSplitter(
            chunk_size=512,
            chunk_overlap=50
        )

    def process_file(self, file_path: str) -> Optional[KnowledgeSource]:
        try:
            file_ext = os.path.splitext(file_path)[1].lower()

            if file_ext not in SUPPORTED_FILE_TYPES:
                raise ValueError(f"Unsupported file type: {file_ext}")

            if file_ext == '.docx':
                st.write(f"Using DocxReader for file: {file_path}")
                loader = DocxReader()
                docs = loader.load_data(file=file_path)
            elif file_ext == '.pdf':
                st.write(f"Using PDFReader for file: {file_path}")
                loader = PDFReader()
                docs = loader.load_data(file=file_path)
            else:
                st.warning(f"No reader available for file type: {file_ext}")
                return None

            if not docs:
                st.warning("No documents loaded from file")
                return None

            content = docs[0].text
            if not content.strip():
                st.warning("Extracted content is empty")
                return None

            st.write(f"Extracted content length: {len(content)}")
            st.write("First 200 characters:", content[:200])

            metadata = {
                'filename': os.path.basename(file_path),
                'file_type': file_ext,
                'timestamp': datetime.now().isoformat()
            }

            return KnowledgeSource(
                source_type='file',
                content=content,
                metadata=metadata
            )
        except Exception as e:
            st.error(f"Error processing file {file_path}: {str(e)}")
            import traceback
            st.error(f"Traceback: {traceback.format_exc()}")
            return None

class EnhancedRAG:
    def __init__(self, model_name: str):
        self.model_name = model_name
        self.llm = get_llm(model_name)
        self.embed_model = get_embeddings(model_name)
        self.document_processor = DocumentProcessor()
        self.web_scraper = WebScraper()
        
        # é…ç½®è®¾ç½®
        Settings.llm = self.llm
        Settings.embed_model = self.embed_model
        
        self._ensure_storage_initialized()
        self._load_domain_specific_sources()  # åŠ è½½åŸŸç‰¹å®šçš„æº
        self._start_scheduler()  # å¯åŠ¨è°ƒåº¦å™¨è¿›è¡Œå®šæœŸæ›´æ–°
        
    def _ensure_storage_initialized(self):
        """ç¡®ä¿å­˜å‚¨è¢«æ­£ç¡®åˆå§‹åŒ–"""
        try:
            os.makedirs(PERSIST_DIR, exist_ok=True)
            
            storage_exists = os.path.exists(PERSIST_DIR) and any(
                os.path.exists(os.path.join(PERSIST_DIR, f))
                for f in ['docstore.json', 'index_store.json', 'vector_store.json']
            )
            
            if not storage_exists:
                self.index = VectorStoreIndex([])
                # Removed adding initial documents during initialization
                # self._add_initial_documents()  # å¦‚æœæ²¡æœ‰å­˜å‚¨ï¼Œåˆ™æ·»åŠ åˆå§‹æ–‡æ¡£
            else:
                try:
                    storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
                    self.index = load_index_from_storage(storage_context)
                except Exception:
                    st.warning("Failed to load existing storage, creating new index")
                    self.index = VectorStoreIndex([])
                    
            # éªŒè¯ç´¢å¼•æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
            if not hasattr(self, 'index') or self.index is None:
                self.index = VectorStoreIndex([])
                
            # å¼ºåˆ¶æŒä¹…åŒ–åˆå§‹çŠ¶æ€
            self.index.storage_context.persist(persist_dir=PERSIST_DIR)
            
        except Exception as e:
            st.error(f"Storage initialization error: {str(e)}")
            self.index = VectorStoreIndex([])
    
    def _add_initial_documents(self):
        """å°†åˆå§‹åŸŸç‰¹å®šæ–‡æ¡£æ·»åŠ åˆ°ç´¢å¼•ä¸­ï¼ŒåŒ…å«å»é‡å’Œæ›´å¥½çš„é”™è¯¯å¤„ç†"""
        try:
            # å®šä¹‰è¦è¯»å–çš„ç›®å½•åˆ—è¡¨
            directories = [
                os.path.join(DATA_DIR, "scientific_literature"),
                os.path.join(DATA_DIR, "technical_blogs")
            ]
            
            all_documents = []
            # æ›´å®‰å…¨çš„æ–¹å¼è·å–ç°æœ‰æ–‡æ¡£æ–‡ä»¶å
            existing_filenames = set()
            if hasattr(self, 'index') and self.index and hasattr(self.index, 'docstore'):
                for doc_id, doc in self.index.docstore.docs.items():
                    if doc and hasattr(doc, 'metadata'):
                        filename = doc.metadata.get('filename')
                        if filename:  # åªæ·»åŠ æœ‰æ•ˆçš„æ–‡ä»¶å
                            existing_filenames.add(filename)
            
            for dir_path in directories:
                if not os.path.exists(dir_path):
                    st.warning(f"Directory {dir_path} does not exist.")
                    continue
                    
                try:
                    reader = SimpleDirectoryReader(
                        input_dir=dir_path,
                        file_extractor={
                            ".pdf": PDFReader(),
                            ".docx": DocxReader(),
                        },
                        recursive=True  # é€’å½’è¯»å–å­ç›®å½•
                    )
                    
                    documents = reader.load_data()
                    
                    # æ·»åŠ æ›´è¯¦ç»†çš„æ—¥å¿—
                    st.info(f"Found {len(documents)} documents in {dir_path}")
                    
                    for doc in documents:
                        if not doc or not hasattr(doc, 'metadata'):
                            st.warning("Found invalid document without metadata")
                            continue
                            
                        filename = doc.metadata.get('filename')
                        if not filename:
                            # Handle missing filename by assigning a default
                            filename = doc.metadata.get('title', f"doc_{uuid.uuid4()}")
                            doc.metadata['filename'] = filename
                            st.warning(f"Document missing filename. Assigned filename: {filename}")
                            
                        if filename not in existing_filenames:
                            st.info(f"Adding new document: {filename}")
                            all_documents.append(doc)
                            existing_filenames.add(filename)  # æ›´æ–°ç°æœ‰æ–‡ä»¶é›†åˆ
                        else:
                            st.info(f"Document '{filename}' already exists in the knowledge base. Skipping.")
                            
                except Exception as e:
                    st.error(f"Error processing directory {dir_path}: {str(e)}")
                    continue
            
            if all_documents:
                # æ‰¹é‡æ’å…¥æ–‡æ¡£
                try:
                    for doc in all_documents:
                        self.index.insert(doc)
                        
                    st.success(f"Successfully added {len(all_documents)} new documents to the knowledge base.")
                    
                    # ç¡®ä¿æ›´æ”¹è¢«æŒä¹…åŒ–
                    try:
                        self.index.storage_context.persist(persist_dir=PERSIST_DIR)
                        st.success("Successfully persisted the updated knowledge base.")
                    except Exception as e:
                        st.error(f"Error persisting changes: {str(e)}")
                        
                except Exception as e:
                    st.error(f"Error inserting documents: {str(e)}")
            else:
                st.info("No new documents to add to the knowledge base.")
                
        except Exception as e:
            st.error(f"Error in _add_initial_documents: {str(e)}")
            import traceback
            st.error(f"Detailed error: {traceback.format_exc()}")
    
    def _load_domain_specific_sources(self):
        """åŠ è½½åŸŸç‰¹å®šçš„æºåˆ°çŸ¥è¯†åº“ä¸­"""
        try:
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦ä¸ºç©ºï¼Œä»¥é¿å…é‡å¤æ·»åŠ 
            if len(self.index.docstore.docs) == 0:
                self._add_initial_documents()
            else:
                st.info("Knowledge base already initialized with domain-specific sources.")
        except Exception as e:
            st.error(f"Error loading domain-specific sources: {str(e)}")
    
    def _start_scheduler(self):
        """å¯åŠ¨ä¸€ä¸ªåå°çº¿ç¨‹ï¼Œç”¨äºè°ƒåº¦å®šæœŸæ›´æ–°ä»»åŠ¡"""
        scheduler_thread = threading.Thread(target=self._run_scheduler, daemon=True)
        scheduler_thread.start()
    
    def _run_scheduler(self):
        """å®šä¹‰å¹¶è¿è¡Œå®šæ—¶ä»»åŠ¡"""
        schedule.every().day.at("02:00").do(self._update_knowledge_base)
        while True:
            schedule.run_pending()
            time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    
    def _update_knowledge_base(self):
        """ä½¿ç”¨æ–°çš„åŸŸç‰¹å®šæ–‡æ¡£æ›´æ–°çŸ¥è¯†åº“"""
        try:
            st.info("Starting knowledge base update...")
            self._add_initial_documents()  # é‡æ–°æ·»åŠ æ–‡æ¡£ï¼ˆç¡®ä¿å»é‡ï¼‰
            st.success("Knowledge base has been updated successfully.")
        except Exception as e:
            st.error(f"Error during knowledge base update: {str(e)}")
    
    def add_knowledge_source(self, source: KnowledgeSource) -> bool:
        """å°†æ–°çš„çŸ¥è¯†æºæ·»åŠ åˆ°ç´¢å¼•ä¸­ï¼ŒåŒ…å«éªŒè¯"""
        try:
            if not source or not source.content or not source.content.strip():
                st.warning("Empty content source provided")
                return False
                
            # åˆ›å»ºå¸¦æœ‰å†…å®¹éªŒè¯çš„æ–‡æ¡£
            doc = Document(
                text=source.content,
                metadata={
                    **source.metadata,
                    'added_at': datetime.now().isoformat()
                }
            )
            
            # éªŒè¯æ–‡æ¡£åˆ›å»º
            if not doc or not doc.text:
                st.warning("Failed to create valid document")
                return False
            
            # æ’å…¥å¹¶éªŒè¯
            self.index.insert(doc)
            doc_count = len(self.index.docstore.docs)
            st.info(f"Index now contains {doc_count} documents")
            
            # æŒä¹…åŒ–æ›´æ”¹
            self.index.storage_context.persist(persist_dir=PERSIST_DIR)
            
            # éªŒè¯æŒä¹…åŒ–
            if not os.path.exists(os.path.join(PERSIST_DIR, 'docstore.json')):
                st.warning("Failed to persist index")
                return False
                
            return True
                
        except Exception as e:
            st.error(f"Error adding knowledge source: {str(e)}")
            return False
                
    def create_query_engine(self, similarity_threshold: float = 0.7) -> RetrieverQueryEngine:
        """åˆ›å»ºä¸€ä¸ªä¼˜åŒ–è¿‡çš„æŸ¥è¯¢å¼•æ“ï¼Œä½¿ç”¨è‡ªå®šä¹‰æç¤º"""
        # è‡ªå®šä¹‰æŸ¥è¯¢æç¤ºæ¨¡æ¿
        query_prompt_tmpl = """Answer the question based on the provided context. If the context doesn't contain relevant information, please indicate that you cannot answer.

Relevant Context:
---------------
{context_str}
---------------

Question: {query_str}

Please provide a detailed answer and reference specific information from the context when possible. For scientific papers, include:
1. Main research objectives
2. Methods used
3. Key findings
4. Important conclusions

Answer:"""
    
        # åˆ›å»ºè‡ªå®šä¹‰æç¤º
        query_prompt = PromptTemplate(query_prompt_tmpl)

        # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºåˆ›å»ºå“åº”åˆæˆå™¨
        response_synthesizer = get_response_synthesizer(
            response_mode="compact",
            text_qa_template=query_prompt,  # ä½¿ç”¨è‡ªå®šä¹‰æŸ¥è¯¢æç¤º
        )

        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=10,  # å¢åŠ æ£€ç´¢çš„æ–‡æ¡£æ•°é‡
            filters=None
        )

        # åå¤„ç†å™¨
        postprocessor = SimilarityPostprocessor(
            similarity_threshold=similarity_threshold
        )

        # åˆ›å»ºå¹¶è¿”å›æŸ¥è¯¢å¼•æ“ï¼Œä¸ä½¿ç”¨å•ç‹¬çš„query_promptå‚æ•°
        return RetrieverQueryEngine(
            retriever=retriever,
            node_postprocessors=[postprocessor],
            response_synthesizer=response_synthesizer
        )

    def query(self, query_text: str, context_window: int = 3) -> Dict:
        """å¢å¼ºçš„æŸ¥è¯¢æ–¹æ³•ï¼ŒåŒ…å«æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œæ€§èƒ½è·Ÿè¸ª"""
        try:
            # éªŒè¯ç´¢å¼•çŠ¶æ€
            if not hasattr(self, 'index') or self.index is None:
                return {
                    'response': "Knowledge base not properly initialized. Please reload documents.",
                    'sources': [],
                    'tokens': 0
                }

            # æ£€æŸ¥æ–‡æ¡£æ•°é‡
            doc_count = len(self.index.docstore.docs)
            if doc_count == 0:
                return {
                    'response': "Knowledge base is empty. Please add some documents first.",
                    'sources': [],
                    'tokens': 0
                }

            st.info(f"Querying knowledge base containing {doc_count} documents...")

            # é¢„å¤„ç†æŸ¥è¯¢
            processed_query = self._preprocess_query(query_text)
            
            # å¼€å§‹è®¡æ—¶
            start_time = time.time()

            # åˆ›å»ºæŸ¥è¯¢å¼•æ“
            query_engine = self.create_query_engine()

            # æ‰§è¡ŒæŸ¥è¯¢
            response = query_engine.query(processed_query)

            # ç»“æŸè®¡æ—¶
            end_time = time.time()
            response_time = end_time - start_time

            # å­˜å‚¨å“åº”æ—¶é—´
            if 'response_times' not in st.session_state:
                st.session_state.response_times = []
            st.session_state.response_times.append(response_time)

            # ä½¿ç”¨tiktokenè¿›è¡ŒTokenè®¡æ•°
            encoding = tiktoken.get_encoding("cl100k_base")  # æ ¹æ®æ¨¡å‹é€‰æ‹©åˆé€‚çš„ç¼–ç å™¨
            tokens = len(encoding.encode(response.response)) if response.response else 0

            if 'token_counts' not in st.session_state:
                st.session_state.token_counts = []
            st.session_state.token_counts.append(tokens)

            if not response or not response.response:
                return {
                    'response': "Unable to generate a valid answer. Please try rephrasing the question.",
                    'sources': [],
                    'tokens': tokens
                }

            # ç»„ç»‡æºä¿¡æ¯
            sources = []
            if hasattr(response, 'source_nodes'):
                for node in response.source_nodes:
                    source = {
                        'content': node.node.text[:300] + "..." if len(node.node.text) > 300 else node.node.text,
                        'metadata': node.node.metadata,
                        'score': node.score
                    }
                    # æ·»åŠ æ–‡æ¡£æ ‡é¢˜
                    if 'title' in node.node.metadata:
                        source['title'] = node.node.metadata['title']
                    elif 'filename' in node.node.metadata:
                        source['title'] = node.node.metadata['filename']
                    sources.append(source)

            return {
                'response': response.response,
                'sources': sources,
                'response_time': response_time,  # åŒ…å«å“åº”æ—¶é—´
                'tokens': tokens  # åŒ…å«Tokenè®¡æ•°
            }

        except Exception as e:
            st.error(f"Error occurred during query processing: {str(e)}")
            import traceback
            st.error(f"Error details: {traceback.format_exc()}")
            return {
                'response': "An error occurred while processing the query. Please try again later.",
                'sources': [],
                'tokens': 0
            }

    def _preprocess_query(self, query_text: str) -> str:
        """é¢„å¤„ç†æŸ¥è¯¢æ–‡æœ¬"""
        # å¤„ç†å¸¸è§çš„æŸ¥è¯¢ç±»å‹
        if query_text.lower().strip() in ['summary', 'summarize', 'summarise']:
            return "Please summarize the main contents of the document, including research objectives, methods, key findings, and conclusions."
        elif query_text.lower().strip() in ['detail', 'details', 'detailed']:
            return "Please provide a detailed content analysis of the document, including research background, methodology, experimental design, results, and discussion."
        
        return query_text

def get_llm(model_name: str) -> Ollama:
    return Ollama(model=model_name, request_timeout=300.0, device='cuda')

def get_embeddings(model_name: str) -> OllamaEmbedding:
    return OllamaEmbedding(
        model_name=model_name,
        request_timeout=300.0,
        device='cuda'
    )

class EnhancedChatInterface:
    def __init__(self, rag_system):
        self.rag_system = rag_system
        self.initialize_session_state()
        
    def initialize_session_state(self):
        """åˆå§‹åŒ–Streamlitä¼šè¯çŠ¶æ€å˜é‡"""
        if "id" not in st.session_state:
            st.session_state.id = str(uuid.uuid4())
        if "messages" not in st.session_state:
            st.session_state.messages = []
        if "feedback" not in st.session_state:
            st.session_state.feedback = {}
        if "response_times" not in st.session_state:
            st.session_state.response_times = []  # åˆå§‹åŒ–å“åº”æ—¶é—´
        if "token_counts" not in st.session_state:
            st.session_state.token_counts = []  # åˆå§‹åŒ–Tokenè®¡æ•°
        if "total_queries" not in st.session_state:
            st.session_state.total_queries = 0
        if "positive_feedback" not in st.session_state:
            st.session_state.positive_feedback = 0
        if "negative_feedback" not in st.session_state:
            st.session_state.negative_feedback = 0
        if "feedback_comments" not in st.session_state:
            st.session_state.feedback_comments = []
            
    def render_main_interface(self):
        """æ¸²æŸ“ä¸»èŠå¤©ç•Œé¢"""
        st.title("Enhanced RAG Chatbot")
        
        # èŠå¤©å†å²
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])
                
                # æ˜¾ç¤ºåŠ©æ‰‹å›ç­”çš„æºä¿¡æ¯
                if msg["role"] == "assistant" and "sources" in msg:
                    with st.expander("View Sources"):
                        for source in msg["sources"]:
                            st.markdown(f"**Source:** {source['metadata'].get('title', 'Unknown')}")
                            st.markdown(f"**Relevance:** {source['score']:.2f}")
                            st.markdown(f"**Extract:** {source['content']}")
                
                # æ·»åŠ åé¦ˆæŒ‰é’®
                if msg["role"] == "assistant":
                    cols = st.columns(2)
                    msg_id = msg.get("id", "unknown")
                    
                    if cols[0].button("ğŸ‘", key=f"thumbs_up_{msg_id}"):
                        st.session_state.feedback[msg_id] = "positive"
                        st.session_state.positive_feedback += 1
                        st.session_state.total_queries += 1
                        st.toast("Thank you for your feedback!")
                        
                    if cols[1].button("ğŸ‘", key=f"thumbs_down_{msg_id}"):
                        st.session_state.feedback[msg_id] = "negative"
                        feedback = st.text_input(
                            "Please tell us what we could improve:",
                            key=f"feedback_{msg_id}"
                        )
                        if feedback:
                            st.session_state.feedback[msg_id] = {"rating": "negative", "comment": feedback}
                            st.session_state.negative_feedback += 1
                            st.session_state.feedback_comments.append(feedback)
                            st.session_state.total_queries += 1
                            st.toast("Thank you for your feedback!")
                    
    def handle_url_input(self, url_input):
        """å¤„ç†URLè¾“å…¥ï¼Œä¿å­˜å†…å®¹åˆ°JSONå¹¶æ·»åŠ åˆ°çŸ¥è¯†åº“"""
        if not self.rag_system:
            st.error("RAG system is not initialized.")
            return

        # æŠ“å–å†…å®¹å¹¶ä¿å­˜åˆ°ä¸´æ—¶JSON
        json_file_path = self.rag_system.web_scraper.scrape_url(url_input)
        
        if json_file_path:
            # ä»JSONæ–‡ä»¶åŠ è½½å†…å®¹åˆ°çŸ¥è¯†åº“
            success = load_from_json_file(json_file_path, self.rag_system)
            if success:
                st.success(f"Content from {url_input} has been added to the knowledge base.")
            else:
                st.error(f"Failed to add content from {url_input} to the knowledge base.")
        else:
            st.error(f"Failed to fetch content from {url_input}.")
    
    def clear_conversation(self):
        """æ¸…é™¤å¯¹è¯å’Œç›¸å…³ä¼šè¯çŠ¶æ€å˜é‡"""
        st.session_state.messages = []
        st.session_state.feedback = {}
        st.session_state.total_queries = 0
        st.session_state.positive_feedback = 0
        st.session_state.negative_feedback = 0
        st.session_state.feedback_comments = []
        st.session_state.response_times = []
        st.session_state.token_counts = []
        st.success("Conversation and feedback have been cleared.")
        st.rerun()

    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜å’Œå­˜å‚¨ç›®å½•"""
        try:
            # åˆ é™¤ç¼“å­˜ç›®å½•
            if os.path.exists(CACHE_DIR):
                for filename in os.listdir(CACHE_DIR):
                    file_path = os.path.join(CACHE_DIR, filename)
                    if os.path.isfile(file_path) or os.path.islink(file_path):
                        os.unlink(file_path)
                    elif os.path.isdir(file_path):
                        import shutil
                        shutil.rmtree(file_path)
                os.rmdir(CACHE_DIR)
                st.success("Cache directory has been cleared.")
            else:
                st.info("Cache directory does not exist.")
            
            # åˆ é™¤å­˜å‚¨ç›®å½•
            if os.path.exists(PERSIST_DIR):
                for filename in os.listdir(PERSIST_DIR):
                    file_path = os.path.join(PERSIST_DIR, filename)
                    if os.path.isfile(file_path) or os.path.islink(file_path):
                        os.unlink(file_path)
                    elif os.path.isdir(file_path):
                        import shutil
                        shutil.rmtree(file_path)
                os.rmdir(PERSIST_DIR)
                st.success("Storage directory has been cleared.")
            else:
                st.info("Storage directory does not exist.")
            
            # æ¸…é™¤ç¼“å­˜åé‡æ–°åˆå§‹åŒ–RAGç³»ç»Ÿ
            self.rag_system._ensure_storage_initialized()
            st.success("RAG system has been reinitialized.")
            
        except Exception as e:
            st.error(f"Error clearing cache: {str(e)}")
    
    def render_sidebar(self):
        """æ¸²æŸ“ä¾§è¾¹æ ï¼ŒåŒ…å«é…ç½®é€‰é¡¹å’Œç»Ÿè®¡ä¿¡æ¯"""
        with st.sidebar:
            st.header("ğŸ“• RAG")
            
            # RAGé…ç½®
            use_rag = st.checkbox("Enable RAG", value=True)
            
            uploaded_files = None
            if use_rag:
                st.subheader("Knowledge Sources")
                
                # æ–‡ä»¶ä¸Šä¼ 
                uploaded_files = st.file_uploader(
                    "Upload Documents",
                    type=[ext[1:] for ext in SUPPORTED_FILE_TYPES],
                    accept_multiple_files=True
                )
                
                # URLè¾“å…¥
                url_input = st.text_input("Add Web Page (URL)")
                if url_input and st.button("Fetch Content"):
                    self.handle_url_input(url_input)
                
                # åŠ è½½åŸŸç‰¹å®šæºæŒ‰é’®
                if st.button("Load Domain-Specific Sources"):
                    self.rag_system._add_initial_documents()
                
                # æ‰‹åŠ¨æ›´æ–°çŸ¥è¯†åº“æŒ‰é’®
                if st.button("Update Knowledge Base Now"):
                    self.rag_system._update_knowledge_base()
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            st.markdown("---")
            st.header("ğŸ“Š Statistics")
            
            # ç”Ÿæˆé€Ÿåº¦ç»Ÿè®¡
            st.subheader("ğŸ”„ Generation Speed")
            if st.session_state.response_times:
                avg_response_time = sum(st.session_state.response_times) / len(st.session_state.response_times)
                recent_times = st.session_state.response_times[-5:]  # æœ€è¿‘5æ¬¡å“åº”æ—¶é—´
                st.write(f"**Average Response Time:** {avg_response_time:.2f} seconds")
                st.write(f"**Recent Response Times:** {', '.join([f'{t:.2f}s' for t in recent_times])}")
            else:
                st.write("No queries yet.")
            
            # Tokenç”Ÿæˆé€Ÿåº¦ç»Ÿè®¡
            st.subheader("ğŸ”¢ Token Generation Speed")
            if st.session_state.response_times and st.session_state.token_counts:
                # è®¡ç®—æ¯æ¬¡å“åº”çš„Tokenæ¯ç§’
                tokens_per_second = [tokens / time_sec if time_sec > 0 else 0 for tokens, time_sec in zip(st.session_state.token_counts, st.session_state.response_times)]
                
                # å¹³å‡Tokenç”Ÿæˆé€Ÿåº¦
                avg_tokens_per_sec = sum(tokens_per_second) / len(tokens_per_second)
                
                # æœ€è¿‘çš„Tokenç”Ÿæˆé€Ÿåº¦
                recent_tokens_per_sec = tokens_per_second[-5:]
                
                st.write(f"**Average Token Generation Speed:** {avg_tokens_per_sec:.2f} tokens/second")
                st.write(f"**Recent Token Generation Speeds:** {', '.join([f'{t:.2f} t/s' for t in recent_tokens_per_sec])}")
            else:
                st.write("No token data yet.")
            
            # è´¨é‡ç»Ÿè®¡
            st.subheader("â­ Quality")
            st.write(f"**Total Queries:** {st.session_state.total_queries}")
            st.write(f"**ğŸ‘ Positive Feedback:** {st.session_state.positive_feedback}")
            st.write(f"**ğŸ‘ Negative Feedback:** {st.session_state.negative_feedback}")
            if st.session_state.total_queries > 0:
                quality_score = (st.session_state.positive_feedback / st.session_state.total_queries) * 100
                st.write(f"**Overall Quality Score:** {quality_score:.2f}%")
            else:
                st.write("**Overall Quality Score:** N/A")
            
            # å¯é€‰ï¼šæ˜¾ç¤ºåé¦ˆè¯„è®º
            if st.session_state.feedback_comments:
                with st.expander("View Feedback Comments"):
                    for idx, comment in enumerate(st.session_state.feedback_comments, 1):
                        st.write(f"{idx}. {comment}")
            
            # æ·»åŠ åˆ†éš”çº¿
            st.markdown("---")
            
            # æ·»åŠ æ¸…ç©ºå¯¹è¯å’Œæ¸…é™¤ç¼“å­˜æŒ‰é’®
            st.header("ğŸ› ï¸ Tools")
            cols = st.columns(2)
            if cols[0].button("Clear ALL"):
                self.clear_conversation()
            if cols[1].button("Clear Cache"):
                self.clear_cache()
                
            return use_rag, uploaded_files

def main():
    # ä¾§è¾¹æ ä¸­çš„æ¨¡å‹é€‰æ‹©
    with st.sidebar:
        st.header("ğŸ¤– Configuration")
        
        # æ¨¡å‹é€‰æ‹©
        selected_model = st.selectbox(
            "Choose Model",
            ["llama3.1", "gemma2:9b"],
            index=0
        )
    
    # ä½¿ç”¨é€‰æ‹©çš„æ¨¡å‹åˆå§‹åŒ–RAGç³»ç»Ÿ
    rag_system = EnhancedRAG(selected_model)
    
    # ä½¿ç”¨RAGç³»ç»Ÿåˆå§‹åŒ–ç•Œé¢
    interface = EnhancedChatInterface(rag_system)
    
    # æ¸²æŸ“ä¾§è¾¹æ å¹¶è·å–é…ç½®
    use_rag, uploaded_files = interface.render_sidebar()
    
    # å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
    if uploaded_files:
        process_uploaded_files(uploaded_files, rag_system)
                
    # æ¸²æŸ“ä¸»ç•Œé¢
    interface.render_main_interface()
    
    # èŠå¤©è¾“å…¥
    if prompt := st.chat_input("Ask a question..."):
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        # ç”Ÿæˆå“åº”
        if use_rag:
            result = rag_system.query(prompt)
            response = result.get('response', "No response generated.")
            sources = result.get('sources', [])
            response_time = result.get('response_time', None)  # è·å–å“åº”æ—¶é—´
            tokens = result.get('tokens', 0)  # è·å–Tokenè®¡æ•°
        else:
            start_time = time.time()  # å¼€å§‹è®¡æ—¶
            response = rag_system.llm.complete(prompt).text
            end_time = time.time()  # ç»“æŸè®¡æ—¶
            response_time = end_time - start_time
            encoding = tiktoken.get_encoding("cl100k_base")  # é€‰æ‹©åˆé€‚çš„ç¼–ç å™¨
            tokens = len(encoding.encode(response)) if response else 0
            sources = []
            
            # å­˜å‚¨å“åº”æ—¶é—´å’ŒToken
            if 'response_times' not in st.session_state:
                st.session_state.response_times = []
            st.session_state.response_times.append(response_time)
            
            if 'token_counts' not in st.session_state:
                st.session_state.token_counts = []
            st.session_state.token_counts.append(tokens)
        
        # æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯å’Œå…ƒæ•°æ®
        msg_id = str(uuid.uuid4())
        st.session_state.messages.append({
            "role": "assistant",
            "content": response,
            "sources": sources,
            "id": msg_id,
            "timestamp": datetime.now().isoformat()
        })
        
        # å¼ºåˆ¶åˆ·æ–°
        try:
            st.rerun()
        except AttributeError:
            try:
                st.rerun()
            except AttributeError:
                # æ›¿ä»£æ–¹æ³•
                st.session_state['rerun'] = st.session_state.get('rerun', 0) + 1

if __name__ == "__main__":
    main()
